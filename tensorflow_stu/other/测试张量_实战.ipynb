{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mUKL7GM8jDst"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import datasets \n",
    "import os \n",
    "\n",
    "\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R9LF5mlTjaxm"
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n5ZYH6Ajjmvz"
   },
   "outputs": [],
   "source": [
    "(x,y),(x_test,y_test)=datasets.mnist.load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pHX_LUA4jrQ-"
   },
   "outputs": [],
   "source": [
    "x=tf.convert_to_tensor(x,dtype=tf.float32)/255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z6JlvBPmj3fe"
   },
   "outputs": [],
   "source": [
    "y=tf.convert_to_tensor(y,dtype=tf.int32) \n",
    "x_test=tf.convert_to_tensor(x_test,dtype=tf.float32)/255.0 \n",
    "y_test=tf.convert_to_tensor(y_test,dtype=tf.int32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pnfxeVIj71p",
    "outputId": "9c80d87d-96a3-4a86-9858-8906566a451d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,y.shape,x.dtype,y.dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfkPxhKjj-Jb",
    "outputId": "641ec10f-94f8-4d12-93c5-c68340db2c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_min(x),tf.reduce_max(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f74DTV4kDgz",
    "outputId": "c4003c20-577e-4895-a3a7-d5b0e6377902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_min(y),tf.reduce_max(y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPL-XW2BkKAE",
    "outputId": "983626e1-78db-43cb-e82a-fa825da798e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: (128, 28, 28) (128,)\n"
     ]
    }
   ],
   "source": [
    "train_db=tf.data.Dataset.from_tensor_slices((x,y)).batch(128) \n",
    "test_db=tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(128) \n",
    "train_iter=iter(train_db) \n",
    "sample=next(train_iter) \n",
    "print(\"batch:\",sample[0].shape,sample[1].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFoNFlWHkyEn",
    "outputId": "c6a8262e-1258-4fc4-c817-da52b0941535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 78 loss: 0.3927045166492462 ;test acc : 0.0689\n",
      "0 78 loss: 0.13074807822704315 ;test acc : 0.2053\n",
      "0 78 loss: 0.10684148967266083 ;test acc : 0.3248\n",
      "0 78 loss: 0.0966406911611557 ;test acc : 0.4232\n",
      "0 78 loss: 0.085474893450737 ;test acc : 0.4926\n",
      "1 78 loss: 0.07621917873620987 ;test acc : 0.5289\n",
      "1 78 loss: 0.07783162593841553 ;test acc : 0.5673\n",
      "1 78 loss: 0.07259271293878555 ;test acc : 0.6003\n",
      "1 78 loss: 0.07032232731580734 ;test acc : 0.6254\n",
      "1 78 loss: 0.0694315955042839 ;test acc : 0.6459\n",
      "2 78 loss: 0.06051882356405258 ;test acc : 0.6585\n",
      "2 78 loss: 0.06519881635904312 ;test acc : 0.6733\n",
      "2 78 loss: 0.062377315014600754 ;test acc : 0.6887\n",
      "2 78 loss: 0.06039971113204956 ;test acc : 0.7007\n",
      "2 78 loss: 0.06237991899251938 ;test acc : 0.7121\n",
      "3 78 loss: 0.05312492698431015 ;test acc : 0.7165\n",
      "3 78 loss: 0.058642417192459106 ;test acc : 0.7269\n",
      "3 78 loss: 0.05676771327853203 ;test acc : 0.7378\n",
      "3 78 loss: 0.054765015840530396 ;test acc : 0.7446\n",
      "3 78 loss: 0.058030206710100174 ;test acc : 0.752\n",
      "4 78 loss: 0.04854545742273331 ;test acc : 0.7545\n",
      "4 78 loss: 0.054197557270526886 ;test acc : 0.7613\n",
      "4 78 loss: 0.05281851440668106 ;test acc : 0.7687\n",
      "4 78 loss: 0.05096406489610672 ;test acc : 0.7734\n",
      "4 78 loss: 0.054818134754896164 ;test acc : 0.778\n",
      "5 78 loss: 0.0453437864780426 ;test acc : 0.78\n",
      "5 78 loss: 0.05091910436749458 ;test acc : 0.7844\n",
      "5 78 loss: 0.04982323199510574 ;test acc : 0.7904\n",
      "5 78 loss: 0.048187244683504105 ;test acc : 0.7932\n",
      "5 78 loss: 0.05221853777766228 ;test acc : 0.7973\n",
      "6 78 loss: 0.04290379211306572 ;test acc : 0.7993\n",
      "6 78 loss: 0.04836530238389969 ;test acc : 0.8018\n",
      "6 78 loss: 0.04742010682821274 ;test acc : 0.8066\n",
      "6 78 loss: 0.04600519686937332 ;test acc : 0.808\n",
      "6 78 loss: 0.050050973892211914 ;test acc : 0.8114\n",
      "7 78 loss: 0.04089025780558586 ;test acc : 0.8131\n",
      "7 78 loss: 0.04624665901064873 ;test acc : 0.8143\n",
      "7 78 loss: 0.04544534534215927 ;test acc : 0.8177\n",
      "7 78 loss: 0.04423404484987259 ;test acc : 0.8194\n",
      "7 78 loss: 0.048175036907196045 ;test acc : 0.8217\n",
      "8 78 loss: 0.03915843740105629 ;test acc : 0.8232\n",
      "8 78 loss: 0.04441515728831291 ;test acc : 0.8232\n",
      "8 78 loss: 0.04374075308442116 ;test acc : 0.8261\n",
      "8 78 loss: 0.04273368418216705 ;test acc : 0.8278\n",
      "8 78 loss: 0.04652276635169983 ;test acc : 0.8285\n",
      "9 78 loss: 0.0376586839556694 ;test acc : 0.8288\n",
      "9 78 loss: 0.04279320687055588 ;test acc : 0.8316\n",
      "9 78 loss: 0.042249370366334915 ;test acc : 0.834\n",
      "9 78 loss: 0.041408538818359375 ;test acc : 0.8344\n",
      "9 78 loss: 0.045024458318948746 ;test acc : 0.8353\n",
      "10 78 loss: 0.036347709596157074 ;test acc : 0.8344\n",
      "10 78 loss: 0.04134592413902283 ;test acc : 0.8374\n",
      "10 78 loss: 0.040927451103925705 ;test acc : 0.8406\n",
      "10 78 loss: 0.04023400694131851 ;test acc : 0.8408\n",
      "10 78 loss: 0.043673738837242126 ;test acc : 0.8422\n",
      "11 78 loss: 0.03518465906381607 ;test acc : 0.8431\n",
      "11 78 loss: 0.040044255554676056 ;test acc : 0.8443\n",
      "11 78 loss: 0.0397229865193367 ;test acc : 0.8467\n",
      "11 78 loss: 0.039187781512737274 ;test acc : 0.8465\n",
      "11 78 loss: 0.04241534695029259 ;test acc : 0.8484\n",
      "12 78 loss: 0.0341530479490757 ;test acc : 0.8481\n",
      "12 78 loss: 0.03886173665523529 ;test acc : 0.8498\n",
      "12 78 loss: 0.0386313833296299 ;test acc : 0.8518\n",
      "12 78 loss: 0.03823036700487137 ;test acc : 0.8522\n",
      "12 78 loss: 0.04126005619764328 ;test acc : 0.854\n",
      "13 78 loss: 0.03321247547864914 ;test acc : 0.8534\n",
      "13 78 loss: 0.037782222032547 ;test acc : 0.8545\n",
      "13 78 loss: 0.037632185965776443 ;test acc : 0.8565\n",
      "13 78 loss: 0.037342749536037445 ;test acc : 0.8573\n",
      "13 78 loss: 0.040181972086429596 ;test acc : 0.8581\n",
      "14 78 loss: 0.032342322170734406 ;test acc : 0.8582\n",
      "14 78 loss: 0.03679082542657852 ;test acc : 0.8597\n",
      "14 78 loss: 0.036706093698740005 ;test acc : 0.8606\n",
      "14 78 loss: 0.036519233137369156 ;test acc : 0.8607\n",
      "14 78 loss: 0.0391717329621315 ;test acc : 0.862\n",
      "15 78 loss: 0.031531527638435364 ;test acc : 0.8627\n",
      "15 78 loss: 0.03588758036494255 ;test acc : 0.8624\n",
      "15 78 loss: 0.03584686294198036 ;test acc : 0.8635\n",
      "15 78 loss: 0.035744816064834595 ;test acc : 0.8641\n",
      "15 78 loss: 0.038234893232584 ;test acc : 0.8647\n",
      "16 78 loss: 0.03077705204486847 ;test acc : 0.8659\n",
      "16 78 loss: 0.03504445403814316 ;test acc : 0.8653\n",
      "16 78 loss: 0.03503517061471939 ;test acc : 0.8672\n",
      "16 78 loss: 0.03501460701227188 ;test acc : 0.8678\n",
      "16 78 loss: 0.03736761584877968 ;test acc : 0.868\n",
      "17 78 loss: 0.03007826767861843 ;test acc : 0.8686\n",
      "17 78 loss: 0.03425715118646622 ;test acc : 0.8685\n",
      "17 78 loss: 0.03428234905004501 ;test acc : 0.8703\n",
      "17 78 loss: 0.03432304784655571 ;test acc : 0.8711\n",
      "17 78 loss: 0.03655780851840973 ;test acc : 0.8718\n",
      "18 78 loss: 0.029414555057883263 ;test acc : 0.8721\n",
      "18 78 loss: 0.03352977707982063 ;test acc : 0.8731\n",
      "18 78 loss: 0.03358232229948044 ;test acc : 0.8742\n",
      "18 78 loss: 0.03366794437170029 ;test acc : 0.8747\n",
      "18 78 loss: 0.03580068424344063 ;test acc : 0.876\n",
      "19 78 loss: 0.028790175914764404 ;test acc : 0.8747\n",
      "19 78 loss: 0.03285829350352287 ;test acc : 0.8761\n",
      "19 78 loss: 0.032922420650720596 ;test acc : 0.8773\n",
      "19 78 loss: 0.033050648868083954 ;test acc : 0.8771\n",
      "19 78 loss: 0.035110920667648315 ;test acc : 0.8781\n",
      "20 78 loss: 0.02821025811135769 ;test acc : 0.8775\n",
      "20 78 loss: 0.03223348408937454 ;test acc : 0.8783\n",
      "20 78 loss: 0.0322999432682991 ;test acc : 0.8788\n",
      "20 78 loss: 0.03247050940990448 ;test acc : 0.8791\n",
      "20 78 loss: 0.034465402364730835 ;test acc : 0.8794\n",
      "21 78 loss: 0.027666252106428146 ;test acc : 0.8797\n",
      "21 78 loss: 0.03165072575211525 ;test acc : 0.8791\n",
      "21 78 loss: 0.0317099392414093 ;test acc : 0.8811\n",
      "21 78 loss: 0.031922124326229095 ;test acc : 0.881\n",
      "21 78 loss: 0.03384748473763466 ;test acc : 0.8812\n",
      "22 78 loss: 0.02714504674077034 ;test acc : 0.8814\n",
      "22 78 loss: 0.031108850613236427 ;test acc : 0.8814\n",
      "22 78 loss: 0.031153783202171326 ;test acc : 0.8825\n",
      "22 78 loss: 0.03139965236186981 ;test acc : 0.8827\n",
      "22 78 loss: 0.03325394541025162 ;test acc : 0.8827\n",
      "23 78 loss: 0.026649311184883118 ;test acc : 0.8825\n",
      "23 78 loss: 0.030598202720284462 ;test acc : 0.8829\n",
      "23 78 loss: 0.030628502368927002 ;test acc : 0.8842\n",
      "23 78 loss: 0.03090520203113556 ;test acc : 0.8846\n",
      "23 78 loss: 0.03268366679549217 ;test acc : 0.8845\n",
      "24 78 loss: 0.026171904057264328 ;test acc : 0.8843\n",
      "24 78 loss: 0.03010713681578636 ;test acc : 0.8845\n",
      "24 78 loss: 0.030130306258797646 ;test acc : 0.8861\n",
      "24 78 loss: 0.03043469414114952 ;test acc : 0.8864\n",
      "24 78 loss: 0.03214145451784134 ;test acc : 0.886\n",
      "25 78 loss: 0.02571077272295952 ;test acc : 0.8861\n",
      "25 78 loss: 0.02963806316256523 ;test acc : 0.8865\n",
      "25 78 loss: 0.029651900753378868 ;test acc : 0.8875\n",
      "25 78 loss: 0.02997756004333496 ;test acc : 0.8876\n",
      "25 78 loss: 0.03162143751978874 ;test acc : 0.8874\n",
      "26 78 loss: 0.025259196758270264 ;test acc : 0.8876\n",
      "26 78 loss: 0.029186133295297623 ;test acc : 0.8884\n",
      "26 78 loss: 0.02919524535536766 ;test acc : 0.8886\n",
      "26 78 loss: 0.029541393741965294 ;test acc : 0.8886\n",
      "26 78 loss: 0.031125802546739578 ;test acc : 0.8892\n",
      "27 78 loss: 0.024825941771268845 ;test acc : 0.8891\n",
      "27 78 loss: 0.028754908591508865 ;test acc : 0.8894\n",
      "27 78 loss: 0.028763681650161743 ;test acc : 0.8904\n",
      "27 78 loss: 0.029125308617949486 ;test acc : 0.89\n",
      "27 78 loss: 0.0306601170450449 ;test acc : 0.8905\n",
      "28 78 loss: 0.024415437132120132 ;test acc : 0.8896\n",
      "28 78 loss: 0.02834520861506462 ;test acc : 0.8908\n",
      "28 78 loss: 0.028363868594169617 ;test acc : 0.8916\n",
      "28 78 loss: 0.0287298746407032 ;test acc : 0.8904\n",
      "28 78 loss: 0.03022109344601631 ;test acc : 0.8914\n",
      "29 78 loss: 0.0240255668759346 ;test acc : 0.8913\n",
      "29 78 loss: 0.02795570157468319 ;test acc : 0.8917\n",
      "29 78 loss: 0.027979379519820213 ;test acc : 0.8921\n",
      "29 78 loss: 0.028346333652734756 ;test acc : 0.892\n",
      "29 78 loss: 0.02980920672416687 ;test acc : 0.8924\n",
      "30 78 loss: 0.023649845272302628 ;test acc : 0.8929\n",
      "30 78 loss: 0.027582336217164993 ;test acc : 0.8927\n",
      "30 78 loss: 0.027612298727035522 ;test acc : 0.8933\n",
      "30 78 loss: 0.02797834575176239 ;test acc : 0.8932\n",
      "30 78 loss: 0.029418934136629105 ;test acc : 0.8941\n",
      "31 78 loss: 0.02328961342573166 ;test acc : 0.894\n",
      "31 78 loss: 0.027226869016885757 ;test acc : 0.8938\n",
      "31 78 loss: 0.027257943525910378 ;test acc : 0.8944\n",
      "31 78 loss: 0.027623092755675316 ;test acc : 0.8949\n",
      "31 78 loss: 0.029056433588266373 ;test acc : 0.8951\n",
      "32 78 loss: 0.022945942357182503 ;test acc : 0.8948\n",
      "32 78 loss: 0.026886790990829468 ;test acc : 0.8947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 78 loss: 0.026917720213532448 ;test acc : 0.8953\n",
      "32 78 loss: 0.02728351019322872 ;test acc : 0.8964\n",
      "32 78 loss: 0.028706630691885948 ;test acc : 0.8966\n",
      "33 78 loss: 0.022611204534769058 ;test acc : 0.8961\n",
      "33 78 loss: 0.02656487561762333 ;test acc : 0.8964\n",
      "33 78 loss: 0.026587393134832382 ;test acc : 0.8969\n",
      "33 78 loss: 0.02695661224424839 ;test acc : 0.8976\n",
      "33 78 loss: 0.02836812101304531 ;test acc : 0.8975\n",
      "34 78 loss: 0.022287271916866302 ;test acc : 0.8977\n",
      "34 78 loss: 0.02625555731356144 ;test acc : 0.8977\n",
      "34 78 loss: 0.02626574970781803 ;test acc : 0.8982\n",
      "34 78 loss: 0.02663826383650303 ;test acc : 0.8984\n",
      "34 78 loss: 0.02804647386074066 ;test acc : 0.899\n",
      "35 78 loss: 0.021977528929710388 ;test acc : 0.899\n",
      "35 78 loss: 0.025960305705666542 ;test acc : 0.8984\n",
      "35 78 loss: 0.02595910057425499 ;test acc : 0.8991\n",
      "35 78 loss: 0.026330390945076942 ;test acc : 0.8998\n",
      "35 78 loss: 0.027739685028791428 ;test acc : 0.8993\n",
      "36 78 loss: 0.021678129211068153 ;test acc : 0.8994\n",
      "36 78 loss: 0.025675997138023376 ;test acc : 0.8996\n",
      "36 78 loss: 0.02566538192331791 ;test acc : 0.8996\n",
      "36 78 loss: 0.02603490650653839 ;test acc : 0.9004\n",
      "36 78 loss: 0.027444183826446533 ;test acc : 0.9003\n",
      "37 78 loss: 0.02139248698949814 ;test acc : 0.9\n",
      "37 78 loss: 0.0254043135792017 ;test acc : 0.9013\n",
      "37 78 loss: 0.025384414941072464 ;test acc : 0.9004\n",
      "37 78 loss: 0.0257499311119318 ;test acc : 0.9013\n",
      "37 78 loss: 0.027153383940458298 ;test acc : 0.9013\n",
      "38 78 loss: 0.021118687465786934 ;test acc : 0.9015\n",
      "38 78 loss: 0.025145381689071655 ;test acc : 0.902\n",
      "38 78 loss: 0.025111908093094826 ;test acc : 0.9015\n",
      "38 78 loss: 0.025474343448877335 ;test acc : 0.9019\n",
      "38 78 loss: 0.026873882859945297 ;test acc : 0.9021\n",
      "39 78 loss: 0.020853836089372635 ;test acc : 0.9022\n",
      "39 78 loss: 0.024899102747440338 ;test acc : 0.9025\n",
      "39 78 loss: 0.024851888418197632 ;test acc : 0.9029\n",
      "39 78 loss: 0.02520732954144478 ;test acc : 0.9024\n",
      "39 78 loss: 0.02660757675766945 ;test acc : 0.9029\n",
      "40 78 loss: 0.02060163952410221 ;test acc : 0.9029\n",
      "40 78 loss: 0.024663003161549568 ;test acc : 0.9035\n",
      "40 78 loss: 0.024601969867944717 ;test acc : 0.9038\n",
      "40 78 loss: 0.024956122040748596 ;test acc : 0.9033\n",
      "40 78 loss: 0.026351939886808395 ;test acc : 0.9036\n",
      "41 78 loss: 0.020356997847557068 ;test acc : 0.9033\n",
      "41 78 loss: 0.024436483159661293 ;test acc : 0.9041\n",
      "41 78 loss: 0.024360129609704018 ;test acc : 0.9046\n",
      "41 78 loss: 0.02471354231238365 ;test acc : 0.9045\n",
      "41 78 loss: 0.026108810678124428 ;test acc : 0.9048\n",
      "42 78 loss: 0.02012253925204277 ;test acc : 0.9043\n",
      "42 78 loss: 0.024219444021582603 ;test acc : 0.905\n",
      "42 78 loss: 0.024121170863509178 ;test acc : 0.9051\n",
      "42 78 loss: 0.02448018454015255 ;test acc : 0.9052\n",
      "42 78 loss: 0.02588064968585968 ;test acc : 0.9053\n",
      "43 78 loss: 0.01989528350532055 ;test acc : 0.9051\n",
      "43 78 loss: 0.024009916931390762 ;test acc : 0.9057\n",
      "43 78 loss: 0.02389531210064888 ;test acc : 0.9059\n",
      "43 78 loss: 0.024254318326711655 ;test acc : 0.9064\n",
      "43 78 loss: 0.025658762082457542 ;test acc : 0.9061\n",
      "44 78 loss: 0.019679095596075058 ;test acc : 0.906\n",
      "44 78 loss: 0.02380768395960331 ;test acc : 0.9063\n",
      "44 78 loss: 0.023676138371229172 ;test acc : 0.9063\n",
      "44 78 loss: 0.024032196030020714 ;test acc : 0.9068\n",
      "44 78 loss: 0.025441324338316917 ;test acc : 0.9066\n",
      "45 78 loss: 0.019469182938337326 ;test acc : 0.9067\n",
      "45 78 loss: 0.023611128330230713 ;test acc : 0.9068\n",
      "45 78 loss: 0.023464495316147804 ;test acc : 0.9073\n",
      "45 78 loss: 0.023815525695681572 ;test acc : 0.9077\n",
      "45 78 loss: 0.025233883410692215 ;test acc : 0.9077\n",
      "46 78 loss: 0.019270142540335655 ;test acc : 0.9075\n",
      "46 78 loss: 0.023422840982675552 ;test acc : 0.9078\n",
      "46 78 loss: 0.023259127512574196 ;test acc : 0.9081\n",
      "46 78 loss: 0.023606227710843086 ;test acc : 0.9083\n",
      "46 78 loss: 0.025037605315446854 ;test acc : 0.9085\n",
      "47 78 loss: 0.019079873338341713 ;test acc : 0.9082\n",
      "47 78 loss: 0.023241419345140457 ;test acc : 0.9085\n",
      "47 78 loss: 0.023061078041791916 ;test acc : 0.9092\n",
      "47 78 loss: 0.023401927202939987 ;test acc : 0.9087\n",
      "47 78 loss: 0.024846680462360382 ;test acc : 0.9094\n",
      "48 78 loss: 0.018894050270318985 ;test acc : 0.909\n",
      "48 78 loss: 0.023066170513629913 ;test acc : 0.9092\n",
      "48 78 loss: 0.022866839542984962 ;test acc : 0.9096\n",
      "48 78 loss: 0.023202860727906227 ;test acc : 0.9101\n",
      "48 78 loss: 0.024663293734192848 ;test acc : 0.9101\n",
      "49 78 loss: 0.018712855875492096 ;test acc : 0.91\n",
      "49 78 loss: 0.022896824404597282 ;test acc : 0.91\n",
      "49 78 loss: 0.022678323090076447 ;test acc : 0.9098\n",
      "49 78 loss: 0.023012056946754456 ;test acc : 0.9105\n",
      "49 78 loss: 0.02448631078004837 ;test acc : 0.9108\n",
      "50 78 loss: 0.01853632740676403 ;test acc : 0.9111\n",
      "50 78 loss: 0.022736437618732452 ;test acc : 0.9103\n",
      "50 78 loss: 0.02249692939221859 ;test acc : 0.9101\n",
      "50 78 loss: 0.02282685786485672 ;test acc : 0.9111\n",
      "50 78 loss: 0.024311071261763573 ;test acc : 0.9111\n",
      "51 78 loss: 0.018362315371632576 ;test acc : 0.9113\n",
      "51 78 loss: 0.022582247853279114 ;test acc : 0.9108\n",
      "51 78 loss: 0.022319890558719635 ;test acc : 0.9107\n",
      "51 78 loss: 0.0226471908390522 ;test acc : 0.9112\n",
      "51 78 loss: 0.02413984201848507 ;test acc : 0.9116\n",
      "52 78 loss: 0.01819784753024578 ;test acc : 0.912\n",
      "52 78 loss: 0.02243715524673462 ;test acc : 0.9114\n",
      "52 78 loss: 0.022150304168462753 ;test acc : 0.9117\n",
      "52 78 loss: 0.022469058632850647 ;test acc : 0.9119\n",
      "52 78 loss: 0.023975133895874023 ;test acc : 0.9119\n",
      "53 78 loss: 0.01803753711283207 ;test acc : 0.9125\n",
      "53 78 loss: 0.02229570783674717 ;test acc : 0.912\n",
      "53 78 loss: 0.021985765546560287 ;test acc : 0.9124\n",
      "53 78 loss: 0.022297443822026253 ;test acc : 0.9127\n",
      "53 78 loss: 0.023814866319298744 ;test acc : 0.9126\n",
      "54 78 loss: 0.017883647233247757 ;test acc : 0.9133\n",
      "54 78 loss: 0.02216382697224617 ;test acc : 0.913\n",
      "54 78 loss: 0.021825674921274185 ;test acc : 0.9127\n",
      "54 78 loss: 0.022128049284219742 ;test acc : 0.9133\n",
      "54 78 loss: 0.023658927530050278 ;test acc : 0.9132\n",
      "55 78 loss: 0.017734404653310776 ;test acc : 0.9135\n",
      "55 78 loss: 0.022036883980035782 ;test acc : 0.9134\n",
      "55 78 loss: 0.021672461181879044 ;test acc : 0.9132\n",
      "55 78 loss: 0.02196277491748333 ;test acc : 0.9135\n",
      "55 78 loss: 0.023508479818701744 ;test acc : 0.914\n",
      "56 78 loss: 0.01759062148630619 ;test acc : 0.9143\n",
      "56 78 loss: 0.021913714706897736 ;test acc : 0.9139\n",
      "56 78 loss: 0.021522050723433495 ;test acc : 0.914\n",
      "56 78 loss: 0.02180159091949463 ;test acc : 0.9138\n",
      "56 78 loss: 0.02336209826171398 ;test acc : 0.9147\n",
      "57 78 loss: 0.017451748251914978 ;test acc : 0.9151\n",
      "57 78 loss: 0.021793950349092484 ;test acc : 0.9145\n",
      "57 78 loss: 0.021377604454755783 ;test acc : 0.9143\n",
      "57 78 loss: 0.02164607122540474 ;test acc : 0.9144\n",
      "57 78 loss: 0.02322034165263176 ;test acc : 0.9156\n",
      "58 78 loss: 0.01731995679438114 ;test acc : 0.9153\n",
      "58 78 loss: 0.021680748090147972 ;test acc : 0.9149\n",
      "58 78 loss: 0.021237824112176895 ;test acc : 0.9149\n",
      "58 78 loss: 0.021493617445230484 ;test acc : 0.9154\n",
      "58 78 loss: 0.02308492735028267 ;test acc : 0.9158\n",
      "59 78 loss: 0.017191680148243904 ;test acc : 0.916\n",
      "59 78 loss: 0.02157152071595192 ;test acc : 0.9158\n",
      "59 78 loss: 0.021099399775266647 ;test acc : 0.9157\n",
      "59 78 loss: 0.021344494074583054 ;test acc : 0.9159\n",
      "59 78 loss: 0.022954266518354416 ;test acc : 0.9163\n",
      "60 78 loss: 0.017067912966012955 ;test acc : 0.9158\n",
      "60 78 loss: 0.021465888246893883 ;test acc : 0.9163\n",
      "60 78 loss: 0.020962312817573547 ;test acc : 0.9161\n",
      "60 78 loss: 0.0211986992508173 ;test acc : 0.9167\n",
      "60 78 loss: 0.022827256470918655 ;test acc : 0.9169\n",
      "61 78 loss: 0.016946909949183464 ;test acc : 0.9166\n",
      "61 78 loss: 0.02136494591832161 ;test acc : 0.9166\n",
      "61 78 loss: 0.020832393318414688 ;test acc : 0.9167\n",
      "61 78 loss: 0.021056436002254486 ;test acc : 0.917\n",
      "61 78 loss: 0.02270093932747841 ;test acc : 0.9173\n",
      "62 78 loss: 0.016830403357744217 ;test acc : 0.9172\n",
      "62 78 loss: 0.021268721669912338 ;test acc : 0.9168\n",
      "62 78 loss: 0.020706744864583015 ;test acc : 0.9173\n",
      "62 78 loss: 0.020917881280183792 ;test acc : 0.9175\n",
      "62 78 loss: 0.022575903683900833 ;test acc : 0.9177\n",
      "63 78 loss: 0.01671936735510826 ;test acc : 0.9176\n",
      "63 78 loss: 0.02117440104484558 ;test acc : 0.9173\n",
      "63 78 loss: 0.020583901554346085 ;test acc : 0.9178\n",
      "63 78 loss: 0.02078350819647312 ;test acc : 0.918\n",
      "63 78 loss: 0.022456001490354538 ;test acc : 0.9181\n",
      "64 78 loss: 0.016610821709036827 ;test acc : 0.918\n",
      "64 78 loss: 0.021082162857055664 ;test acc : 0.9181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 78 loss: 0.020466594025492668 ;test acc : 0.9185\n",
      "64 78 loss: 0.020653704181313515 ;test acc : 0.9183\n",
      "64 78 loss: 0.02233697846531868 ;test acc : 0.9186\n",
      "65 78 loss: 0.016505492851138115 ;test acc : 0.9184\n",
      "65 78 loss: 0.020994190126657486 ;test acc : 0.9185\n",
      "65 78 loss: 0.02034853771328926 ;test acc : 0.919\n",
      "65 78 loss: 0.020525138825178146 ;test acc : 0.9188\n",
      "65 78 loss: 0.022220429033041 ;test acc : 0.9189\n",
      "66 78 loss: 0.016402671113610268 ;test acc : 0.9188\n",
      "66 78 loss: 0.020904744043946266 ;test acc : 0.919\n",
      "66 78 loss: 0.02023285999894142 ;test acc : 0.9193\n",
      "66 78 loss: 0.020398836582899094 ;test acc : 0.9194\n",
      "66 78 loss: 0.022107992321252823 ;test acc : 0.9196\n",
      "67 78 loss: 0.016304831951856613 ;test acc : 0.9193\n",
      "67 78 loss: 0.020817067474126816 ;test acc : 0.9197\n",
      "67 78 loss: 0.020116019994020462 ;test acc : 0.9196\n",
      "67 78 loss: 0.02027280442416668 ;test acc : 0.9194\n",
      "67 78 loss: 0.021997826173901558 ;test acc : 0.92\n",
      "68 78 loss: 0.016207540407776833 ;test acc : 0.9198\n",
      "68 78 loss: 0.020730972290039062 ;test acc : 0.9202\n",
      "68 78 loss: 0.020000485703349113 ;test acc : 0.9199\n",
      "68 78 loss: 0.020151156932115555 ;test acc : 0.9197\n",
      "68 78 loss: 0.021889615803956985 ;test acc : 0.9203\n",
      "69 78 loss: 0.016113897785544395 ;test acc : 0.9202\n",
      "69 78 loss: 0.02064497582614422 ;test acc : 0.9206\n",
      "69 78 loss: 0.019890502095222473 ;test acc : 0.9202\n",
      "69 78 loss: 0.02003178745508194 ;test acc : 0.92\n",
      "69 78 loss: 0.021786216646432877 ;test acc : 0.9203\n",
      "70 78 loss: 0.01602657325565815 ;test acc : 0.9203\n",
      "70 78 loss: 0.020560624077916145 ;test acc : 0.921\n",
      "70 78 loss: 0.019782427698373795 ;test acc : 0.9207\n",
      "70 78 loss: 0.019917229190468788 ;test acc : 0.9207\n",
      "70 78 loss: 0.02168542519211769 ;test acc : 0.921\n",
      "71 78 loss: 0.015939874574542046 ;test acc : 0.921\n",
      "71 78 loss: 0.02047901041805744 ;test acc : 0.9211\n",
      "71 78 loss: 0.019677598029375076 ;test acc : 0.921\n",
      "71 78 loss: 0.019804779440164566 ;test acc : 0.9213\n",
      "71 78 loss: 0.02158622443675995 ;test acc : 0.9218\n",
      "72 78 loss: 0.015855329111218452 ;test acc : 0.9215\n",
      "72 78 loss: 0.020399246364831924 ;test acc : 0.9219\n",
      "72 78 loss: 0.01957441121339798 ;test acc : 0.9211\n",
      "72 78 loss: 0.0196952186524868 ;test acc : 0.9215\n",
      "72 78 loss: 0.02148479036986828 ;test acc : 0.9217\n",
      "73 78 loss: 0.015771720558404922 ;test acc : 0.9218\n",
      "73 78 loss: 0.020317833870649338 ;test acc : 0.9221\n",
      "73 78 loss: 0.019472714513540268 ;test acc : 0.9218\n",
      "73 78 loss: 0.019588690251111984 ;test acc : 0.9217\n",
      "73 78 loss: 0.021386684849858284 ;test acc : 0.9223\n",
      "74 78 loss: 0.015689034014940262 ;test acc : 0.922\n",
      "74 78 loss: 0.020239679142832756 ;test acc : 0.9221\n",
      "74 78 loss: 0.01937459409236908 ;test acc : 0.9218\n",
      "74 78 loss: 0.019485682249069214 ;test acc : 0.9217\n",
      "74 78 loss: 0.021290922537446022 ;test acc : 0.9225\n",
      "75 78 loss: 0.015609420835971832 ;test acc : 0.9221\n",
      "75 78 loss: 0.02016417309641838 ;test acc : 0.9225\n",
      "75 78 loss: 0.019277598708868027 ;test acc : 0.9221\n",
      "75 78 loss: 0.01938459649682045 ;test acc : 0.922\n",
      "75 78 loss: 0.021196193993091583 ;test acc : 0.9226\n",
      "76 78 loss: 0.015530372969806194 ;test acc : 0.9222\n",
      "76 78 loss: 0.02008877322077751 ;test acc : 0.9227\n",
      "76 78 loss: 0.01918380707502365 ;test acc : 0.9227\n",
      "76 78 loss: 0.019287090748548508 ;test acc : 0.9226\n",
      "76 78 loss: 0.021106699481606483 ;test acc : 0.9229\n",
      "77 78 loss: 0.015452826395630836 ;test acc : 0.9228\n",
      "77 78 loss: 0.0200168676674366 ;test acc : 0.923\n",
      "77 78 loss: 0.019092410802841187 ;test acc : 0.9237\n",
      "77 78 loss: 0.01919110119342804 ;test acc : 0.9229\n",
      "77 78 loss: 0.021015401929616928 ;test acc : 0.9231\n",
      "78 78 loss: 0.015377345494925976 ;test acc : 0.9229\n",
      "78 78 loss: 0.019946910440921783 ;test acc : 0.9234\n",
      "78 78 loss: 0.019003916531801224 ;test acc : 0.9236\n",
      "78 78 loss: 0.01909605599939823 ;test acc : 0.9236\n",
      "78 78 loss: 0.02092837169766426 ;test acc : 0.9233\n",
      "79 78 loss: 0.015304374508559704 ;test acc : 0.9231\n",
      "79 78 loss: 0.0198791716247797 ;test acc : 0.9235\n",
      "79 78 loss: 0.018916627392172813 ;test acc : 0.9237\n",
      "79 78 loss: 0.019004056230187416 ;test acc : 0.9239\n",
      "79 78 loss: 0.020843226462602615 ;test acc : 0.9232\n",
      "80 78 loss: 0.015232810750603676 ;test acc : 0.9233\n",
      "80 78 loss: 0.01981249824166298 ;test acc : 0.9236\n",
      "80 78 loss: 0.018830714747309685 ;test acc : 0.9237\n",
      "80 78 loss: 0.018912067636847496 ;test acc : 0.924\n",
      "80 78 loss: 0.020758330821990967 ;test acc : 0.9234\n",
      "81 78 loss: 0.01516298670321703 ;test acc : 0.9234\n",
      "81 78 loss: 0.019748205319046974 ;test acc : 0.9238\n",
      "81 78 loss: 0.018745720386505127 ;test acc : 0.9242\n",
      "81 78 loss: 0.01882179081439972 ;test acc : 0.9245\n",
      "81 78 loss: 0.020675521343946457 ;test acc : 0.9238\n",
      "82 78 loss: 0.015095353126525879 ;test acc : 0.9234\n",
      "82 78 loss: 0.019684094935655594 ;test acc : 0.9239\n",
      "82 78 loss: 0.01866156980395317 ;test acc : 0.9248\n",
      "82 78 loss: 0.018734870478510857 ;test acc : 0.9247\n",
      "82 78 loss: 0.02059347555041313 ;test acc : 0.924\n",
      "83 78 loss: 0.015030508860945702 ;test acc : 0.9237\n",
      "83 78 loss: 0.019620370119810104 ;test acc : 0.9242\n",
      "83 78 loss: 0.01857869140803814 ;test acc : 0.9248\n",
      "83 78 loss: 0.01864936389029026 ;test acc : 0.9248\n",
      "83 78 loss: 0.020513571798801422 ;test acc : 0.9243\n",
      "84 78 loss: 0.014965156093239784 ;test acc : 0.9239\n",
      "84 78 loss: 0.01955815777182579 ;test acc : 0.9246\n",
      "84 78 loss: 0.018495777621865273 ;test acc : 0.9253\n",
      "84 78 loss: 0.018566301092505455 ;test acc : 0.925\n",
      "84 78 loss: 0.020434552803635597 ;test acc : 0.9248\n",
      "85 78 loss: 0.014900267124176025 ;test acc : 0.9242\n",
      "85 78 loss: 0.019495822489261627 ;test acc : 0.9251\n",
      "85 78 loss: 0.018412042409181595 ;test acc : 0.9254\n",
      "85 78 loss: 0.018484044820070267 ;test acc : 0.9252\n",
      "85 78 loss: 0.020355045795440674 ;test acc : 0.9253\n",
      "86 78 loss: 0.014839190058410168 ;test acc : 0.9248\n",
      "86 78 loss: 0.01943289488554001 ;test acc : 0.9251\n",
      "86 78 loss: 0.018329396843910217 ;test acc : 0.9258\n",
      "86 78 loss: 0.018402835354208946 ;test acc : 0.9257\n",
      "86 78 loss: 0.020276907831430435 ;test acc : 0.9254\n",
      "87 78 loss: 0.014778770506381989 ;test acc : 0.925\n",
      "87 78 loss: 0.019371336326003075 ;test acc : 0.9253\n",
      "87 78 loss: 0.01824985258281231 ;test acc : 0.926\n",
      "87 78 loss: 0.018323693424463272 ;test acc : 0.926\n",
      "87 78 loss: 0.020201316103339195 ;test acc : 0.926\n",
      "88 78 loss: 0.014720229431986809 ;test acc : 0.9251\n",
      "88 78 loss: 0.01931210421025753 ;test acc : 0.9254\n",
      "88 78 loss: 0.018169758841395378 ;test acc : 0.926\n",
      "88 78 loss: 0.018245328217744827 ;test acc : 0.9262\n",
      "88 78 loss: 0.02012806572020054 ;test acc : 0.9266\n",
      "89 78 loss: 0.014663314446806908 ;test acc : 0.9257\n",
      "89 78 loss: 0.01925189234316349 ;test acc : 0.9257\n",
      "89 78 loss: 0.018092745915055275 ;test acc : 0.9261\n",
      "89 78 loss: 0.0181693434715271 ;test acc : 0.9266\n",
      "89 78 loss: 0.020055163651704788 ;test acc : 0.927\n",
      "90 78 loss: 0.014607829041779041 ;test acc : 0.9261\n",
      "90 78 loss: 0.019192975014448166 ;test acc : 0.926\n",
      "90 78 loss: 0.018017565831542015 ;test acc : 0.9266\n",
      "90 78 loss: 0.018095463514328003 ;test acc : 0.9269\n",
      "90 78 loss: 0.019983407109975815 ;test acc : 0.9271\n",
      "91 78 loss: 0.014553183689713478 ;test acc : 0.9272\n",
      "91 78 loss: 0.01913570612668991 ;test acc : 0.9267\n",
      "91 78 loss: 0.01794375292956829 ;test acc : 0.927\n",
      "91 78 loss: 0.018021564930677414 ;test acc : 0.9274\n",
      "91 78 loss: 0.01991276815533638 ;test acc : 0.9274\n",
      "92 78 loss: 0.014498273842036724 ;test acc : 0.9272\n",
      "92 78 loss: 0.019076863303780556 ;test acc : 0.9272\n",
      "92 78 loss: 0.017871122807264328 ;test acc : 0.9273\n",
      "92 78 loss: 0.017949339002370834 ;test acc : 0.9276\n",
      "92 78 loss: 0.019841164350509644 ;test acc : 0.9275\n",
      "93 78 loss: 0.014444887638092041 ;test acc : 0.9276\n",
      "93 78 loss: 0.019019130617380142 ;test acc : 0.9274\n",
      "93 78 loss: 0.017800061032176018 ;test acc : 0.9274\n",
      "93 78 loss: 0.01787778176367283 ;test acc : 0.9279\n",
      "93 78 loss: 0.019769752398133278 ;test acc : 0.9279\n",
      "94 78 loss: 0.014391660690307617 ;test acc : 0.9279\n",
      "94 78 loss: 0.018961811438202858 ;test acc : 0.9275\n",
      "94 78 loss: 0.017731212079524994 ;test acc : 0.9279\n",
      "94 78 loss: 0.017807845026254654 ;test acc : 0.9281\n",
      "94 78 loss: 0.019700752571225166 ;test acc : 0.9282\n",
      "95 78 loss: 0.014339101500809193 ;test acc : 0.928\n",
      "95 78 loss: 0.01890389248728752 ;test acc : 0.9278\n",
      "95 78 loss: 0.01766234263777733 ;test acc : 0.9281\n",
      "95 78 loss: 0.01773666962981224 ;test acc : 0.9284\n",
      "95 78 loss: 0.019630352035164833 ;test acc : 0.9285\n",
      "96 78 loss: 0.014287124387919903 ;test acc : 0.9283\n",
      "96 78 loss: 0.018848136067390442 ;test acc : 0.9282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 78 loss: 0.017593957483768463 ;test acc : 0.9286\n",
      "96 78 loss: 0.017666051164269447 ;test acc : 0.9288\n",
      "96 78 loss: 0.019562486559152603 ;test acc : 0.9285\n",
      "97 78 loss: 0.014236497692763805 ;test acc : 0.9287\n",
      "97 78 loss: 0.01879177987575531 ;test acc : 0.9284\n",
      "97 78 loss: 0.01752633973956108 ;test acc : 0.9288\n",
      "97 78 loss: 0.01759621873497963 ;test acc : 0.9289\n",
      "97 78 loss: 0.019496072083711624 ;test acc : 0.9287\n",
      "98 78 loss: 0.014186844229698181 ;test acc : 0.9291\n",
      "98 78 loss: 0.0187370702624321 ;test acc : 0.9287\n",
      "98 78 loss: 0.017460156232118607 ;test acc : 0.9294\n",
      "98 78 loss: 0.017527032643556595 ;test acc : 0.9292\n",
      "98 78 loss: 0.01942979358136654 ;test acc : 0.9292\n",
      "99 78 loss: 0.014138281345367432 ;test acc : 0.9292\n",
      "99 78 loss: 0.018682602792978287 ;test acc : 0.9291\n",
      "99 78 loss: 0.017394665628671646 ;test acc : 0.9294\n",
      "99 78 loss: 0.01745874248445034 ;test acc : 0.9298\n",
      "99 78 loss: 0.019364366307854652 ;test acc : 0.9295\n"
     ]
    }
   ],
   "source": [
    "w1=tf.Variable(tf.random.truncated_normal([784,256],stddev=0.1))\n",
    "b1=tf.Variable(tf.zeros([256])) \n",
    "\n",
    "w2=tf.Variable(tf.random.truncated_normal([256,128],stddev=0.1))\n",
    "b2=tf.Variable(tf.zeros([128])) \n",
    "\n",
    "w3=tf.Variable(tf.random.truncated_normal([128,10],stddev=0.1))\n",
    "b3=tf.Variable(tf.zeros([10])) \n",
    "lr=0.01\n",
    "\n",
    "for epoch in range(100):\n",
    "  for step,(x,y) in enumerate(train_db):\n",
    "    x=tf.reshape(x,[-1,28*28]) \n",
    "    with tf.GradientTape() as tape:\n",
    "      h1=x@w1+tf.broadcast_to(b1,[x.shape[0],256]) \n",
    "      h1=tf.nn.relu(h1) \n",
    "      h2=h1@w2+b2 \n",
    "      h2=tf.nn.relu(h2) \n",
    "      out=h2@w3+b3 \n",
    "      y_onehot=tf.one_hot(y,depth=10) \n",
    "      loss=tf.square(y_onehot-out) \n",
    "      loss=tf.reduce_mean(loss) \n",
    "    grads=tape.gradient(loss,[w1,b1,w2,b2,w3,b3]) \n",
    "    w1.assign_sub(lr*grads[0]) \n",
    "    b1.assign_sub(lr*grads[1]) \n",
    "    w2.assign_sub(lr*grads[2]) \n",
    "    b2.assign_sub(lr*grads[3]) \n",
    "    w3.assign_sub(lr*grads[4]) \n",
    "    b3.assign_sub(lr*grads[5])\n",
    "    if step%100==0:\n",
    "        total_correct, total_num = 0,0  \n",
    "        for step,(x,y) in enumerate(test_db):\n",
    "          x=tf.reshape(x,[-1,28*28]) \n",
    "          h1=tf.nn.relu(x@w1+b1)\n",
    "          h2=tf.nn.relu(h1@w2+b2) \n",
    "          out=h2@w3+b3 \n",
    "          prob = tf.nn.softmax(out,axis=1) \n",
    "          pred=tf.argmax(prob,axis=1)\n",
    "          pred=tf.cast(pred,dtype=tf.int32)\n",
    "          correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "          correct=tf.reduce_sum(correct) \n",
    "          total_correct+=int(correct) \n",
    "          total_num+=x.shape[0] \n",
    "        acc=total_correct/total_num \n",
    "        print(epoch,step,'loss:',float(loss),\";test acc :\",acc ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "725WaxyqmQOg",
    "outputId": "64761c56-934f-46fa-a206-c56ba41f34bf"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SH23iV6HrDUC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "测试张量-实战.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
