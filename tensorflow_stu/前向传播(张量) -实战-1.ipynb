{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chemical-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import datasets,layers,optimizers \n",
    "\n",
    "\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stable-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y),_=datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proud-daniel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "maritime-easter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "herbal-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.convert_to_tensor(x,dtype=tf.float32)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "needed-franchise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mighty-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.convert_to_tensor(y,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cheap-antique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "settled-incidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=6, shape=(), dtype=float32, numpy=1.0>,\n",
       " <tf.Tensor: id=8, shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(x),tf.reduce_min(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "consecutive-charter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=12, shape=(), dtype=int32, numpy=9>,\n",
       " <tf.Tensor: id=14, shape=(), dtype=int32, numpy=0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(y),tf.reduce_min(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sonic-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db=tf.data.Dataset.from_tensor_slices((x,y)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "smart-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter=iter(train_db) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exterior-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "finite-memorial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 28, 28]), TensorShape([128]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0].shape,sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "competitive-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=tf.Variable( tf.random.truncated_normal([784,256],stddev=0.1))\n",
    "b1=tf.Variable( tf.zeros([256]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "universal-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=tf.Variable( tf.random.truncated_normal([256,128],stddev=0.1))\n",
    "b2=tf.Variable( tf.zeros([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "composite-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "w3=tf.Variable( tf.random.truncated_normal([128,10],stddev=0.1))\n",
    "b3=tf.Variable( tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "statutory-special",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 ;step: 0 loss: 0.3790063261985779\n",
      "epoch: 0 ;step: 100 loss: 0.33902618288993835\n",
      "epoch: 0 ;step: 200 loss: 0.3045094311237335\n",
      "epoch: 0 ;step: 300 loss: 0.27059608697891235\n",
      "epoch: 0 ;step: 400 loss: 0.3237118124961853\n",
      "epoch: 1 ;step: 0 loss: 0.27128535509109497\n",
      "epoch: 1 ;step: 100 loss: 0.25748783349990845\n",
      "epoch: 1 ;step: 200 loss: 0.2378804236650467\n",
      "epoch: 1 ;step: 300 loss: 0.2161216288805008\n",
      "epoch: 1 ;step: 400 loss: 0.25417426228523254\n",
      "epoch: 2 ;step: 0 loss: 0.22271756827831268\n",
      "epoch: 2 ;step: 100 loss: 0.2192154824733734\n",
      "epoch: 2 ;step: 200 loss: 0.20655295252799988\n",
      "epoch: 2 ;step: 300 loss: 0.18951493501663208\n",
      "epoch: 2 ;step: 400 loss: 0.21831700205802917\n",
      "epoch: 3 ;step: 0 loss: 0.19742240011692047\n",
      "epoch: 3 ;step: 100 loss: 0.1984933763742447\n",
      "epoch: 3 ;step: 200 loss: 0.18972530961036682\n",
      "epoch: 3 ;step: 300 loss: 0.17476820945739746\n",
      "epoch: 3 ;step: 400 loss: 0.19770169258117676\n",
      "epoch: 4 ;step: 0 loss: 0.18257257342338562\n",
      "epoch: 4 ;step: 100 loss: 0.1858936995267868\n",
      "epoch: 4 ;step: 200 loss: 0.17951859533786774\n",
      "epoch: 4 ;step: 300 loss: 0.16560348868370056\n",
      "epoch: 4 ;step: 400 loss: 0.18457815051078796\n",
      "epoch: 5 ;step: 0 loss: 0.17290036380290985\n",
      "epoch: 5 ;step: 100 loss: 0.17736756801605225\n",
      "epoch: 5 ;step: 200 loss: 0.17253589630126953\n",
      "epoch: 5 ;step: 300 loss: 0.15926286578178406\n",
      "epoch: 5 ;step: 400 loss: 0.17541328072547913\n",
      "epoch: 6 ;step: 0 loss: 0.16592176258563995\n",
      "epoch: 6 ;step: 100 loss: 0.17100733518600464\n",
      "epoch: 6 ;step: 200 loss: 0.16723378002643585\n",
      "epoch: 6 ;step: 300 loss: 0.1544685661792755\n",
      "epoch: 6 ;step: 400 loss: 0.16847831010818481\n",
      "epoch: 7 ;step: 0 loss: 0.16048257052898407\n",
      "epoch: 7 ;step: 100 loss: 0.16590765118598938\n",
      "epoch: 7 ;step: 200 loss: 0.1628788411617279\n",
      "epoch: 7 ;step: 300 loss: 0.15055780112743378\n",
      "epoch: 7 ;step: 400 loss: 0.16290602087974548\n",
      "epoch: 8 ;step: 0 loss: 0.15597079694271088\n",
      "epoch: 8 ;step: 100 loss: 0.16159318387508392\n",
      "epoch: 8 ;step: 200 loss: 0.15909568965435028\n",
      "epoch: 8 ;step: 300 loss: 0.14719566702842712\n",
      "epoch: 8 ;step: 400 loss: 0.1582099348306656\n",
      "epoch: 9 ;step: 0 loss: 0.15205678343772888\n",
      "epoch: 9 ;step: 100 loss: 0.15783177316188812\n",
      "epoch: 9 ;step: 200 loss: 0.15569016337394714\n",
      "epoch: 9 ;step: 300 loss: 0.14419585466384888\n",
      "epoch: 9 ;step: 400 loss: 0.15410470962524414\n",
      "epoch: 10 ;step: 0 loss: 0.14856860041618347\n",
      "epoch: 10 ;step: 100 loss: 0.15444578230381012\n",
      "epoch: 10 ;step: 200 loss: 0.15255214273929596\n",
      "epoch: 10 ;step: 300 loss: 0.14146408438682556\n",
      "epoch: 10 ;step: 400 loss: 0.1504303365945816\n",
      "epoch: 11 ;step: 0 loss: 0.14539554715156555\n",
      "epoch: 11 ;step: 100 loss: 0.15132352709770203\n",
      "epoch: 11 ;step: 200 loss: 0.14962388575077057\n",
      "epoch: 11 ;step: 300 loss: 0.13894572854042053\n",
      "epoch: 11 ;step: 400 loss: 0.1471034437417984\n",
      "epoch: 12 ;step: 0 loss: 0.14246627688407898\n",
      "epoch: 12 ;step: 100 loss: 0.14841745793819427\n",
      "epoch: 12 ;step: 200 loss: 0.14688313007354736\n",
      "epoch: 12 ;step: 300 loss: 0.1365877240896225\n",
      "epoch: 12 ;step: 400 loss: 0.14405329525470734\n",
      "epoch: 13 ;step: 0 loss: 0.13974136114120483\n",
      "epoch: 13 ;step: 100 loss: 0.14570504426956177\n",
      "epoch: 13 ;step: 200 loss: 0.14429961144924164\n",
      "epoch: 13 ;step: 300 loss: 0.13438615202903748\n",
      "epoch: 13 ;step: 400 loss: 0.14122144877910614\n",
      "epoch: 14 ;step: 0 loss: 0.13719025254249573\n",
      "epoch: 14 ;step: 100 loss: 0.1431630551815033\n",
      "epoch: 14 ;step: 200 loss: 0.1418631672859192\n",
      "epoch: 14 ;step: 300 loss: 0.13232421875\n",
      "epoch: 14 ;step: 400 loss: 0.13857820630073547\n",
      "epoch: 15 ;step: 0 loss: 0.13477449119091034\n",
      "epoch: 15 ;step: 100 loss: 0.1407744437456131\n",
      "epoch: 15 ;step: 200 loss: 0.13956518471240997\n",
      "epoch: 15 ;step: 300 loss: 0.1303708404302597\n",
      "epoch: 15 ;step: 400 loss: 0.13610705733299255\n",
      "epoch: 16 ;step: 0 loss: 0.13249024748802185\n",
      "epoch: 16 ;step: 100 loss: 0.1385258138179779\n",
      "epoch: 16 ;step: 200 loss: 0.1373877227306366\n",
      "epoch: 16 ;step: 300 loss: 0.12852376699447632\n",
      "epoch: 16 ;step: 400 loss: 0.13379059731960297\n",
      "epoch: 17 ;step: 0 loss: 0.13033083081245422\n",
      "epoch: 17 ;step: 100 loss: 0.1363975554704666\n",
      "epoch: 17 ;step: 200 loss: 0.13532397150993347\n",
      "epoch: 17 ;step: 300 loss: 0.12676632404327393\n",
      "epoch: 17 ;step: 400 loss: 0.13160920143127441\n",
      "epoch: 18 ;step: 0 loss: 0.12828700244426727\n",
      "epoch: 18 ;step: 100 loss: 0.13438628613948822\n",
      "epoch: 18 ;step: 200 loss: 0.13336528837680817\n",
      "epoch: 18 ;step: 300 loss: 0.1250923126935959\n",
      "epoch: 18 ;step: 400 loss: 0.12954555451869965\n",
      "epoch: 19 ;step: 0 loss: 0.12634949386119843\n",
      "epoch: 19 ;step: 100 loss: 0.1324806809425354\n",
      "epoch: 19 ;step: 200 loss: 0.13150739669799805\n",
      "epoch: 19 ;step: 300 loss: 0.12350215017795563\n",
      "epoch: 19 ;step: 400 loss: 0.12759409844875336\n",
      "epoch: 20 ;step: 0 loss: 0.12450220435857773\n",
      "epoch: 20 ;step: 100 loss: 0.13067646324634552\n",
      "epoch: 20 ;step: 200 loss: 0.12973877787590027\n",
      "epoch: 20 ;step: 300 loss: 0.12199077755212784\n",
      "epoch: 20 ;step: 400 loss: 0.12575647234916687\n",
      "epoch: 21 ;step: 0 loss: 0.12274664640426636\n",
      "epoch: 21 ;step: 100 loss: 0.12896224856376648\n",
      "epoch: 21 ;step: 200 loss: 0.12805168330669403\n",
      "epoch: 21 ;step: 300 loss: 0.12055009603500366\n",
      "epoch: 21 ;step: 400 loss: 0.12402491271495819\n",
      "epoch: 22 ;step: 0 loss: 0.12107856571674347\n",
      "epoch: 22 ;step: 100 loss: 0.1273261457681656\n",
      "epoch: 22 ;step: 200 loss: 0.1264369785785675\n",
      "epoch: 22 ;step: 300 loss: 0.11917398124933243\n",
      "epoch: 22 ;step: 400 loss: 0.12238427251577377\n",
      "epoch: 23 ;step: 0 loss: 0.11949323117733002\n",
      "epoch: 23 ;step: 100 loss: 0.1257631927728653\n",
      "epoch: 23 ;step: 200 loss: 0.12489570677280426\n",
      "epoch: 23 ;step: 300 loss: 0.11786055564880371\n",
      "epoch: 23 ;step: 400 loss: 0.12082679569721222\n",
      "epoch: 24 ;step: 0 loss: 0.11798230558633804\n",
      "epoch: 24 ;step: 100 loss: 0.12427350133657455\n",
      "epoch: 24 ;step: 200 loss: 0.12341960519552231\n",
      "epoch: 24 ;step: 300 loss: 0.11660482734441757\n",
      "epoch: 24 ;step: 400 loss: 0.11935128271579742\n",
      "epoch: 25 ;step: 0 loss: 0.11653882265090942\n",
      "epoch: 25 ;step: 100 loss: 0.12284709513187408\n",
      "epoch: 25 ;step: 200 loss: 0.12200567871332169\n",
      "epoch: 25 ;step: 300 loss: 0.1154034361243248\n",
      "epoch: 25 ;step: 400 loss: 0.1179497092962265\n",
      "epoch: 26 ;step: 0 loss: 0.11515418440103531\n",
      "epoch: 26 ;step: 100 loss: 0.1214742660522461\n",
      "epoch: 26 ;step: 200 loss: 0.12065336853265762\n",
      "epoch: 26 ;step: 300 loss: 0.11425097286701202\n",
      "epoch: 26 ;step: 400 loss: 0.11661704629659653\n",
      "epoch: 27 ;step: 0 loss: 0.1138262152671814\n",
      "epoch: 27 ;step: 100 loss: 0.12016125023365021\n",
      "epoch: 27 ;step: 200 loss: 0.1193598136305809\n",
      "epoch: 27 ;step: 300 loss: 0.11314551532268524\n",
      "epoch: 27 ;step: 400 loss: 0.11534802615642548\n",
      "epoch: 28 ;step: 0 loss: 0.11254757642745972\n",
      "epoch: 28 ;step: 100 loss: 0.11890149116516113\n",
      "epoch: 28 ;step: 200 loss: 0.11812057346105576\n",
      "epoch: 28 ;step: 300 loss: 0.11208442598581314\n",
      "epoch: 28 ;step: 400 loss: 0.11413564532995224\n",
      "epoch: 29 ;step: 0 loss: 0.1113208681344986\n",
      "epoch: 29 ;step: 100 loss: 0.11768971383571625\n",
      "epoch: 29 ;step: 200 loss: 0.1169317215681076\n",
      "epoch: 29 ;step: 300 loss: 0.1110660582780838\n",
      "epoch: 29 ;step: 400 loss: 0.11297722905874252\n",
      "epoch: 30 ;step: 0 loss: 0.11014046519994736\n",
      "epoch: 30 ;step: 100 loss: 0.116523876786232\n",
      "epoch: 30 ;step: 200 loss: 0.11578796058893204\n",
      "epoch: 30 ;step: 300 loss: 0.11008437722921371\n",
      "epoch: 30 ;step: 400 loss: 0.11186619102954865\n",
      "epoch: 31 ;step: 0 loss: 0.10900130122900009\n",
      "epoch: 31 ;step: 100 loss: 0.11540551483631134\n",
      "epoch: 31 ;step: 200 loss: 0.11469046771526337\n",
      "epoch: 31 ;step: 300 loss: 0.10913866758346558\n",
      "epoch: 31 ;step: 400 loss: 0.11079893261194229\n",
      "epoch: 32 ;step: 0 loss: 0.10790499299764633\n",
      "epoch: 32 ;step: 100 loss: 0.11433181911706924\n",
      "epoch: 32 ;step: 200 loss: 0.11363766342401505\n",
      "epoch: 32 ;step: 300 loss: 0.10822329670190811\n",
      "epoch: 32 ;step: 400 loss: 0.10977859795093536\n",
      "epoch: 33 ;step: 0 loss: 0.10685104131698608\n",
      "epoch: 33 ;step: 100 loss: 0.11329908668994904\n",
      "epoch: 33 ;step: 200 loss: 0.11262588202953339\n",
      "epoch: 33 ;step: 300 loss: 0.10733947902917862\n",
      "epoch: 33 ;step: 400 loss: 0.10880522429943085\n",
      "epoch: 34 ;step: 0 loss: 0.1058359146118164\n",
      "epoch: 34 ;step: 100 loss: 0.11230529844760895\n",
      "epoch: 34 ;step: 200 loss: 0.11165408045053482\n",
      "epoch: 34 ;step: 300 loss: 0.10648397356271744\n",
      "epoch: 34 ;step: 400 loss: 0.10786928981542587\n",
      "epoch: 35 ;step: 0 loss: 0.10485850274562836\n",
      "epoch: 35 ;step: 100 loss: 0.11134622246026993\n",
      "epoch: 35 ;step: 200 loss: 0.11071600019931793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 ;step: 300 loss: 0.10565695911645889\n",
      "epoch: 35 ;step: 400 loss: 0.10696756839752197\n",
      "epoch: 36 ;step: 0 loss: 0.10391615331172943\n",
      "epoch: 36 ;step: 100 loss: 0.11041946709156036\n",
      "epoch: 36 ;step: 200 loss: 0.1098112016916275\n",
      "epoch: 36 ;step: 300 loss: 0.10485969483852386\n",
      "epoch: 36 ;step: 400 loss: 0.1060977578163147\n",
      "epoch: 37 ;step: 0 loss: 0.10300582647323608\n",
      "epoch: 37 ;step: 100 loss: 0.10952291637659073\n",
      "epoch: 37 ;step: 200 loss: 0.10894407331943512\n",
      "epoch: 37 ;step: 300 loss: 0.10409121215343475\n",
      "epoch: 37 ;step: 400 loss: 0.10525895655155182\n",
      "epoch: 38 ;step: 0 loss: 0.10212542861700058\n",
      "epoch: 38 ;step: 100 loss: 0.10865753889083862\n",
      "epoch: 38 ;step: 200 loss: 0.10811004787683487\n",
      "epoch: 38 ;step: 300 loss: 0.10334771871566772\n",
      "epoch: 38 ;step: 400 loss: 0.10445298254489899\n",
      "epoch: 39 ;step: 0 loss: 0.10127701610326767\n",
      "epoch: 39 ;step: 100 loss: 0.10781899839639664\n",
      "epoch: 39 ;step: 200 loss: 0.10730072110891342\n",
      "epoch: 39 ;step: 300 loss: 0.10262502729892731\n",
      "epoch: 39 ;step: 400 loss: 0.10367368161678314\n",
      "epoch: 40 ;step: 0 loss: 0.10045770555734634\n",
      "epoch: 40 ;step: 100 loss: 0.10700734704732895\n",
      "epoch: 40 ;step: 200 loss: 0.10651751607656479\n",
      "epoch: 40 ;step: 300 loss: 0.10192456096410751\n",
      "epoch: 40 ;step: 400 loss: 0.10292066633701324\n",
      "epoch: 41 ;step: 0 loss: 0.09966325759887695\n",
      "epoch: 41 ;step: 100 loss: 0.10621976852416992\n",
      "epoch: 41 ;step: 200 loss: 0.10575796663761139\n",
      "epoch: 41 ;step: 300 loss: 0.10124435275793076\n",
      "epoch: 41 ;step: 400 loss: 0.10219260305166245\n",
      "epoch: 42 ;step: 0 loss: 0.09889162331819534\n",
      "epoch: 42 ;step: 100 loss: 0.10545644909143448\n",
      "epoch: 42 ;step: 200 loss: 0.10502064228057861\n",
      "epoch: 42 ;step: 300 loss: 0.1005825400352478\n",
      "epoch: 42 ;step: 400 loss: 0.10148745775222778\n",
      "epoch: 43 ;step: 0 loss: 0.09814313799142838\n",
      "epoch: 43 ;step: 100 loss: 0.10471446812152863\n",
      "epoch: 43 ;step: 200 loss: 0.10430379956960678\n",
      "epoch: 43 ;step: 300 loss: 0.09994026273488998\n",
      "epoch: 43 ;step: 400 loss: 0.10080405324697495\n",
      "epoch: 44 ;step: 0 loss: 0.09741733968257904\n",
      "epoch: 44 ;step: 100 loss: 0.10399677604436874\n",
      "epoch: 44 ;step: 200 loss: 0.10360655933618546\n",
      "epoch: 44 ;step: 300 loss: 0.0993172749876976\n",
      "epoch: 44 ;step: 400 loss: 0.10014212131500244\n",
      "epoch: 45 ;step: 0 loss: 0.09671226143836975\n",
      "epoch: 45 ;step: 100 loss: 0.10330174118280411\n",
      "epoch: 45 ;step: 200 loss: 0.10292986780405045\n",
      "epoch: 45 ;step: 300 loss: 0.09871076047420502\n",
      "epoch: 45 ;step: 400 loss: 0.09950096905231476\n",
      "epoch: 46 ;step: 0 loss: 0.09602818638086319\n",
      "epoch: 46 ;step: 100 loss: 0.10262852907180786\n",
      "epoch: 46 ;step: 200 loss: 0.10227243602275848\n",
      "epoch: 46 ;step: 300 loss: 0.0981188714504242\n",
      "epoch: 46 ;step: 400 loss: 0.09888015687465668\n",
      "epoch: 47 ;step: 0 loss: 0.09536340087652206\n",
      "epoch: 47 ;step: 100 loss: 0.10197534412145615\n",
      "epoch: 47 ;step: 200 loss: 0.10163374245166779\n",
      "epoch: 47 ;step: 300 loss: 0.09754090011119843\n",
      "epoch: 47 ;step: 400 loss: 0.09827816486358643\n",
      "epoch: 48 ;step: 0 loss: 0.09471829980611801\n",
      "epoch: 48 ;step: 100 loss: 0.10133995860815048\n",
      "epoch: 48 ;step: 200 loss: 0.10101310163736343\n",
      "epoch: 48 ;step: 300 loss: 0.09697644412517548\n",
      "epoch: 48 ;step: 400 loss: 0.09769201278686523\n",
      "epoch: 49 ;step: 0 loss: 0.09409081935882568\n",
      "epoch: 49 ;step: 100 loss: 0.10072274506092072\n",
      "epoch: 49 ;step: 200 loss: 0.10040916502475739\n",
      "epoch: 49 ;step: 300 loss: 0.09642694145441055\n",
      "epoch: 49 ;step: 400 loss: 0.0971234068274498\n",
      "epoch: 50 ;step: 0 loss: 0.0934797152876854\n",
      "epoch: 50 ;step: 100 loss: 0.1001250371336937\n",
      "epoch: 50 ;step: 200 loss: 0.09982097148895264\n",
      "epoch: 50 ;step: 300 loss: 0.09589125216007233\n",
      "epoch: 50 ;step: 400 loss: 0.09657041728496552\n",
      "epoch: 51 ;step: 0 loss: 0.09288366883993149\n",
      "epoch: 51 ;step: 100 loss: 0.09954487532377243\n",
      "epoch: 51 ;step: 200 loss: 0.09925027191638947\n",
      "epoch: 51 ;step: 300 loss: 0.0953693613409996\n",
      "epoch: 51 ;step: 400 loss: 0.09603219479322433\n",
      "epoch: 52 ;step: 0 loss: 0.09230262041091919\n",
      "epoch: 52 ;step: 100 loss: 0.09898322075605392\n",
      "epoch: 52 ;step: 200 loss: 0.09869494289159775\n",
      "epoch: 52 ;step: 300 loss: 0.09486109763383865\n",
      "epoch: 52 ;step: 400 loss: 0.09550575911998749\n",
      "epoch: 53 ;step: 0 loss: 0.09173408895730972\n",
      "epoch: 53 ;step: 100 loss: 0.09843701869249344\n",
      "epoch: 53 ;step: 200 loss: 0.09815235435962677\n",
      "epoch: 53 ;step: 300 loss: 0.09436636418104172\n",
      "epoch: 53 ;step: 400 loss: 0.09499204158782959\n",
      "epoch: 54 ;step: 0 loss: 0.09117726981639862\n",
      "epoch: 54 ;step: 100 loss: 0.09790552407503128\n",
      "epoch: 54 ;step: 200 loss: 0.09762310981750488\n",
      "epoch: 54 ;step: 300 loss: 0.093882255256176\n",
      "epoch: 54 ;step: 400 loss: 0.09449072927236557\n",
      "epoch: 55 ;step: 0 loss: 0.09063156694173813\n",
      "epoch: 55 ;step: 100 loss: 0.09738817065954208\n",
      "epoch: 55 ;step: 200 loss: 0.09710836410522461\n",
      "epoch: 55 ;step: 300 loss: 0.0934106782078743\n",
      "epoch: 55 ;step: 400 loss: 0.09399978071451187\n",
      "epoch: 56 ;step: 0 loss: 0.09009867161512375\n",
      "epoch: 56 ;step: 100 loss: 0.09688453376293182\n",
      "epoch: 56 ;step: 200 loss: 0.09660626947879791\n",
      "epoch: 56 ;step: 300 loss: 0.09294810146093369\n",
      "epoch: 56 ;step: 400 loss: 0.0935208797454834\n",
      "epoch: 57 ;step: 0 loss: 0.08957859873771667\n",
      "epoch: 57 ;step: 100 loss: 0.09639426320791245\n",
      "epoch: 57 ;step: 200 loss: 0.09611561894416809\n",
      "epoch: 57 ;step: 300 loss: 0.09249557554721832\n",
      "epoch: 57 ;step: 400 loss: 0.09305283427238464\n",
      "epoch: 58 ;step: 0 loss: 0.08906838297843933\n",
      "epoch: 58 ;step: 100 loss: 0.09591750800609589\n",
      "epoch: 58 ;step: 200 loss: 0.09563515335321426\n",
      "epoch: 58 ;step: 300 loss: 0.09205272048711777\n",
      "epoch: 58 ;step: 400 loss: 0.09259773790836334\n",
      "epoch: 59 ;step: 0 loss: 0.08856873214244843\n",
      "epoch: 59 ;step: 100 loss: 0.09545131772756577\n",
      "epoch: 59 ;step: 200 loss: 0.09516690671443939\n",
      "epoch: 59 ;step: 300 loss: 0.09161925315856934\n",
      "epoch: 59 ;step: 400 loss: 0.09215418994426727\n",
      "epoch: 60 ;step: 0 loss: 0.08807960897684097\n",
      "epoch: 60 ;step: 100 loss: 0.09499327838420868\n",
      "epoch: 60 ;step: 200 loss: 0.0947098433971405\n",
      "epoch: 60 ;step: 300 loss: 0.09119348227977753\n",
      "epoch: 60 ;step: 400 loss: 0.09172133356332779\n",
      "epoch: 61 ;step: 0 loss: 0.08760161697864532\n",
      "epoch: 61 ;step: 100 loss: 0.09454606473445892\n",
      "epoch: 61 ;step: 200 loss: 0.09426198899745941\n",
      "epoch: 61 ;step: 300 loss: 0.09077586233615875\n",
      "epoch: 61 ;step: 400 loss: 0.09129844605922699\n",
      "epoch: 62 ;step: 0 loss: 0.08713368326425552\n",
      "epoch: 62 ;step: 100 loss: 0.09410927444696426\n",
      "epoch: 62 ;step: 200 loss: 0.09382513910531998\n",
      "epoch: 62 ;step: 300 loss: 0.09036818146705627\n",
      "epoch: 62 ;step: 400 loss: 0.09088645875453949\n",
      "epoch: 63 ;step: 0 loss: 0.08667536079883575\n",
      "epoch: 63 ;step: 100 loss: 0.0936809554696083\n",
      "epoch: 63 ;step: 200 loss: 0.09339882433414459\n",
      "epoch: 63 ;step: 300 loss: 0.08996907621622086\n",
      "epoch: 63 ;step: 400 loss: 0.090485118329525\n",
      "epoch: 64 ;step: 0 loss: 0.08622614294290543\n",
      "epoch: 64 ;step: 100 loss: 0.09326130896806717\n",
      "epoch: 64 ;step: 200 loss: 0.0929826945066452\n",
      "epoch: 64 ;step: 300 loss: 0.08957970142364502\n",
      "epoch: 64 ;step: 400 loss: 0.09009338915348053\n",
      "epoch: 65 ;step: 0 loss: 0.0857851654291153\n",
      "epoch: 65 ;step: 100 loss: 0.09285137057304382\n",
      "epoch: 65 ;step: 200 loss: 0.09257595986127853\n",
      "epoch: 65 ;step: 300 loss: 0.08919723331928253\n",
      "epoch: 65 ;step: 400 loss: 0.08970978111028671\n",
      "epoch: 66 ;step: 0 loss: 0.0853518396615982\n",
      "epoch: 66 ;step: 100 loss: 0.09244921803474426\n",
      "epoch: 66 ;step: 200 loss: 0.09217886626720428\n",
      "epoch: 66 ;step: 300 loss: 0.08882167190313339\n",
      "epoch: 66 ;step: 400 loss: 0.08933502435684204\n",
      "epoch: 67 ;step: 0 loss: 0.0849265605211258\n",
      "epoch: 67 ;step: 100 loss: 0.09205280989408493\n",
      "epoch: 67 ;step: 200 loss: 0.09179069846868515\n",
      "epoch: 67 ;step: 300 loss: 0.08845355361700058\n",
      "epoch: 67 ;step: 400 loss: 0.08897072076797485\n",
      "epoch: 68 ;step: 0 loss: 0.08450973778963089\n",
      "epoch: 68 ;step: 100 loss: 0.09166396409273148\n",
      "epoch: 68 ;step: 200 loss: 0.09141082316637039\n",
      "epoch: 68 ;step: 300 loss: 0.08809255808591843\n",
      "epoch: 68 ;step: 400 loss: 0.08861546218395233\n",
      "epoch: 69 ;step: 0 loss: 0.08410140126943588\n",
      "epoch: 69 ;step: 100 loss: 0.091281458735466\n",
      "epoch: 69 ;step: 200 loss: 0.09104060381650925\n",
      "epoch: 69 ;step: 300 loss: 0.08773935586214066\n",
      "epoch: 69 ;step: 400 loss: 0.0882694274187088\n",
      "epoch: 70 ;step: 0 loss: 0.08370132744312286\n",
      "epoch: 70 ;step: 100 loss: 0.09090603142976761\n",
      "epoch: 70 ;step: 200 loss: 0.09067771583795547\n",
      "epoch: 70 ;step: 300 loss: 0.08739500492811203\n",
      "epoch: 70 ;step: 400 loss: 0.08793049305677414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 71 ;step: 0 loss: 0.08330849558115005\n",
      "epoch: 71 ;step: 100 loss: 0.09053897857666016\n",
      "epoch: 71 ;step: 200 loss: 0.09032091498374939\n",
      "epoch: 71 ;step: 300 loss: 0.0870562344789505\n",
      "epoch: 71 ;step: 400 loss: 0.08759820461273193\n",
      "epoch: 72 ;step: 0 loss: 0.08292277157306671\n",
      "epoch: 72 ;step: 100 loss: 0.09017948806285858\n",
      "epoch: 72 ;step: 200 loss: 0.08997128903865814\n",
      "epoch: 72 ;step: 300 loss: 0.08672411739826202\n",
      "epoch: 72 ;step: 400 loss: 0.08727283775806427\n",
      "epoch: 73 ;step: 0 loss: 0.08254468441009521\n",
      "epoch: 73 ;step: 100 loss: 0.08982620388269424\n",
      "epoch: 73 ;step: 200 loss: 0.08962903171777725\n",
      "epoch: 73 ;step: 300 loss: 0.08639834821224213\n",
      "epoch: 73 ;step: 400 loss: 0.08695326000452042\n",
      "epoch: 74 ;step: 0 loss: 0.08217326551675797\n",
      "epoch: 74 ;step: 100 loss: 0.08947883546352386\n",
      "epoch: 74 ;step: 200 loss: 0.08929289132356644\n",
      "epoch: 74 ;step: 300 loss: 0.08607786148786545\n",
      "epoch: 74 ;step: 400 loss: 0.08663970977067947\n",
      "epoch: 75 ;step: 0 loss: 0.08180824667215347\n",
      "epoch: 75 ;step: 100 loss: 0.08913697302341461\n",
      "epoch: 75 ;step: 200 loss: 0.08896143734455109\n",
      "epoch: 75 ;step: 300 loss: 0.08576290309429169\n",
      "epoch: 75 ;step: 400 loss: 0.08633027225732803\n",
      "epoch: 76 ;step: 0 loss: 0.08144982159137726\n",
      "epoch: 76 ;step: 100 loss: 0.0888013020157814\n",
      "epoch: 76 ;step: 200 loss: 0.08863531053066254\n",
      "epoch: 76 ;step: 300 loss: 0.08545279502868652\n",
      "epoch: 76 ;step: 400 loss: 0.08602627366781235\n",
      "epoch: 77 ;step: 0 loss: 0.08109738677740097\n",
      "epoch: 77 ;step: 100 loss: 0.08847132325172424\n",
      "epoch: 77 ;step: 200 loss: 0.08831419050693512\n",
      "epoch: 77 ;step: 300 loss: 0.08514727652072906\n",
      "epoch: 77 ;step: 400 loss: 0.08572739362716675\n",
      "epoch: 78 ;step: 0 loss: 0.08075187355279922\n",
      "epoch: 78 ;step: 100 loss: 0.08814647048711777\n",
      "epoch: 78 ;step: 200 loss: 0.0880005806684494\n",
      "epoch: 78 ;step: 300 loss: 0.0848466008901596\n",
      "epoch: 78 ;step: 400 loss: 0.08543239533901215\n",
      "epoch: 79 ;step: 0 loss: 0.08041234314441681\n",
      "epoch: 79 ;step: 100 loss: 0.0878276377916336\n",
      "epoch: 79 ;step: 200 loss: 0.0876922458410263\n",
      "epoch: 79 ;step: 300 loss: 0.08455102890729904\n",
      "epoch: 79 ;step: 400 loss: 0.08514206856489182\n",
      "epoch: 80 ;step: 0 loss: 0.0800778716802597\n",
      "epoch: 80 ;step: 100 loss: 0.08751425892114639\n",
      "epoch: 80 ;step: 200 loss: 0.08738850057125092\n",
      "epoch: 80 ;step: 300 loss: 0.08425933122634888\n",
      "epoch: 80 ;step: 400 loss: 0.08485713601112366\n",
      "epoch: 81 ;step: 0 loss: 0.0797484740614891\n",
      "epoch: 81 ;step: 100 loss: 0.08720587939023972\n",
      "epoch: 81 ;step: 200 loss: 0.08709017187356949\n",
      "epoch: 81 ;step: 300 loss: 0.08397234231233597\n",
      "epoch: 81 ;step: 400 loss: 0.08457641303539276\n",
      "epoch: 82 ;step: 0 loss: 0.07942461222410202\n",
      "epoch: 82 ;step: 100 loss: 0.08690229058265686\n",
      "epoch: 82 ;step: 200 loss: 0.08679615706205368\n",
      "epoch: 82 ;step: 300 loss: 0.0836893692612648\n",
      "epoch: 82 ;step: 400 loss: 0.08430013805627823\n",
      "epoch: 83 ;step: 0 loss: 0.07910578697919846\n",
      "epoch: 83 ;step: 100 loss: 0.08660299330949783\n",
      "epoch: 83 ;step: 200 loss: 0.08650617301464081\n",
      "epoch: 83 ;step: 300 loss: 0.08340992778539658\n",
      "epoch: 83 ;step: 400 loss: 0.0840281993150711\n",
      "epoch: 84 ;step: 0 loss: 0.07879173010587692\n",
      "epoch: 84 ;step: 100 loss: 0.08630777895450592\n",
      "epoch: 84 ;step: 200 loss: 0.08622079342603683\n",
      "epoch: 84 ;step: 300 loss: 0.08313417434692383\n",
      "epoch: 84 ;step: 400 loss: 0.08376019448041916\n",
      "epoch: 85 ;step: 0 loss: 0.07848335057497025\n",
      "epoch: 85 ;step: 100 loss: 0.08601582050323486\n",
      "epoch: 85 ;step: 200 loss: 0.08593909442424774\n",
      "epoch: 85 ;step: 300 loss: 0.08286264538764954\n",
      "epoch: 85 ;step: 400 loss: 0.0834960862994194\n",
      "epoch: 86 ;step: 0 loss: 0.07818108797073364\n",
      "epoch: 86 ;step: 100 loss: 0.08572746813297272\n",
      "epoch: 86 ;step: 200 loss: 0.0856608897447586\n",
      "epoch: 86 ;step: 300 loss: 0.08259507268667221\n",
      "epoch: 86 ;step: 400 loss: 0.08323647826910019\n",
      "epoch: 87 ;step: 0 loss: 0.07788334786891937\n",
      "epoch: 87 ;step: 100 loss: 0.08544269949197769\n",
      "epoch: 87 ;step: 200 loss: 0.08538676798343658\n",
      "epoch: 87 ;step: 300 loss: 0.0823313444852829\n",
      "epoch: 87 ;step: 400 loss: 0.0829809233546257\n",
      "epoch: 88 ;step: 0 loss: 0.0775899887084961\n",
      "epoch: 88 ;step: 100 loss: 0.08516129106283188\n",
      "epoch: 88 ;step: 200 loss: 0.08511723577976227\n",
      "epoch: 88 ;step: 300 loss: 0.08207134902477264\n",
      "epoch: 88 ;step: 400 loss: 0.08272888511419296\n",
      "epoch: 89 ;step: 0 loss: 0.07730107009410858\n",
      "epoch: 89 ;step: 100 loss: 0.08488474786281586\n",
      "epoch: 89 ;step: 200 loss: 0.08485157787799835\n",
      "epoch: 89 ;step: 300 loss: 0.08181556314229965\n",
      "epoch: 89 ;step: 400 loss: 0.08247953653335571\n",
      "epoch: 90 ;step: 0 loss: 0.07701648026704788\n",
      "epoch: 90 ;step: 100 loss: 0.08461257070302963\n",
      "epoch: 90 ;step: 200 loss: 0.08458930999040604\n",
      "epoch: 90 ;step: 300 loss: 0.08156392723321915\n",
      "epoch: 90 ;step: 400 loss: 0.0822334885597229\n",
      "epoch: 91 ;step: 0 loss: 0.07673622667789459\n",
      "epoch: 91 ;step: 100 loss: 0.08434436470270157\n",
      "epoch: 91 ;step: 200 loss: 0.08433139324188232\n",
      "epoch: 91 ;step: 300 loss: 0.08131665736436844\n",
      "epoch: 91 ;step: 400 loss: 0.08199137449264526\n",
      "epoch: 92 ;step: 0 loss: 0.07645964622497559\n",
      "epoch: 92 ;step: 100 loss: 0.08407944440841675\n",
      "epoch: 92 ;step: 200 loss: 0.08407770097255707\n",
      "epoch: 92 ;step: 300 loss: 0.08107324689626694\n",
      "epoch: 92 ;step: 400 loss: 0.08175242692232132\n",
      "epoch: 93 ;step: 0 loss: 0.07618682086467743\n",
      "epoch: 93 ;step: 100 loss: 0.08381769061088562\n",
      "epoch: 93 ;step: 200 loss: 0.0838281512260437\n",
      "epoch: 93 ;step: 300 loss: 0.08083298057317734\n",
      "epoch: 93 ;step: 400 loss: 0.08151732385158539\n",
      "epoch: 94 ;step: 0 loss: 0.07591784000396729\n",
      "epoch: 94 ;step: 100 loss: 0.08355891704559326\n",
      "epoch: 94 ;step: 200 loss: 0.0835825726389885\n",
      "epoch: 94 ;step: 300 loss: 0.0805959478020668\n",
      "epoch: 94 ;step: 400 loss: 0.08128523081541061\n",
      "epoch: 95 ;step: 0 loss: 0.07565351575613022\n",
      "epoch: 95 ;step: 100 loss: 0.08330361545085907\n",
      "epoch: 95 ;step: 200 loss: 0.0833401307463646\n",
      "epoch: 95 ;step: 300 loss: 0.08036188781261444\n",
      "epoch: 95 ;step: 400 loss: 0.08105552196502686\n",
      "epoch: 96 ;step: 0 loss: 0.07539330422878265\n",
      "epoch: 96 ;step: 100 loss: 0.08305124938488007\n",
      "epoch: 96 ;step: 200 loss: 0.08310098201036453\n",
      "epoch: 96 ;step: 300 loss: 0.08013094216585159\n",
      "epoch: 96 ;step: 400 loss: 0.08082878589630127\n",
      "epoch: 97 ;step: 0 loss: 0.07513587176799774\n",
      "epoch: 97 ;step: 100 loss: 0.08280237764120102\n",
      "epoch: 97 ;step: 200 loss: 0.08286450058221817\n",
      "epoch: 97 ;step: 300 loss: 0.07990290969610214\n",
      "epoch: 97 ;step: 400 loss: 0.08060510456562042\n",
      "epoch: 98 ;step: 0 loss: 0.07488171756267548\n",
      "epoch: 98 ;step: 100 loss: 0.08255789428949356\n",
      "epoch: 98 ;step: 200 loss: 0.08263152092695236\n",
      "epoch: 98 ;step: 300 loss: 0.07967755198478699\n",
      "epoch: 98 ;step: 400 loss: 0.08038468658924103\n",
      "epoch: 99 ;step: 0 loss: 0.07463134825229645\n",
      "epoch: 99 ;step: 100 loss: 0.08231619745492935\n",
      "epoch: 99 ;step: 200 loss: 0.08240184187889099\n",
      "epoch: 99 ;step: 300 loss: 0.07945466041564941\n",
      "epoch: 99 ;step: 400 loss: 0.08016666769981384\n"
     ]
    }
   ],
   "source": [
    "lr=1e-4\n",
    "for epoch in range(100):\n",
    "    for step,(x,y) in enumerate(train_db): \n",
    "        x=tf.reshape(x,[-1,28*28])\n",
    "        with tf.GradientTape() as tape:\n",
    "            h1=x@w1+tf.broadcast_to(b1,[x.shape[0],256]) \n",
    "            h1=tf.nn.relu(h1)\n",
    "            h2=h1@w2+b2\n",
    "            h2=tf.nn.relu(h2)\n",
    "            out=h2@w3+b3 \n",
    "            y_onehot=tf.one_hot(y,depth=10) \n",
    "            loss=tf.square(y_onehot-out)\n",
    "            loss=tf.reduce_mean(loss)\n",
    "        # compute gradients \n",
    "        grads=tape.gradient(loss,[w1,b1,w2,b2,w3,b3])  \n",
    "        w1.assign_sub(lr*grads[0])\n",
    "        b1.assign_sub(lr*grads[1])\n",
    "        w2.assign_sub(lr*grads[2])\n",
    "        b2.assign_sub(lr*grads[3])\n",
    "        w3.assign_sub(lr*grads[4])\n",
    "        b3.assign_sub(lr*grads[5]) \n",
    "        if step%100==0:\n",
    "            print(\"epoch:\",epoch,\";step:\",step,\"loss:\",float(loss)) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-hawaii",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-bullet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
