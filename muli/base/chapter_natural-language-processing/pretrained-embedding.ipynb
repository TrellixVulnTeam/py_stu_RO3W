{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用预训练的词向量\n",
    "\n",
    "本节介绍如何通过`mxnet.contrib.text`使用预训练的词向量。需要注意的是，`mxnet.contrib.text`正在测试中并可能在未来有改动。如有改动，本节内容会作相应更新。\n",
    "\n",
    "本节使用的预训练的GloVe和fastText词向量分别来自：\n",
    "\n",
    "* GloVe项目网站：https://nlp.stanford.edu/projects/glove/\n",
    "* fastText项目网站：https://fasttext.cc/\n",
    "\n",
    "我们先载入需要的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "from mxnet.contrib import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 由数据集建立词典和载入词向量——以fastText为例\n",
    "\n",
    "看一下fastText前五个预训练的词向量。它们分别从不同语言的Wikipedia数据集训练得到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crawl-300d-2M.vec',\n",
       " 'wiki.aa.vec',\n",
       " 'wiki.ab.vec',\n",
       " 'wiki.ace.vec',\n",
       " 'wiki.ady.vec',\n",
       " 'wiki.af.vec',\n",
       " 'wiki.ak.vec',\n",
       " 'wiki.als.vec',\n",
       " 'wiki.am.vec',\n",
       " 'wiki.ang.vec']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.embedding.get_pretrained_file_names('fasttext')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 访问词向量 \n",
    "为了演示方便，我们创建一个很小的文本数据集，并计算词频。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data=' hello world \\n hello nice world \\n hi world \\n'\n",
    "counter=text.utils.count_tokens_from_str(text_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hello': 2, 'world': 3, 'nice': 1, 'hi': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先根据数据集建立词典，并为该词典中的词载入fastText 词向量。这里使用 Simple English 的预训练词向量。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dske/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/contrib/text/embedding.py:279: UserWarning: At line 1 of the pre-trained text embedding file: token 111051 with 1-dimensional vector [300.0] is likely a header and is skipped.\n",
      "  'skipped.' % (line_num, token, elems))\n"
     ]
    }
   ],
   "source": [
    "my_vocab=text.vocab.Vocabulary(counter) \n",
    "my_embedding=text.embedding.create(\n",
    "'fasttext',pretrained_file_name='wiki.simple.vec',vocabulary=my_vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedding.unknown_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "<NDArray 10 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedding.get_vecs_by_tokens('beautiful')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_embedding) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意一个词典意外以外的词向量默认为零向量。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "<NDArray 300 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedding.get_vecs_by_tokens('beautiful') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 3.9567e-01  2.1454e-01 -3.5389e-02 -2.4299e-01 -9.5645e-02  7.0196e-01\n",
       " -2.2217e-01  5.9566e-01  3.5400e-01  1.1223e-01  1.7507e-01 -1.8420e-01\n",
       " -4.5743e-01  2.0431e-01 -6.8720e-02 -7.5822e-03 -3.1745e-02 -1.2260e-01\n",
       " -8.4278e-02 -4.0954e-01  1.5536e-01 -1.0237e-01  1.6776e-01  7.2908e-02\n",
       " -3.3974e-01  4.0964e-01  3.1588e-01 -3.0134e-01  4.5805e-01  5.0324e-02\n",
       " -5.4599e-01 -4.3832e-02 -1.1785e-01  2.2829e-01  1.9781e-01  2.7429e-01\n",
       "  1.5484e-01 -1.6361e-02  5.6247e-01 -2.8905e-02  3.2125e-01 -2.2172e-01\n",
       " -1.8451e-01 -9.5252e-02 -3.7475e-01 -3.0096e-03 -1.7133e-01 -2.4177e-02\n",
       " -4.2394e-02  5.5292e-02 -3.6133e-01  7.9608e-02 -7.3816e-02 -1.3945e-01\n",
       " -5.7920e-02  6.0948e-03  3.1616e-01 -6.9531e-02 -4.1851e-01 -7.8901e-02\n",
       " -1.9404e-01 -1.6711e-02 -4.2909e-01 -3.2553e-02 -4.0582e-01 -1.8679e-01\n",
       "  2.3638e-02 -3.6106e-01 -1.1680e-04  6.6403e-02  1.0071e-01  2.1626e-01\n",
       " -1.2835e-01  3.6103e-01 -1.3715e-01 -6.1190e-02 -3.8188e-01 -2.8119e-01\n",
       "  3.6750e-01  7.1560e-03  5.0234e-01 -1.1317e-01 -1.5920e-01  4.4633e-02\n",
       " -3.0035e-01 -2.1490e-01  2.3501e-01 -2.2151e-01  9.2836e-02 -8.1031e-01\n",
       " -4.6148e-01  7.8923e-02 -4.9757e-01 -2.5092e-02  4.4697e-01  2.4795e-01\n",
       " -7.1856e-01 -1.5004e-01  1.1904e-01 -2.0064e-01  1.3000e-01 -6.2002e-02\n",
       " -1.7859e-02 -6.6152e-03 -4.8471e-01  8.1129e-02 -5.3102e-01  4.1571e-02\n",
       " -4.6091e-02 -2.0755e-01 -1.4421e-01 -2.5514e-01  6.7258e-02 -1.0041e-01\n",
       " -2.2771e-01 -2.9983e-01  1.5760e-01  7.8602e-01 -4.7287e-01  7.8859e-02\n",
       "  6.9349e-02  4.8157e-01 -3.8714e-02  6.4797e-02 -4.8402e-01 -8.2103e-01\n",
       "  1.1725e-01  4.1851e-02  5.4920e-02 -1.8430e-01  1.1429e-01  1.7205e-01\n",
       " -7.3599e-02 -1.6165e-02 -3.4740e-01 -6.0199e-02  2.5808e-01 -1.3113e-01\n",
       " -1.6971e-01 -4.2851e-01 -6.5230e-01  1.8635e-01 -2.3473e-01  1.2852e-01\n",
       "  1.9721e-01  8.0693e-02 -5.2418e-01 -2.2415e-01  1.1740e-01 -7.6664e-02\n",
       " -2.2182e-01  8.9106e-02  2.0090e-01  1.9626e-02  4.0090e-01  1.5189e-01\n",
       " -6.7135e-02  2.2292e-02  1.2248e-01 -1.7770e-01 -2.3174e-01 -3.5582e-01\n",
       "  1.5086e-01 -8.8843e-02  3.0459e-02  4.1640e-02  5.2011e-02 -4.3609e-01\n",
       "  5.0947e-01  1.8445e-01 -2.2596e-01 -9.7855e-02 -4.6621e-01  7.7472e-02\n",
       "  2.0312e-02  1.0437e-02 -1.9767e-01 -1.4682e-01  3.9875e-01 -1.4261e-01\n",
       " -6.3095e-01  3.9198e-02 -3.2454e-01  2.7128e-01  2.6706e-01  1.8585e-02\n",
       "  2.1183e-01 -7.2323e-02  1.2289e-01  6.4784e-01  1.4635e-01  9.6362e-02\n",
       "  2.5686e-01 -7.2680e-02 -3.7782e-01 -4.3519e-01 -8.7646e-01 -1.9399e-01\n",
       "  6.2858e-02 -3.0186e-01  3.5454e-01  2.6528e-01  2.1853e-01  1.5688e-01\n",
       " -5.0348e-01  1.7768e-01  7.4341e-02  4.5868e-01 -3.1830e-01 -2.0047e-01\n",
       " -1.6847e-01 -9.8182e-02  4.9339e-01 -5.0095e-01 -1.9910e-01  2.1468e-01\n",
       " -1.8846e-01 -3.2755e-01 -3.5421e-01  3.5366e-01 -2.9484e-01 -2.1658e-02\n",
       " -4.8068e-01  1.1578e-01  1.6799e-01 -1.2026e-01 -8.6954e-03  2.6344e-01\n",
       " -4.3505e-02  1.5306e-01  5.8245e-02  4.8804e-01 -3.1821e-01  1.6461e-01\n",
       " -3.4359e-01  1.7962e-01 -3.1692e-01  1.0989e-02  1.4989e-01 -3.1745e-02\n",
       "  2.9266e-01 -2.4258e-03 -2.2247e-01  2.4630e-01  6.7556e-01 -1.7419e-01\n",
       "  6.9367e-01 -2.1804e-01  3.2332e-02  3.6521e-02 -1.8767e-01  5.1153e-01\n",
       " -6.0152e-01 -8.0548e-02 -3.0559e-02 -1.4123e-02 -1.3207e-01  3.6317e-01\n",
       " -4.3494e-01  8.0458e-02 -3.2099e-01 -4.3120e-01  2.6166e-01  4.2963e-01\n",
       " -7.9263e-02 -1.9196e-01  1.6546e-01  9.6910e-02 -1.6433e-01 -7.8967e-01\n",
       " -3.0059e-02 -1.5591e-01  3.6001e-01 -2.7975e-02  1.4566e-01  3.1766e-01\n",
       " -7.3518e-02  5.6064e-01  3.4178e-01 -3.0879e-01  2.6112e-01 -1.3669e-01\n",
       " -1.2612e-01 -6.3995e-01  5.6239e-02  3.3859e-01  4.1397e-02  5.4060e-02\n",
       "  1.6268e-01  5.1144e-01 -6.6267e-02 -2.6571e-01 -5.1054e-01 -4.2417e-01\n",
       "  7.4714e-01  4.0900e-02 -7.5418e-01 -3.1443e-01  2.4018e-02 -7.6101e-02]\n",
       "<NDArray 300 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedding.get_vecs_by_tokens('hello') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下数据集中两个词'hello' 和 ‘world' 词向量的形状.fastText 中每个词均使用300维的词向量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedding.get_vecs_by_tokens(['hello','world','beautiful']).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印‘hello'和 'world'词向量前5个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.39567   0.21454  -0.035389 -0.24299  -0.095645]\n",
       " [ 0.10444  -0.10858   0.27212   0.13299  -0.33165 ]\n",
       " [ 0.        0.        0.        0.        0.      ]]\n",
       "<NDArray 3x5 @cpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedding.get_vecs_by_tokens(['hello','world','beautiful'])[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedding.to_indices(['hello','world','beautiful','boy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用预训练词向量初始化gluon.nn.Embedding \n",
    "我们可以使用预训练的词向量初始化 gluon.nn.Embedding。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=gluon.nn.Embedding(len(my_embedding),my_embedding.vec_len)\n",
    "layer.initialize() \n",
    "layer.weight.set_data(my_embedding.idx_to_vec) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用词典中 'hello'和‘world'两个词典中的索引，我们可以通过gluon.nn.Embedding得到它们的词向量。并向神经网络的下一层传递。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.39567   0.21454  -0.035389 -0.24299  -0.095645]\n",
       " [ 0.10444  -0.10858   0.27212   0.13299  -0.33165 ]]\n",
       "<NDArray 2x5 @cpu(0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " layer(nd.array([2,1]))[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 由预训练向量建立词典--以 GloVe为例 \n",
    "看一下Glove前5个预训练的词向量 。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.42B.300d.txt',\n",
       " 'glove.6B.50d.txt',\n",
       " 'glove.6B.100d.txt',\n",
       " 'glove.6B.200d.txt',\n",
       " 'glove.6B.300d.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.embedding.get_pretrained_file_names('glove')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了演示简单，我们使用小一点的词向量，例如50维。这里不再传入根据数据集建立的词典，而是直接使用预训练词向量中的词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "glove_6b50d=text.embedding.create('glove',\n",
    "                                 pretrained_file_name='glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下这个词典多大。注意其中包含一个特殊的未知词符号。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400001\n"
     ]
    }
   ],
   "source": [
    "print(len(glove_6b50d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以访问词向量的属性 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3367\n",
      "beautiful\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# 词到索引。 \n",
    "print(glove_6b50d.token_to_idx['beautiful']) \n",
    "# 索引 到词 。 \n",
    "print(glove_6b50d.idx_to_token[3367]) \n",
    "# 词向量长度 \n",
    "print(glove_6b50d.vec_len) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预训练词向量的应用 -- 以 GloVe 为例 \n",
    "为了应用训练词向量，我们需要定义余弦相似度。它可以比较两个向量之间的相似度。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "def cos_sim(x,y):\n",
    "    return nd.dot(x,y)/(nd.norm(x)*nd.norm(y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "余弦相似度的值域在-1到1之间。余弦相似度越大，两个向量越接近 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-1.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x=nd.array([1,2]) \n",
    "y=nd.array([10,20]) \n",
    "z=nd.array([-1,-2]) \n",
    "print(cos_sim(x,z)) \n",
    "print(cos_sim(x,y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 求近似词\n",
    "\n",
    "给定任意词，我们可以从整个词典（大小40万，不含未知词符号）中找出与它最接近的$k$个词（$k$ nearest neighbors）。词与词之间的相似度可以用两个词向量的余弦相似度表示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vecs_by_row(x):\n",
    "    return x/nd.sqrt(nd.sum(x*x,axis=1)).reshape((-1,1)) \n",
    "\n",
    "def get_knn(token_embedding,k,word):\n",
    "    word_vec=token_embedding.get_vecs_by_tokens([word]).reshape((-1,1)) \n",
    "    vocab_vecs=norm_vecs_by_row(token_embedding.idx_to_vec) \n",
    "    dot_prod=nd.dot(vocab_vecs,word_vec) \n",
    "    indices=nd.topk(dot_prod.reshape((len(token_embedding),)),k=k+2,\n",
    "                   ret_typ='indices') \n",
    "    indices=[int(i.asscalar()) for i in indices]\n",
    "    print('dot_prod',dot_prod) \n",
    "    print('dot_prod.dtype',type(dot_prod) )\n",
    "    print('indices',indices)\n",
    "    print('indices.dtype',type(indices))\n",
    "    \n",
    "    # 除去未知词和输入词 \n",
    "    return token_embedding.to_tokens(indices[2:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查找词典中与'bady'最接近的5个词 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_prod \n",
      "[[        nan]\n",
      " [ 2.1890206 ]\n",
      " [ 2.6500103 ]\n",
      " ...\n",
      " [-0.90959346]\n",
      " [-2.5016034 ]\n",
      " [-2.2118573 ]]\n",
      "<NDArray 400001x1 @cpu(0)>\n",
      "dot_prod.dtype <class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "indices [2233, 0, 3, 7, 6325, 1607, 1750]\n",
      "indices.dtype <class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.', 'in', 'babies', 'boy', 'girl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(glove_6b50d,5,'baby') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证一下baby和 babies 的两个词向量之间的余弦相似度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.83871305]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(glove_6b50d.get_vecs_by_tokens('baby'),\n",
    "       glove_6b50d.get_vecs_by_tokens('babies'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查找词典中与 'compters'最接近的5个词 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_prod \n",
      "[[       nan]\n",
      " [ 2.4114583]\n",
      " [ 2.5894804]\n",
      " ...\n",
      " [-1.316555 ]\n",
      " [-1.8794672]\n",
      " [-1.5496533]]\n",
      "<NDArray 400001x1 @cpu(0)>\n",
      "dot_prod.dtype <class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "indices [2803, 0, 3, 11, 952, 4775, 10793]\n",
      "indices.dtype <class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.', 'for', 'computer', 'phones', 'pcs']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(glove_6b50d,5,'computers') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查找词典中与'run‘最接近的5个词 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_prod \n",
      "[[       nan]\n",
      " [ 3.5588462]\n",
      " [ 3.3498194]\n",
      " ...\n",
      " [-1.7190579]\n",
      " [-1.2515405]\n",
      " [-2.634035 ]]\n",
      "<NDArray 400001x1 @cpu(0)>\n",
      "dot_prod.dtype <class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "indices [308, 0, 798, 977, 389, 466, 1422]\n",
      "indices.dtype <class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['running', 'runs', 'went', 'start', 'ran']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(glove_6b50d,5,'run') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_prod \n",
      "[[        nan]\n",
      " [ 2.2918036 ]\n",
      " [ 2.477077  ]\n",
      " ...\n",
      " [-0.98178345]\n",
      " [-2.835377  ]\n",
      " [-1.817589  ]]\n",
      "<NDArray 400001x1 @cpu(0)>\n",
      "dot_prod.dtype <class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "indices [3367, 11128, 16430, 0, 5206, 12388, 4283]\n",
      "indices.dtype <class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gorgeous', '<unk>', 'wonderful', 'charming', 'beauty']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(glove_6b50d,5,'beautiful') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 求类比词\n",
    "\n",
    "我们可以使用预训练词向量求词与词之间的类比关系。例如，man : woman :: son : daughter 是一个类比例子：“man”之于“woman”相当于“son”之于“daughter”。求类比词问题可以定义为：对于类比关系中的四个词 a : b :: c : d，给定前三个词a, b, c，求d。解类比词的思路是，找到和c+(b-a)的结果词向量最相似的词向量。\n",
    "\n",
    "本例中，我们将从整个词典（大小40万，不含未知词符号）中找类比词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_by_analogy(token_embedding, k, word1, word2, word3):\n",
    "    word_vecs = token_embedding.get_vecs_by_tokens([word1, word2, word3])\n",
    "    word_diff = (word_vecs[1] - word_vecs[0] + word_vecs[2]).reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(token_embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_diff)\n",
    "    indices = nd.topk(dot_prod.reshape((len(token_embedding), )), k=k+1,\n",
    "                      ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "\n",
    "    # 不考虑未知词为可能的类比词。\n",
    "    if token_embedding.to_tokens(indices[0]) == token_embedding.unknown_token:\n",
    "        return token_embedding.to_tokens(indices[1:])\n",
    "    else:\n",
    "        return token_embedding.to_tokens(indices[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“男-女”类比：“man”之于“woman”相当于“son”之于什么？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(glove_6b50d, 1, 'man', 'woman', 'son')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证一下vec(“son”)+vec(“woman”)-vec(“man”)与vec(“daughter”)两个向量之间的余弦相似度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.9658341]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cos_sim_word_analogy(token_embedding, word1, word2, word3, word4):\n",
    "    words = [word1, word2, word3, word4]\n",
    "    vecs = token_embedding.get_vecs_by_tokens(words)\n",
    "    return cos_sim(vecs[1] - vecs[0] + vecs[2], vecs[3])\n",
    "\n",
    "cos_sim_word_analogy(glove_6b50d, 'man', 'woman', 'son', 'daughter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“首都-国家”类比：“beijing”之于“china”相当于“tokyo”之于什么？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(glove_6b50d, 1, 'beijing', 'china', 'tokyo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“形容词-形容词最高级”类比：“bad”之于“worst”相当于“big”之于什么？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(glove_6b50d, 1, 'bad', 'worst', 'big')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“动词一般时-动词过去时”类比：“do”之于“did”相当于“go”之于什么？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(glove_6b50d, 1, 'do', 'did', 'go')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 结论\n",
    "\n",
    "* 使用`mxnet.contrib.text`可以轻松载入预训练的词向量。\n",
    "* 我们可以应用预训练的词向量求相似词和类比词。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 将近似词和类比词应用中的$k$调大一些，观察结果。\n",
    "* 测试一下fastText的中文词向量（pretrained_file_name='wiki.zh.vec'）。\n",
    "* 如果在[使用循环神经网络的语言模型](../chapter_recurrent-neural-networks/rnn-gluon.md)中将Embedding层初始化为预训练的词向量，效果如何？\n",
    "\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/4373)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
