{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import nd \n",
    "def pure_batch_norm(X,gamma,beta,eps=1e-5):\n",
    "    assert len(X.shape) in (2,4)\n",
    "    # 全连接：batch_size x feature \n",
    "    if len(X.shape)==2:\n",
    "        mean=X.mean(axis=0)\n",
    "        variance=((X-mean)**2).mean(axis=0) \n",
    "    else:\n",
    "        mean=X.mean(axis=(0,2,3),keepdims=True)\n",
    "        variance=((X-mean)**2).mean(axis=(0,2,3),keepdims=True) \n",
    "    # 均一化\n",
    "    X_hat= (X-mean)/nd.sqrt(variance+eps) \n",
    "    # 拉升和偏移 \n",
    "    return gamma.reshape(mean.shape)*X_hat+beta.reshape(mean.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 1.]\n",
       " [2. 3.]\n",
       " [4. 5.]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=nd.arange(6).reshape((3,2))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ -1.2247427 -13.472169 ]\n",
       " [  0.          0.       ]\n",
       " [  1.2247427  13.472169 ]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(A,gamma=nd.array([1,11]),beta=nd.array([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们定义二维卷积网络层的输入是这样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[ 0.  1.  2.]\n",
       "   [ 3.  4.  5.]\n",
       "   [ 6.  7.  8.]]\n",
       "\n",
       "  [[ 9. 10. 11.]\n",
       "   [12. 13. 14.]\n",
       "   [15. 16. 17.]]]]\n",
       "<NDArray 1x2x3x3 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=nd.arange(18).reshape((1,2,3,3))\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果也如预期那样，我们对每个通道做了归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[ -1.5491922   -1.1618942   -0.7745961 ]\n",
       "   [ -0.38729805   0.           0.38729805]\n",
       "   [  0.7745961    1.1618942    1.5491922 ]]\n",
       "\n",
       "  [[-17.041115   -12.780836    -8.520557  ]\n",
       "   [ -4.2602787    0.           4.2602787 ]\n",
       "   [  8.520557    12.780836    17.041115  ]]]]\n",
       "<NDArray 1x2x3x3 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(B,gamma=nd.array([1,11]),beta=nd.array([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量归一化 \n",
    "事实上，我们测试时还是需要继续使用批量归一化的，只是需要做些改动。\n",
    "当训练数据很大时。我们用移动平均的方法来近似计算。\n",
    "（参见实现中的moving_mean 和 moving_variance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X,gamma,beta,is_training,moving_mean,moving_variance,eps=1e-5,moving_momentum=0.9):\n",
    "    assert len(X.shape) in (2,4) \n",
    "    # 全连接：batch_size x feature \n",
    "    if len(X.shape)==2:\n",
    "        # 每个输入纬度在样本上的平均和方差 \n",
    "        mean=X.mean(axis=0)\n",
    "        variance=((X-mean)**2).mean(axis=0)\n",
    "    #2D 卷积；batch_size * channel * height * width \n",
    "    else:\n",
    "        # 对每个通道算均值和方差，需要保持4D形状使得可以正确的广播 \n",
    "        mean=X.mean(axis=(0,2,3),keepdims=True) \n",
    "        variance=((X-mean)**2).mean(axis=(0,2,3),keepdims=True) \n",
    "        # 变形使得可以正确的广播 \n",
    "        moving_mean=moving_mean.reshape(mean.shape)\n",
    "        moving_variance=moving_variance.reshape(mean.shape) \n",
    "    # 均一化 \n",
    "    if is_training:\n",
    "        X_hat=(X-mean)/nd.sqrt(variance+eps) \n",
    "        # !!! 更新全局的均值和方差 \n",
    "        moving_mean[:]=moving_momentum+moving_mean+( \n",
    "        1.0*-moving_momentum)*variance \n",
    "    else:\n",
    "        #!!! 测试阶段使用全局的均值和方差 \n",
    "        X_hat=(X-moving_mean)/nd.sqrt(moving_variance+eps) \n",
    "    # 拉升和偏移 \n",
    "    return gamma.reshape(mean.shape)*X_hat+beta.reshape(mean.shape) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型 \n",
    "我们尝试使用GPU运行本教程代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cpu(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..') \n",
    "import utils \n",
    "ctx=utils.try_gpu()\n",
    "ctx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先定义参数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale=0.01 \n",
    "# output channels=20,kernel=(5,5) \n",
    "c1=20 \n",
    "W1=nd.random.normal(shape=(c1,1,5,5),scale=weight_scale,ctx=ctx) \n",
    "b1=nd.zeros(c1,ctx=ctx) \n",
    "# batch norm 1 \n",
    "gamma1=nd.random.normal(shape=c1,scale=weight_scale,ctx=ctx) \n",
    "beta1=nd.random.normal(shape=c1,scale=weight_scale,ctx=ctx) \n",
    "moving_mean1=nd.zeros(c1,ctx=ctx) \n",
    "mvoing_variance1=nd.zeros(c1,ctx=ctx) \n",
    "# output channels = 50 ,kernel=(3,3) \n",
    "c2=50 \n",
    "W2=nd.random_normal(shape=(c2,c1,3,3),scale=weight_scale,ctx=ctx)\n",
    "b2=nd.zeros(c2,ctx=ctx) \n",
    "\n",
    "# batch norm 2 \n",
    "gamma2=nd.random.normal(shape=c2,scale=weight_scale,ctx=ctx) \n",
    "beta2=nd.random.normal(shape=c2,scale=weight_scale,ctx=ctx) \n",
    "moving_mean2=nd.zeros(c2,ctx=ctx )\n",
    "moving_variance2=nd.zeros(c2,ctx=ctx) \n",
    "\n",
    "# output dim = 128 \n",
    "o3=128 \n",
    "W3=nd.random.normal(shape=(1250,o3),scale=weight_scale,ctx=ctx) \n",
    "b3=nd.zeros(o3,ctx=ctx) \n",
    "\n",
    "# output dim = 10 \n",
    "W4=nd.random_normal(shape=(W3.shape[1],10),scale=weight_scale,\n",
    "                   ctx=ctx) \n",
    "b4=nd.zeros(W4.shape[1],ctx=ctx) \n",
    "\n",
    "# 注意这里moving_* 是不需要更新的恶\n",
    "params=[W1,b1,gamma1,beta1,\n",
    "       W2,b2,gamma2,beta2,\n",
    "       W3,b3,W4,b4]\n",
    "for param in params: \n",
    "    param.attach_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义模型。我们添加了批量归一化层。特别是注意我们添加的位置：在\n",
    "卷积层后，在激活函数前 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X,is_training=False,verbose=False):\n",
    "    X=X.as_in_context(W1.context)\n",
    "    # 第一层卷积 \n",
    "    h1_conv=nd.Convolution(data=X,weight=W1,bias=b1,kernel=W1.shape[2:],num_filter=c1) \n",
    "    ### 添加了批量归一化层 \n",
    "    h1_bn=batch_norm(h1_conv,gamma1,beta1,is_training,\n",
    "                    moving_mean1,mvoing_variance1)\n",
    "    h1_activation=nd.relu(h1_bn)\n",
    "    h1=nd.Pooling(\n",
    "        data=h1_activation,pool_type='max',kernel=(2,2),stride=(2,2))\n",
    "    #  第二层 \n",
    "    h2_conv=nd.Convolution(\n",
    "        data=h1,weight=W2,bias=b2,kernel=W2.shape[2:],num_filter=c2) \n",
    "    ## 添加了批量归一化 \n",
    "    h2_bn=batch_norm(h2_conv,gamma2,beta2,is_training,\n",
    "                    moving_mean2,moving_variance2) \n",
    "    h2_activation=nd.relu(h2_bn) \n",
    "    h2=nd.Pooling(data=h2_activation,pool_type='max',kernel=(2,2),stride=(2,2))\n",
    "    h2=nd.flatten(h2) \n",
    "    # 第一层全连接 \n",
    "    h3_linear=nd.dot(h2,W3)+b3 \n",
    "    h3=nd.relu(h3_linear) \n",
    "    # 第二层全连接 \n",
    "    h4_linear=nd.dot(h3,W4)+b4 \n",
    "    if verbose:\n",
    "        print('lst conv block:',h1.shape) \n",
    "        print('2nd conv block:',h2.shape) \n",
    "        print('lst dense:',h3.shape) \n",
    "        print('2nd dense:',h4_linear.shape) \n",
    "        print('output:',h4_linear) \n",
    "    return h4_linear \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 . loss :2.198269 ,train acc 0.150574,Test acc0.100060 \n",
      "Epoch 1 . loss :0.644039 ,train acc 0.756193,Test acc0.100060 \n",
      "Epoch 2 . loss :0.408433 ,train acc 0.848708,Test acc0.100060 \n",
      "Epoch 3 . loss :0.347054 ,train acc 0.870777,Test acc0.100060 \n",
      "Epoch 4 . loss :0.312347 ,train acc 0.884732,Test acc0.100060 \n",
      "191.68880820274353\n"
     ]
    }
   ],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon \n",
    "from time import * \n",
    "batch_size=256 \n",
    "train_data,test_data=utils.load_data_fashion_mnist(batch_size) \n",
    "softmax_cross_entropy=gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "learning_rate=0.2 \n",
    "start=time()  \n",
    "for epoch in range(5):\n",
    "    train_loss=0.\n",
    "    train_acc= 0. \n",
    "    for data,label in train_data:\n",
    "        label =label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output=net(data,is_training=True)\n",
    "            loss=softmax_cross_entropy(output,label)\n",
    "        loss.backward()\n",
    "        utils.SGD(params,learning_rate/batch_size) \n",
    "        train_loss+=nd.mean(loss).asscalar()\n",
    "        train_acc+=utils.accuracy(output,label)\n",
    "    test_acc=utils.evaluate_accuracy(test_data,net,ctx) \n",
    "    print(\"Epoch %d . loss :%f ,train acc %f,Test acc%f \"% \n",
    "          (epoch,train_loss/len(train_data),train_acc/len(train_data),test_acc)\n",
    "          ) \n",
    "end=time()  \n",
    "print(end-start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结 \n",
    "相比 卷积神经网络--从0 开始来说。通过加入批量归一化层，即使是同样的参数，\n",
    "测试精度也有明显的提升，尤其是最开始几轮 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
