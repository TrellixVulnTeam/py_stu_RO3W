{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络 -- 从0 开始 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "[[[[0. 1. 2.]\n",
      "   [3. 4. 5.]\n",
      "   [6. 7. 8.]]]]\n",
      "<NDArray 1x1x3x3 @cpu(0)> \n",
      "\n",
      " weight: \n",
      "[[[[0. 1.]\n",
      "   [2. 3.]]]]\n",
      "<NDArray 1x1x2x2 @cpu(0)> \n",
      "\n",
      "bias \n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)> \n",
      "\n",
      "output: \n",
      "[[[[20. 26.]\n",
      "   [38. 44.]]]]\n",
      "<NDArray 1x1x2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd \n",
    "# 输入输出数据格式是 batch x channel x height x width ,这里batch和channel 都是1\n",
    "# 权重格式是 input_filter x output_filter x height x width ,这里的input_filter和out_filter \n",
    "\n",
    "w=nd.arange(4).reshape((1,1,2,2))\n",
    "b=nd.array([1]) \n",
    "data=nd.arange(9).reshape((1,1,3,3)) \n",
    "out=nd.Convolution(data,w,b,kernel=w.shape[2:],num_filter=w.shape[1]) \n",
    "print(\"input:\",data,\"\\n\\n weight:\",w,\"\\n\\nbias\",b ,\"\\n\\noutput:\",out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "[[[[ 0.  1.  2.]\n",
      "   [ 3.  4.  5.]\n",
      "   [ 6.  7.  8.]]\n",
      "\n",
      "  [[ 9. 10. 11.]\n",
      "   [12. 13. 14.]\n",
      "   [15. 16. 17.]]]]\n",
      "<NDArray 1x2x3x3 @cpu(0)> \n",
      "\n",
      " weight: \n",
      "[[[[0. 1.]\n",
      "   [2. 3.]]\n",
      "\n",
      "  [[4. 5.]\n",
      "   [6. 7.]]]]\n",
      "<NDArray 1x2x2x2 @cpu(0)> \n",
      "\n",
      "bias: \n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)> \n",
      "\n",
      "output: \n",
      "[[[[269. 297.]\n",
      "   [353. 381.]]]]\n",
      "<NDArray 1x1x2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "w=nd.arange(8).reshape((1,2,2,2))\n",
    "data=nd.arange(18).reshape((1,2,3,3)) \n",
    "out=nd.Convolution(data,w,b,kernel=w.shape[2:],num_filter=w.shape[0]) \n",
    "print(\"input:\",data,\"\\n\\n weight:\",w,\"\\n\\nbias:\",b,\"\\n\\noutput:\",out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "[[[[ 0.  1.  2.]\n",
      "   [ 3.  4.  5.]\n",
      "   [ 6.  7.  8.]]\n",
      "\n",
      "  [[ 9. 10. 11.]\n",
      "   [12. 13. 14.]\n",
      "   [15. 16. 17.]]]]\n",
      "<NDArray 1x2x3x3 @cpu(0)> \n",
      "\n",
      "weight: \n",
      "[[[[ 0.  1.]\n",
      "   [ 2.  3.]]\n",
      "\n",
      "  [[ 4.  5.]\n",
      "   [ 6.  7.]]]\n",
      "\n",
      "\n",
      " [[[ 8.  9.]\n",
      "   [10. 11.]]\n",
      "\n",
      "  [[12. 13.]\n",
      "   [14. 15.]]]]\n",
      "<NDArray 2x2x2x2 @cpu(0)> \n",
      "\n",
      " bias: \n",
      "[1. 2.]\n",
      "<NDArray 2 @cpu(0)> \n",
      "\n",
      "output: \n",
      "[[[[ 269.  297.]\n",
      "   [ 353.  381.]]\n",
      "\n",
      "  [[ 686.  778.]\n",
      "   [ 962. 1054.]]]]\n",
      "<NDArray 1x2x2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "w=nd.arange(16).reshape((2,2,2,2)) \n",
    "data=nd.arange(18).reshape((1,2,3,3))\n",
    "b=nd.array([1,2]) \n",
    "\n",
    "out=nd.Convolution(data,w,b,kernel=w.shape[2:],num_filter=w.shape[0])\n",
    "print(\"input:\",data,\"\\n\\nweight:\",w,\"\\n\\n bias:\",b,\"\\n\\noutput:\",out) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 池化层 （pooling) \n",
    "因为卷积层每次作用在一个窗口，它对位置很敏感。池化层能够很好的缓解这个问题。它跟卷积类似每次看一个小窗口，然后选择窗口里面最大的元素，或者平均元素作为输出。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      "[[[[ 0.  1.  2.]\n",
      "   [ 3.  4.  5.]\n",
      "   [ 6.  7.  8.]]\n",
      "\n",
      "  [[ 9. 10. 11.]\n",
      "   [12. 13. 14.]\n",
      "   [15. 16. 17.]]]]\n",
      "<NDArray 1x2x3x3 @cpu(0)> \n",
      "\n",
      "max pooling: \n",
      "[[[[ 4.  5.]\n",
      "   [ 7.  8.]]\n",
      "\n",
      "  [[13. 14.]\n",
      "   [16. 17.]]]]\n",
      "<NDArray 1x2x2x2 @cpu(0)> \n",
      "\n",
      " avg pooling: \n",
      "[[[[ 2.  3.]\n",
      "   [ 5.  6.]]\n",
      "\n",
      "  [[11. 12.]\n",
      "   [14. 15.]]]]\n",
      "<NDArray 1x2x2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "data=nd.arange(18).reshape((1,2,3,3,)) \n",
    "max_pool=nd.Pooling(data=data,pool_type='max',kernel=(2,2))\n",
    "avg_pool=nd.Pooling(data=data,pool_type='avg',kernel=(2,2)) \n",
    "print('data:',data,'\\n\\nmax pooling:', max_pool,'\\n\\n avg pooling:',avg_pool) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据 \n",
    "我们继续使用FashionMNIST (希望你还没有彻底厌烦这个数据） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import d2lzh\n",
    "\n",
    "sys.path.append('..')  \n",
    "\n",
    "batch_size=256 \n",
    "train_data,test_data = d2lzh.utils.load_data_fashion_mnist(batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mxnet.gluon.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mxnet.gluon.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "(256, 1, 28, 28)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "for data,label in train_data:\n",
    "    print(type(data))\n",
    "    print(type(label))\n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型\n",
    "因为卷积网络计算比全连接要复杂，这里我们默认使用GPU来计算。如果GPU不能用，默认使用CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu(0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx \n",
    "try:\n",
    "    ctx=mx.gpu(0)\n",
    "    __=nd.zeros((1,),ctx=ctx)\n",
    "except:\n",
    "    ctx=mx.cpu()\n",
    "ctx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale=0.01 \n",
    "num_outputs=10 \n",
    "num_fc=128 \n",
    "# output channels=20 ,kernel = (5,5) \n",
    "W1=nd.random_normal(shape=(20,1,5,5),scale=weight_scale,ctx=ctx)\n",
    "b1=nd.zeros(W1.shape[0],ctx=ctx) \n",
    "\n",
    "# output channels=50 ,kernel = (3,3) \n",
    "W2=nd.random_normal(shape=(50,20,3,3),scale=weight_scale,ctx=ctx) \n",
    "b2=nd.zeros(W2.shape[0],ctx=ctx)\n",
    "\n",
    "# output dim = 128 \n",
    "W3=nd.random_normal(shape=(1250,128),scale=weight_scale,ctx=ctx) \n",
    "b3=nd.zeros(W3.shape[1],ctx=ctx) \n",
    "\n",
    "# output dim =10 \n",
    "W4=nd.random_normal(shape=(W3.shape[1],10),scale=weight_scale,ctx=ctx)\n",
    "b4=nd.zeros(W4.shape[1],ctx=ctx) \n",
    "\n",
    "params=[W1,b1,W2,b2,W3,b3,W4,b4] \n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积模块通常是\" 卷积层-激活层-池化层\".然后转成2D 矩阵输出给后面的全连接层."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X,verbose=False):\n",
    "    X=X.as_in_context(W1.context) \n",
    "    # 第一层卷积 \n",
    "    h1_conv=nd.Convolution(\n",
    "                data=X,weight=W1,bias=b1,kernel=W1.shape[2:],num_filter=W1.shape[0])\n",
    "    h1_activation=nd.relu(h1_conv) \n",
    "    h1=nd.Pooling( \n",
    "         data=h1_activation,pool_type='max',kernel=(2,2),stride=(2,2))\n",
    "    # 第二层卷积 \n",
    "    h2_conv=nd.Convolution( \n",
    "        data=h1,weight=W2,bias=b2,kernel=W2.shape[2:],num_filter=W2.shape[0])\n",
    "    h2_activation=nd.relu(h2_conv) \n",
    "    h2=nd.Pooling(data=h2_activation,pool_type='max',kernel=(2,2),stride=(2,2))\n",
    "    h2=nd.flatten(h2)\n",
    "    # 第一层全连接 \n",
    "    h3_linear=nd.dot(h2,W3)+b3 \n",
    "    h3=nd.relu(h3_linear) \n",
    "    # 第二次全连接\n",
    "    h4_linear=nd.dot(h3,W4)+b4 \n",
    "    if verbose:\n",
    "        print('lst conv block:',h1.shape)\n",
    "        print('2nd conv block:',h2.shape)\n",
    "        print('1st dense:',h3.shape)\n",
    "        print('2nd dense:',h4_linear.shape) \n",
    "        print('output:',h4_linear) \n",
    "    return h4_linear "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下，输出中间结果形状（当然可以直接打印结果）和最终结果 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1, 28, 28)\n",
      "lst conv block: (256, 20, 12, 12)\n",
      "2nd conv block: (256, 1250)\n",
      "1st dense: (256, 128)\n",
      "2nd dense: (256, 10)\n",
      "output: \n",
      "[[-1.8783900e-05 -6.2933759e-07 -9.3574359e-05 ... -5.2623993e-05\n",
      "  -5.7633759e-05  3.9603750e-05]\n",
      " [-3.5778103e-05  5.5189012e-05 -1.3546072e-04 ... -7.6321958e-05\n",
      "  -2.8267150e-05 -1.8101000e-05]\n",
      " [-1.3687852e-05  2.7907761e-05 -8.0980739e-05 ... -3.7690043e-05\n",
      "  -2.2359180e-05 -1.5780366e-05]\n",
      " ...\n",
      " [-6.0828897e-05 -1.6831953e-05 -8.5152213e-05 ...  2.2986067e-05\n",
      "  -4.0296814e-05  4.3750617e-05]\n",
      " [-2.4852467e-05  2.2363269e-05 -9.7990822e-05 ... -4.1857689e-05\n",
      "  -4.7825364e-05  3.6065005e-05]\n",
      " [-4.8120573e-06 -9.9961744e-07 -5.5666016e-05 ... -3.5472171e-05\n",
      "  -3.2510230e-05 -5.7275970e-06]]\n",
      "<NDArray 256x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for data,__ in train_data:\n",
    "    print(data.shape)\n",
    "    net(data,verbose=True) \n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练 \n",
    "跟前面没有什么不同 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(params,lr):\n",
    "    for param in params:\n",
    "        param[:]=param -lr * param.grad \n",
    "        \n",
    "def accuracy(output,label):\n",
    "    return nd.mean(output.argmax(axis=1)==label).asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mxnet.gluon.data.dataloader.DataLoader object at 0x7ff9ec92f4a8>\n",
      "\n",
      "[8 3 3 2 4 6 1 6 3 1 0 9 1 3 5 4 2 4 6 1 6 9 1 4 4 1 8 4 8 2 2 1 0 3 0 1 9\n",
      " 2 2 3 9 0 4 2 1 3 4 4 7 3 7 1 4 8 4 6 8 6 0 5 6 3 2 5 0 9 4 8 6 1 8 9 2 2\n",
      " 0 2 9 4 7 6 1 3 9 2 4 0 1 9 5 0 5 4 0 7 2 3]\n",
      "<NDArray 96 @cpu(0)>\n",
      "36.25201106071472\n"
     ]
    }
   ],
   "source": [
    "from mxnet import autograd as autograd \n",
    "# from d2lzh.utils import SGD,accuracy,evaluate_accuracy \n",
    "# from d2lzh_my import utils\n",
    "import d2lzh_my\n",
    "from mxnet import gluon \n",
    "from time import * \n",
    "begin_time=time() \n",
    "softmax_cross_entropy=gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "learning_rate=0.2 \n",
    "for epoch in range(5):\n",
    "    train_loss=0. \n",
    "    train_acc=0. \n",
    "    for data,label in train_data:\n",
    "        label=label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output=net(data)\n",
    "            loss=softmax_cross_entropy(output,label) \n",
    "        loss.backward()\n",
    "        d2lzh_my.utils.SGD(params,learning_rate/batch_size) \n",
    "        train_loss+=nd.mean(loss).asscalar()\n",
    "        train_acc+=d2lzh_my.utils.accuracy(output,label.astype(\"float32\"))\n",
    "    print( test_data)\n",
    "    print(label)\n",
    "    break \n",
    "    test_acc=d2lzh_my.utils.evaluate_accuracy(test_data,net,ctx)\n",
    "    print(\"Epoch %d . loss :%f ,train acc %f,Test acc%f \"% \n",
    "          (epoch,train_loss/len(train_data),train_acc.asscalar()/len(train_data),test_acc)\n",
    "          ) \n",
    "    break \n",
    "       \n",
    "\n",
    "end_time=time()\n",
    "print(end_time-begin_time)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
