{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 23 11:40:51 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\r\n",
      "| 27%   28C    P8    20W / 250W |   1413MiB / 11016MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1814      G   /usr/lib/xorg/Xorg                 14MiB |\r\n",
      "|    0   N/A  N/A      1902      G   /usr/bin/gnome-shell               57MiB |\r\n",
      "|    0   N/A  N/A      2483      G   /usr/lib/xorg/Xorg                 94MiB |\r\n",
      "|    0   N/A  N/A      2624      G   /usr/bin/gnome-shell              155MiB |\r\n",
      "|    0   N/A  N/A      3384      G   ...mviewer/tv_bin/TeamViewer       22MiB |\r\n",
      "|    0   N/A  N/A     11290      C   ...da3/envs/gluon/bin/python     1061MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 ｐｉｐ 来确认下 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: Package(s) not found: mxnet\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: Package(s) not found: mxnet-cu75\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: Package(s) not found: mxnet-cu80\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mxnet-cu101\n",
      "Version: 1.7.0\n",
      "Summary: MXNet is an ultra-scalable deep learning framework. This version uses CUDA-10.1.\n",
      "Home-page: https://github.com/apache/incubator-mxnet\n",
      "Author: None\n",
      "Author-email: None\n",
      "License: Apache 2.0\n",
      "Location: /home/dske/anaconda3/envs/gluon/lib/python3.6/site-packages\n",
      "Requires: numpy, graphviz, requests\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "import pip \n",
    "for pkg in ['mxnet','mxnet-cu75','mxnet-cu80','mxnet-cu101']:\n",
    "    pip.main(['show',pkg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context \n",
    "MXNET 使用 Context来指定使用哪个设备来存储和计算。默认会将数据开在主内成，然后利用CPU来计算，这个由mx.cpu()来表示。GPU则由mx.gpu()来表示。注意mx.cpu()表示所有的物理CPU和内成，意味着计算上会尽量使用多有的CPU核。但mx.gpu()只代表一块显卡和其对应的显卡内成。\n",
    "如果有多块GPU，我们用mx.gpu(i)来表示第i快GPU （i 从0开始) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cpu(0), gpu(0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "# [mx.cpu(),mx.gpu(),mx,gpu(1)]\n",
    "[mx.cpu(),mx.gpu()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDArray 的GPU计算 \n",
    "每个NDArray 都有一个context 属性来表示它存在哪个设备上，默认会是cpu。这是为什么前面每次我们打印NDArray的时候都会看到@cpu(0)这个标识 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "x=nd.array([1,2,3]) \n",
    "x.context "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 上创建内存 \n",
    "我们可以在创建的时候指定创建在哪个设备上（如果GPU不能用或者没有装MXnet GPU 版本，这里会有error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1. 2. 3.]\n",
       " <NDArray 3 @gpu(0)>,\n",
       " \n",
       " [[0. 0.]\n",
       "  [0. 0.]\n",
       "  [0. 0.]]\n",
       " <NDArray 3x2 @gpu(0)>,\n",
       " \n",
       " [[0.33810216 0.03127927 0.2290985 ]\n",
       "  [0.7409259  0.51678467 0.48066086]]\n",
       " <NDArray 2x3 @gpu(0)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=nd.array([1,2,3],ctx=mx.gpu()) \n",
    "b=nd.zeros((3,2),ctx=mx.gpu())\n",
    "c=nd.random.uniform(shape=(2,3),ctx=mx.gpu()) \n",
    "(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"src/engine/./../common/cuda_utils.h\", line 395\n",
      "CUDA: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: invalid device ordinal"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "try:\n",
    "    nd.array([1,2,3],ctx=mx.gpu(1))\n",
    "except mx.MXNetError as err:\n",
    "    haha=err \n",
    "    sys.stderr.write(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过copyto和 as_in_context来在设备上直接传输数据 。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1. 2. 3.]\n",
       " <NDArray 3 @gpu(0)>,\n",
       " \n",
       " [1. 2. 3.]\n",
       " <NDArray 3 @gpu(0)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x.copyto(mx.gpu()) \n",
    "z=x.as_in_context(mx.gpu())\n",
    "(y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两个函数的主要区别是，如果源和目标的context一致，as_in_context不复制，而copyto总是会新建内存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy=y.as_in_context(mx.gpu())\n",
    "zz=z.copyto(mx.gpu())\n",
    "(yy is y , zz is z )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 上的计算 \n",
    "计算会在数据的context上执行。所以为了使用GPU,我们只需要事先将数据放在上面就行了。结果会自动保存在对应的设备设。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 20.085537 109.1963   445.2395  ]\n",
       "<NDArray 3 @gpu(0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.exp(z+2)*y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意所有计算要求输入数据在同一个设备上，不一致的时候，系统不进行自动赋值。这个设计的目的是因为设备之间的数据交互通常\n",
    "比较昂贵，我们希望用户确切置到数据放在哪里，而不是隐藏这个细节。下面代码尝试将CPU上和GPU 上的y做运输。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 默认会复制CPU的操作 \n",
    "如果某个操作需要将NDArray里面的内容转出来，例如打印或变成numpy格式，如果需要的话，系统都会自动将数据copy到主内存 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1. 2. 3.]\n",
      "<NDArray 3 @gpu(0)>\n",
      "[1. 2. 3.]\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y.asnumpy())\n",
    "print(y.sum().asscalar())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gluon的GPU计算 \n",
    "同NDArray类似，Gluon的大部分函数可以通过ctx指定设备。下面代码将模型参数初始化在GPU上:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon \n",
    "net=gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(1))\n",
    "net.initialize(ctx=mx.gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入到GPU上的数据，会在GPU上计算结果　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.00699626]\n",
       " [0.01652164]\n",
       " [0.00990379]]\n",
       "<NDArray 3x1 @gpu(0)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=nd.random.uniform(shape=[3,2],ctx=mx.gpu())\n",
    "net(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 确认下权重 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.0068339  0.01299825]]\n",
       "<NDArray 1x2 @gpu(0)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过copyto和as_in_context来在设备直接传输数据。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1. 2. 3.]\n",
       " <NDArray 3 @gpu(0)>,\n",
       " \n",
       " [1. 2. 3.]\n",
       " <NDArray 3 @gpu(0)>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x.copyto(mx.gpu())\n",
    "z=x.as_in_context(mx.gpu())\n",
    "(z,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
