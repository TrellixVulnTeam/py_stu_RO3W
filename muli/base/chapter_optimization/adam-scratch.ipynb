{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam --- 从0开始\n",
    "\n",
    "Adam是一个组合了[动量法](momentum-scratch.md)和[RMSProp](rmsprop-scratch.md)的优化算法。\n",
    "\n",
    "\n",
    "\n",
    "## Adam算法\n",
    "\n",
    "Adam算法会使用一个动量变量$\\mathbf{v}$和一个RMSProp中梯度按元素平方的指数加权移动平均变量$\\mathbf{s}$，并将它们中每个元素初始化为0。在每次迭代中，首先计算[小批量梯度](gd-sgd-scratch.md) $\\mathbf{g}$，并递增迭代次数\n",
    "\n",
    "$$t := t + 1$$\n",
    "\n",
    "然后对梯度做指数加权移动平均并计算动量变量$\\mathbf{v}$:\n",
    "\n",
    "$$\\mathbf{v} := \\beta_1 \\mathbf{v} + (1 - \\beta_1) \\mathbf{g} $$\n",
    "\n",
    "\n",
    "该梯度按元素平方后做指数加权移动平均并计算$\\mathbf{s}$：\n",
    "\n",
    "$$\\mathbf{s} := \\beta_2 \\mathbf{s} + (1 - \\beta_2) \\mathbf{g} \\odot \\mathbf{g} $$\n",
    "\n",
    "\n",
    "在Adam算法里，为了减轻$\\mathbf{v}$和$\\mathbf{s}$被初始化为0在迭代初期对计算指数加权移动平均的影响，我们做下面的偏差修正：\n",
    "\n",
    "$$\\hat{\\mathbf{v}} := \\frac{\\mathbf{v}}{1 - \\beta_1^t} $$\n",
    "\n",
    "和\n",
    "\n",
    "$$\\hat{\\mathbf{s}} := \\frac{\\mathbf{s}}{1 - \\beta_2^t} $$\n",
    "\n",
    "\n",
    "\n",
    "可以看到，当$0 \\leq \\beta_1, \\beta_2 < 1$时（算法作者建议分别设为0.9和0.999），当迭代后期$t$较大时，偏差修正几乎就不再有影响。我们使用以上偏差修正后的动量变量和RMSProp中梯度按元素平方的指数加权移动平均变量，将模型参数中每个元素的学习率通过按元素操作重新调整一下：\n",
    "\n",
    "$$\\mathbf{g}^\\prime := \\frac{\\eta \\hat{\\mathbf{v}}}{\\sqrt{\\hat{\\mathbf{s}} + \\epsilon}} $$\n",
    "\n",
    "其中$\\eta$是初始学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，例如$10^{-8}$。和Adagrad一样，模型参数中每个元素都分别拥有自己的学习率。\n",
    "\n",
    "同样地，最后的参数迭代步骤与小批量随机梯度下降类似。只是这里梯度前的学习率已经被调整过了：\n",
    "\n",
    "$$\\mathbf{x} := \\mathbf{x} - \\mathbf{g}^\\prime $$\n",
    "\n",
    "\n",
    "## Adam的实现\n",
    "\n",
    "\n",
    "Adam的实现很简单。我们只需要把上面的数学公式翻译成代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam。\n",
    "def adam(params, vs, sqrs, lr, batch_size, t):\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps_stable = 1e-8\n",
    "    for param, v, sqr in zip(params, vs, sqrs):      \n",
    "        g = param.grad / batch_size\n",
    "        v[:] = beta1 * v + (1. - beta1) * g\n",
    "        sqr[:] = beta2 * sqr + (1. - beta2) * nd.square(g)\n",
    "        v_bias_corr = v / (1. - beta1 ** t)\n",
    "        sqr_bias_corr = sqr / (1. - beta2 ** t)\n",
    "        div = lr * v_bias_corr / (nd.sqrt(sqr_bias_corr) + eps_stable)        \n",
    "        param[:] = param - div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 实验\n",
    "\n",
    "实验中，我们以线性回归为例。其中真实参数`w`为[2, -3.4]，`b`为4.2。我们把算法中基于指数加权移动平均的变量初始化为和参数形状相同的零张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import gluon\n",
    "import random\n",
    "\n",
    "mx.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# 生成数据集。\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "X = nd.random_normal(scale=1, shape=(num_examples, num_inputs))\n",
    "y = true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b\n",
    "y += .01 * nd.random_normal(scale=1, shape=y.shape)\n",
    "dataset = gluon.data.ArrayDataset(X, y)\n",
    "\n",
    "# 构造迭代器。\n",
    "import random\n",
    "def data_iter(batch_size):\n",
    "    idx = list(range(num_examples))\n",
    "    random.shuffle(idx)\n",
    "    for batch_i, i in enumerate(range(0, num_examples, batch_size)):\n",
    "        j = nd.array(idx[i: min(i + batch_size, num_examples)])\n",
    "        yield batch_i, X.take(j), y.take(j)\n",
    "\n",
    "# 初始化模型参数。\n",
    "def init_params():\n",
    "    w = nd.random_normal(scale=1, shape=(num_inputs, 1))\n",
    "    b = nd.zeros(shape=(1,))\n",
    "    params = [w, b]\n",
    "    vs = []\n",
    "    sqrs = []\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "        # 把算法中基于指数加权移动平均的变量初始化为和参数形状相同的零张量。\n",
    "        vs.append(param.zeros_like())\n",
    "        sqrs.append(param.zeros_like())\n",
    "    return params, vs, sqrs\n",
    "\n",
    "# 线性回归模型。\n",
    "def net(X, w, b):\n",
    "    return nd.dot(X, w) + b\n",
    "\n",
    "# 损失函数。\n",
    "def square_loss(yhat, y):\n",
    "    return (yhat - y.reshape(yhat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来定义训练函数。当epoch大于2时（epoch从1开始计数），学习率以自乘0.1的方式自我衰减。训练函数的period参数说明，每次采样过该数目的数据点后，记录当前目标函数值用于作图。例如，当period和batch_size都为10时，每次迭代后均会记录目标函数值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 120\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def train(batch_size, lr, epochs, period):\n",
    "    assert period >= batch_size and period % batch_size == 0\n",
    "    [w, b], vs, sqrs = init_params()\n",
    "    total_loss = [np.mean(square_loss(net(X, w, b), y).asnumpy())]\n",
    "\n",
    "    # 注意epoch从1开始计数。\n",
    "    t = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for batch_i, data, label in data_iter(batch_size):\n",
    "            with autograd.record():\n",
    "                output = net(data, w, b)\n",
    "                loss = square_loss(output, label)\n",
    "            loss.backward()\n",
    "            # 必须在调用Adam前。\n",
    "            t += 1\n",
    "            adam([w, b], vs, sqrs, lr, batch_size, t)\n",
    "            if batch_i * batch_size % period == 0:\n",
    "                total_loss.append(np.mean(square_loss(net(X, w, b), y).asnumpy()))\n",
    "        print(\"Batch size %d, Learning rate %f, Epoch %d, loss %.4e\" % \n",
    "              (batch_size, lr, epoch, total_loss[-1]))\n",
    "    print('w:', np.reshape(w.asnumpy(), (1, -1)), \n",
    "          'b:', b.asnumpy()[0], '\\n')\n",
    "    x_axis = np.linspace(0, epochs, len(total_loss), endpoint=True)\n",
    "    plt.semilogy(x_axis, total_loss)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Adam，最终学到的参数值与真实值较接近。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 10, Learning rate 0.100000, Epoch 1, loss 1.7606e-03\n",
      "Batch size 10, Learning rate 0.100000, Epoch 2, loss 4.8414e-05\n",
      "Batch size 10, Learning rate 0.100000, Epoch 3, loss 4.9217e-05\n",
      "w: [[ 1.9990062 -3.3995001]] b: 4.2015085 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(batch_size=10, lr=0.1, epochs=3, period=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 结论\n",
    "\n",
    "* Adam组合了动量法和RMSProp。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 你是怎样理解Adam算法中的偏差修正项的？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/2279)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
