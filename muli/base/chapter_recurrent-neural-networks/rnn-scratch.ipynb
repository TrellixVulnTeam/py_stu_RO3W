{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络 --- 从0开始\n",
    "\n",
    "前面的教程里我们使用的网络都属于**前馈神经网络**。之所以叫前馈，是因为整个网络是一条链（回想下`gluon.nn.Sequential`），每一层的结果都是反馈给下一层。这一节我们介绍**循环神经网络**，这里每一层不仅输出给下一层，同时还输出一个**隐含状态**，给当前层在处理下一个样本时使用。下图展示这两种网络的区别。\n",
    "\n",
    "![](../img/rnn_1.png)\n",
    "\n",
    "循环神经网络的这种结构使得它适合处理前后有依赖关系数据样本。我们拿语言模型举个例子来解释这个是怎么工作的。语言模型的任务是给定句子的前*t*个字符，然后预测第*t+1*个字符。假设我们的句子是“你好世界”，使用前馈神经网络来预测的一个做法是，在时间1输入“你”，预测”好“，时间2向同一个网络输入“好”预测“世”。下图左边展示了这个过程。\n",
    "\n",
    "![](../img/rnn_2.png)\n",
    "\n",
    "注意到一个问题是，当我们预测“世”的时候只给了“好”这个输入，而完全忽略了“你”。直觉上“你”这个词应该对这次的预测比较重要。虽然这个问题通常可以通过**n-gram**来缓解，就是说预测第*t+1*个字符的时候，我们输入前*n*个字符。如果*n=1*，那就是我们这里用的。我们可以增大*n*来使得输入含有更多信息。但我们不能任意增大*n*，因为这样通常带来模型复杂度的增加从而导致需要大量数据和计算来训练模型。\n",
    "\n",
    "循环神经网络使用一个隐含状态来记录前面看到的数据来帮助当前预测。上图右边展示了这个过程。在预测“好”的时候，我们输出一个隐含状态。我们用这个状态和新的输入“好”来一起预测“世”，然后同时输出一个更新过的隐含状态。我们希望前面的信息能够保存在这个隐含状态里，从而提升预测效果。\n",
    "\n",
    "## 循环神经网络\n",
    "\n",
    "在对输入输出数据有了解后，我们来正式介绍循环神经网络。\n",
    "\n",
    "首先回忆一下单隐含层的前馈神经网络的定义，例如[多层感知机](../chapter_supervised-learning/mlp-scratch.md)。假设隐含层的激活函数是$\\phi$，对于一个样本数为$n$特征向量维度为$x$的批量数据$\\mathbf{X} \\in \\mathbb{R}^{n \\times x}$（$\\mathbf{X}$是一个$n$行$x$列的实数矩阵）来说，那么这个隐含层的输出就是\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h)$$\n",
    "\n",
    "假定隐含层长度为$h$，其中的$\\mathbf{W}_{xh} \\in \\mathbb{R}^{x \\times h}$是权重参数。偏移参数 $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$在与前一项$\\mathbf{X} \\mathbf{W}_{xh} \\in \\mathbb{R}^{n \\times h}$ 相加时使用了[广播](../chapter_crashcourse/ndarray.md)。这个隐含层的输出的尺寸为$\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$。\n",
    "\n",
    "把隐含层的输出$\\mathbf{H}$作为输出层的输入，最终的输出\n",
    "\n",
    "$$\\hat{\\mathbf{Y}} = \\text{softmax}(\\mathbf{H} \\mathbf{W}_{hy} + \\mathbf{b}_y)$$\n",
    "\n",
    "假定每个样本对应的输出向量维度为$y$，其中 $\\hat{\\mathbf{Y}} \\in \\mathbb{R}^{n \\times y}, \\mathbf{W}_{hy} \\in \\mathbb{R}^{h \\times y}, \\mathbf{b}_y \\in \\mathbb{R}^{1 \\times y}$且两项相加使用了[广播](../chapter_crashcourse/ndarray.md)。\n",
    "\n",
    "\n",
    "将上面网络改成循环神经网络，我们首先对输入输出加上时间戳$t$。假设$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$是序列中的第$t$个批量输入（样本数为$n$，每个样本的特征向量维度为$x$），对应的隐含层输出是隐含状态$\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$（隐含层长度为$h$），而对应的最终输出是$\\hat{\\mathbf{Y}}_t \\in \\mathbb{R}^{n \\times y}$（每个样本对应的输出向量维度为$y$）。在计算隐含层的输出的时候，循环神经网络只需要在前馈神经网络基础上加上跟前一时间$t-1$输入隐含层$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$的加权和。为此，我们引入一个新的可学习的权重$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$：\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h)$$\n",
    "\n",
    "输出的计算跟前面一致：\n",
    "\n",
    "$$\\hat{\\mathbf{Y}}_t = \\text{softmax}(\\mathbf{H}_t \\mathbf{W}_{hy}  + \\mathbf{b}_y)$$\n",
    "\n",
    "一开始我们提到过，隐含状态可以认为是这个网络的记忆。该网络中，时刻$t$的隐含状态就是该时刻的隐含层变量$\\mathbf{H}_t$。它存储前面时间里面的信息。我们的输出是只基于这个状态。最开始的隐含状态里的元素通常会被初始化为0。\n",
    "\n",
    "\n",
    "## 周杰伦歌词数据集\n",
    "\n",
    "\n",
    "为了实现并展示循环神经网络，我们使用周杰伦歌词数据集来训练模型作词。该数据集里包含了著名创作型歌手周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》所有歌曲的歌词。\n",
    "\n",
    "![](../img/jay.jpg)\n",
    "\n",
    "\n",
    "下面我们读取这个数据并看看前面49个字符（char）是什么样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每天在想想想想著你\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile \n",
    "with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip','r') as zin:\n",
    "    zin.extractall('../data/') \n",
    "with open('../data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars=f.read()\n",
    "print(corpus_chars[0:49]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64925"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_chars) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们稍微处理下数据集。为了打印方便，我们把换行符替换成空格，然后截去后面一段使得接下来的训练会快一点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chars=corpus_chars.replace('\\n',' ').replace('\\r',' ') \n",
    "# corpus_chars=corpus_chars[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每天在想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_chars[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 字符的数值表示\n",
    "\n",
    "先把数据里面所有不同的字符拿出来做成一个字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2613\n"
     ]
    }
   ],
   "source": [
    "idx_to_char=list(set(corpus_chars)) \n",
    "char_to_idx=dict([(char,i) for i,char in enumerate(idx_to_char)]) \n",
    "vocab_size=len(char_to_idx) \n",
    "print('vocab size:',vocab_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后可以把每个字符转成从0开始的索引(index)来方便之后的使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: \n",
      " 想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每\n",
      "\n",
      " indices: \n",
      " [744, 1453, 2181, 1940, 2005, 297, 2117, 744, 1453, 693, 1785, 2504, 2193, 992, 165, 646, 2117, 744, 1453, 693, 1785, 1085, 1915, 850, 835, 1445, 2117, 1085, 1915, 850, 992, 165, 2375, 2117, 2578, 2439, 1483, 2439, 1483, 2439]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices=[char_to_idx[char] for char in corpus_chars] \n",
    "sample=corpus_indices[:40]\n",
    "print('chars: \\n',''.join([idx_to_char[idx] for idx in sample])) \n",
    "print('\\n indices: \\n',sample)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 时序数据的批量采样\n",
    "\n",
    "同之前一样我们需要每次随机读取一些（`batch_size`个）样本和其对用的标号。这里的样本跟前面有点不一样，这里一个样本通常包含一系列连续的字符（前馈神经网络里可能每个字符作为一个样本）。\n",
    "\n",
    "如果我们把序列长度（`num_steps`）设成5，那么一个可能的样本是“想要有直升”。其对应的标号仍然是长为5的序列，每个字符是对应的样本里字符的后面那个。例如前面样本的标号就是“要有直升机”。\n",
    "\n",
    "\n",
    "### 随机批量采样\n",
    "下面代码每次从数据里随机采样一个批量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from mxnet import nd\n",
    "def data_iter_random(corpus_indices,batch_size,num_steps,ctx=None):\n",
    "    # \n",
    "    num_examples=(len(corpus_indices)-1)//num_steps \n",
    "    epoch_size=num_examples//batch_size \n",
    "    # 随机化样本 \n",
    "    example_indices=list(range(num_examples)) \n",
    "    random.shuffle(example_indices) \n",
    "    # 返回num_steps 个数据 \n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos:pos+num_steps]\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size 个随机样本。\n",
    "        i=i*batch_size \n",
    "        batch_indices=example_indices[i:i+batch_size] \n",
    "        data=nd.array(\n",
    "            [_data(j*num_steps) for j in batch_indices],ctx=ctx)\n",
    "        label=nd.array(\n",
    "            [_data(j*num_steps+1) for j in batch_indices],ctx=ctx) \n",
    "        yield data,label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了便于理解时序数据上的随机批量采样，让我们输入一个从0到29的人工序列，看下读出来长什么样：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      "[[ 9. 10. 11.]\n",
      " [15. 16. 17.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[10. 11. 12.]\n",
      " [16. 17. 18.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[24. 25. 26.]\n",
      " [ 3.  4.  5.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[25. 26. 27.]\n",
      " [ 4.  5.  6.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[18. 19. 20.]\n",
      " [ 0.  1.  2.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[19. 20. 21.]\n",
      " [ 1.  2.  3.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[12. 13. 14.]\n",
      " [21. 22. 23.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[13. 14. 15.]\n",
      " [22. 23. 24.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq=list(range(30)) \n",
    "for data,label in data_iter_random(my_seq,batch_size=2,num_steps=3):\n",
    "    print('data:',data,'\\nlabel:',label,'\\n') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "由于各个采样在原始序列上的位置是随机的时序长度为`num_steps`的连续数据点，相邻的两个随机批量在原始序列上的位置不一定相毗邻。因此，在训练模型时，读取每个随机时序批量前需要重新初始化隐含状态。\n",
    "\n",
    "\n",
    "### 相邻批量采样\n",
    "\n",
    "除了对原序列做随机批量采样之外，我们还可以使相邻的两个随机批量在原始序列上的位置相毗邻。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    indices = corpus_indices[0: batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    # 减一是因为label的索引是相应data的索引加一\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        data = indices[:, i: i + num_steps]\n",
    "        label = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield data, label\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相同地，为了便于理解时序数据上的相邻批量采样，让我们输入一个从0到29的人工序列，看下读出来长什么样：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      "[[ 0.  1.  2.]\n",
      " [15. 16. 17.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 1.  2.  3.]\n",
      " [16. 17. 18.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[ 3.  4.  5.]\n",
      " [18. 19. 20.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 4.  5.  6.]\n",
      " [19. 20. 21.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[ 6.  7.  8.]\n",
      " [21. 22. 23.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 7.  8.  9.]\n",
      " [22. 23. 24.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[ 9. 10. 11.]\n",
      " [24. 25. 26.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[10. 11. 12.]\n",
      " [25. 26. 27.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq=list(range(30)) \n",
    "for data,label in data_iter_consecutive(my_seq,batch_size=2,num_steps=3):\n",
    "    print('data:',data,'\\nlabel:',label,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "由于各个采样在原始序列上的位置是毗邻的时序长度为`num_steps`的连续数据点，因此，使用相邻批量采样训练模型时，读取每个时序批量前，我们需要将该批量最开始的隐含状态设为上个批量最终输出的隐含状态。在同一个epoch中，隐含状态只需要在该epoch开始的时候初始化。\n",
    "\n",
    "\n",
    "## One-hot向量\n",
    "\n",
    "注意到每个字符现在是用一个整数来表示，而输入进网络我们需要一个定长的向量。一个常用的办法是使用one-hot来将其表示成向量。也就是说，如果一个字符的整数值是$i$, 那么我们创建一个全0的长为`vocab_size`的向量，并将其第$i$位设成1。该向量就是对原字符的one-hot向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 1. ... 0. 0. 0.]]\n",
       "<NDArray 2x2613 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.one_hot(nd.array([0,2]),vocab_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记得前面我们每次得到的数据是一个`batch_size * num_steps`的批量。下面这个函数将其转换成`num_steps`个可以输入进网络的`batch_size * vocab_size`的矩阵。对于一个长度为`num_steps`的序列，每个批量输入$\\mathbf{X} \\in \\mathbb{R}^{n \\times x}$，其中$n=$ `batch_size`，而$x=$`vocab_size`（onehot编码向量维度）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 3\n",
      "input[0] shape: (2, 2613)\n"
     ]
    }
   ],
   "source": [
    "def get_inputs(data):\n",
    "    return [nd.one_hot(X,vocab_size) for X in data.T] \n",
    "\n",
    "inputs=get_inputs(data) \n",
    "print('Input length:',len(inputs)) \n",
    "print('input[0] shape:',inputs[0].shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 初始化模型参数\n",
    "\n",
    "对于序列中任意一个时间戳，一个字符的输入是维度为`vocab_size`的one-hot向量，对应输出是预测下一个时间戳为词典中任意字符的概率，因而该输出是维度为`vocab_size`的向量。\n",
    "\n",
    "当序列中某一个时间戳的输入为一个样本数为`batch_size`（对应模型定义中的$n$）的批量，每个时间戳上的输入和输出皆为尺寸`batch_size * vocab_size`（对应模型定义中的$n \\times x$）的矩阵。假设每个样本对应的隐含状态的长度为`hidden_dim`（对应模型定义中隐含层长度$h$），根据矩阵乘法定义，我们可以推断出模型隐含层和输出层中各个参数的尺寸。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaa\n",
      "will use gpu(0)\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..') \n",
    "import utils \n",
    "ctx=utils.try_gpu() \n",
    "print('will use',ctx) \n",
    "\n",
    "input_dim=vocab_size \n",
    "# 隐含状态长度 \n",
    "hidden_dim=256\n",
    "output_dim=vocab_size \n",
    "std=0.01\n",
    "\n",
    "def get_params():\n",
    "    # 隐含层\n",
    "    W_xh=nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx) \n",
    "    W_hh=nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx) \n",
    "    b_h=nd.zeros(hidden_dim,ctx=ctx) \n",
    "    \n",
    "    # 输出层 \n",
    "    W_hy=nd.random_normal(scale=std,shape=(hidden_dim,output_dim),ctx=ctx) \n",
    "    b_y=nd.zeros(output_dim,ctx=ctx) \n",
    "    params=[W_xh,W_hh,b_h,W_hy,b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad() \n",
    "    return params \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 定义模型\n",
    "\n",
    "当序列中某一个时间戳的输入为一个样本数为`batch_size`的批量，而整个序列长度为`num_steps`时，以下`rnn`函数的`inputs`和`outputs`皆为`num_steps` 个尺寸为`batch_size * vocab_size`的矩阵，隐含变量$\\mathbf{H}$是一个尺寸为`batch_size * hidden_dim`的矩阵。该隐含变量$\\mathbf{H}$也是循环神经网络的隐含状态`state`。\n",
    "\n",
    "我们将前面的模型公式翻译成代码。这里的激活函数使用了按元素操作的双曲正切函数\n",
    "\n",
    "$$\\text{tanh}(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}$$\n",
    "\n",
    "需要注意的是，双曲正切函数的值域是$[-1, 1]$。如果自变量均匀分布在整个实域，该激活函数输出的均值为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs,state,*params):\n",
    "    # inputs: num_steps 个尺寸为batch_size*vocab_size 矩阵。\n",
    "    # H:尺寸为 batch_size * hidden_dim 矩阵 \n",
    "    # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵。\n",
    "    H=state \n",
    "    W_xh,W_hh,b_h,W_hy,b_y=params \n",
    "    outputs=[]\n",
    "    for X in inputs:\n",
    "        H=nd.tanh(nd.dot(X,W_xh)+nd.dot(H,W_hh)+b_h) \n",
    "        Y=nd.dot(H,W_hy)+b_y \n",
    "        outputs.append(Y)\n",
    "    return (outputs,H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做个简单的测试：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length: 3\n",
      "output[0] shape: (2, 2613)\n",
      "state shape: (2, 256)\n"
     ]
    }
   ],
   "source": [
    "state=nd.zeros(shape=(data.shape[0],hidden_dim),ctx=ctx) \n",
    "params=get_params() \n",
    "outputs,state_new=rnn(get_inputs(data.as_in_context(ctx)),state,*params) \n",
    "print('output length:',len(outputs))\n",
    "print('output[0] shape:',outputs[0].shape) \n",
    "print('state shape:',state_new.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 预测序列\n",
    "\n",
    "在做预测时我们只需要给定时间0的输入和起始隐含变量。然后我们每次将上一个时间的输出作为下一个时间的输入。\n",
    "\n",
    "![](../img/rnn_3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(rnn,prefix,num_chars,params,hidden_dim,ctx,idx_to_char,char_to_idx,get_inputs,is_lstm=False):\n",
    "    # 预测 以 prefix 开始的接下来的 num_chars 个字符 \n",
    "    prefix=prefix.lower()\n",
    "    state_h=nd.zeros(shape=(1,hidden_dim),ctx=ctx) \n",
    "    if is_lstm:\n",
    "        # 当 RNN 使用LSTM时才会用到，这里可以忽略。\n",
    "        state_C=nd.zeros(shape=(1,hidden_dim),ctx=ctx) \n",
    "    output=[char_to_idx[prefix[0]]]\n",
    "    for i in range(num_chars+len(prefix)):\n",
    "        X=nd.array([output[-1]],ctx=ctx) \n",
    "        # 在序列中循环迭代隐含变量。\n",
    "        if is_lstm:\n",
    "            # 当 RNN使用LSTM时才会用到，这里可以忽略。\n",
    "            Y,state_h,state_c=rnn(get_inputs(X),state_h,state_c,*params)\n",
    "        else:\n",
    "            Y,state_h=rnn(get_inputs(X),state_h,*params) \n",
    "        if i<len(prefix)-1:\n",
    "            next_input=char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            next_input=int(Y[0].argmax(axis=1).asscalar()) \n",
    "        output.append(next_input) \n",
    "    return ''.join([idx_to_char[i] for i in output]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 梯度剪裁\n",
    "\n",
    "我们在[正向传播和反向传播](../chapter_supervised-learning/backprop.md)中提到，\n",
    "训练神经网络往往需要依赖梯度计算的优化算法，例如我们之前介绍的[随机梯度下降](../chapter_supervised-learning/linear-regression-scratch.md)。\n",
    "而在循环神经网络的训练中，当每个时序训练数据样本的时序长度`num_steps`较大或者时刻$t$较小，目标函数有关$t$时刻的隐含层变量梯度较容易出现衰减（valishing）或爆炸（explosion）。我们会在[下一节](bptt.md)详细介绍出现该现象的原因。\n",
    "\n",
    "为了应对梯度爆炸，一个常用的做法是如果梯度特别大，那么就投影到一个比较小的尺度上。假设我们把所有梯度接成一个向量 $\\boldsymbol{g}$，假设剪裁的阈值是$\\theta$，那么我们这样剪裁使得$\\|\\boldsymbol{g}\\|$不会超过$\\theta$：\n",
    "\n",
    "$$ \\boldsymbol{g} = \\min\\left(\\frac{\\theta}{\\|\\boldsymbol{g}\\|}, 1\\right)\\boldsymbol{g}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params,theta,ctx):\n",
    "    if theta is not None:\n",
    "        norm=nd.array([0.0],ctx) \n",
    "        for p in params:\n",
    "            norm+=nd.sum(p.grad**2)\n",
    "        norm=nd.sqrt(norm).asscalar() \n",
    "        if norm>theta:\n",
    "            for p in params:\n",
    "                p.grad[:]*=theta/norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 训练模型\n",
    "\n",
    "下面我们可以还是训练模型。跟前面前置网络的教程比，这里有以下几个不同。\n",
    "\n",
    "1. 通常我们使用困惑度（Perplexity）这个指标。\n",
    "2. 在更新前我们对梯度做剪裁。\n",
    "3. 在训练模型时，对时序数据采用不同批量采样方法将导致隐含变量初始化的不同。\n",
    "\n",
    "### 困惑度（Perplexity）\n",
    "\n",
    "回忆以下我们之前介绍的[交叉熵损失函数](../chapter_supervised-learning/softmax-regression-scratch.md)。在语言模型中，该损失函数即被预测字符的对数似然平均值的相反数：\n",
    "\n",
    "$$\\text{loss} = -\\frac{1}{N} \\sum_{i=1}^N \\log p_{\\text{target}_i}$$\n",
    "\n",
    "其中$N$是预测的字符总数，$p_{\\text{target}_i}$是在第$i$个预测中真实的下个字符被预测的概率。\n",
    "\n",
    "而这里的困惑度可以简单的认为就是对交叉熵做exp运算使得数值更好读。\n",
    "\n",
    "为了解释困惑度的意义，我们先考虑一个完美结果：模型总是把真实的下个字符的概率预测为1。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 1$。这种完美情况下，困惑度值为1。\n",
    "\n",
    "我们再考虑一个基线结果：给定不重复的字符集合$W$及其字符总数$|W|$，模型总是预测下个字符为集合$W$中任一字符的概率都相同。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 1/|W|$。这种基线情况下，困惑度值为$|W|$。\n",
    "\n",
    "最后，我们可以考虑一个最坏结果：模型总是把真实的下个字符的概率预测为0。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 0$。这种最坏情况下，困惑度值为正无穷。\n",
    "\n",
    "任何一个有效模型的困惑度值必须小于预测集中元素的数量。在本例中，困惑度必须小于字典中的字符数$|W|$。如果一个模型可以取得较低的困惑度的值（更靠近1），通常情况下，该模型预测更加准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd \n",
    "from mxnet import gluon \n",
    "from math import exp \n",
    "\n",
    "def train_and_predict_rnn(rnn,is_random_iter,epochs,num_steps,hidden_dim,\n",
    "                         learning_rate,clipping_theta,batch_size,\n",
    "                        pred_period,pred_len,seqs,get_params,get_inputs,\n",
    "                         ctx,corpus_indices,idx_to_char,char_to_idx,\n",
    "                         is_lstm=False):\n",
    "    if is_random_iter:\n",
    "        data_iter=data_iter_random\n",
    "    else:\n",
    "        data_iter=data_iter_consecutive \n",
    "    params=get_params() \n",
    "    softmax_cross_entropy=gluon.loss.SoftmaxCrossEntropyLoss() \n",
    "    for e in range(1,epochs+1):\n",
    "        # 如使用相邻批量采样，在同一个epoch中，隐含变量只需要在该epoch 开始的时候初始化 \n",
    "        if not is_random_iter:\n",
    "            state_h=nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx) \n",
    "            if is_lstm:\n",
    "                state_c=nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx) \n",
    "        train_loss,num_examples=0,0 \n",
    "        for data,label in data_iter(corpus_indices,batch_size,num_steps,ctx):\n",
    "            # 如果使用随机批量采样，处理每个随机小批量前都需要初始化隐含变量。\n",
    "            if is_random_iter:\n",
    "                state_h=nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx) \n",
    "                if is_lstm:\n",
    "                    # 当RNN使用 LSTM 时才会用到，这里可以忽略 。\n",
    "                    state_c=nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx) \n",
    "            with autograd.record():\n",
    "                # outputs 尺寸：（batch_size,vocab_size) \n",
    "                if is_lstm:\n",
    "                    # 当RNN 使用LSTM时才会用到，这里可以忽略 \n",
    "                    outputs,state_h,state_c=rnn(get_inputs(data),state_h,\n",
    "                                               state_c,*params)\n",
    "                else:\n",
    "                    outputs,state_h=rnn(get_inputs(data),state_h,*params) \n",
    "                # 设 t_ib_j 为 i时间批量中的j元素 \n",
    "                # label尺寸：（batch_size * num_steps \n",
    "                label=label.T.reshape((-1,))\n",
    "                # 拼接outpus，尺寸：(batch_size * num_steps,vocab_size) .\n",
    "                outputs=nd.concat(*outputs,dim=0) \n",
    "                #经过上述操作，outputs 和label已经对齐 \n",
    "                loss=softmax_cross_entropy(outputs,label) \n",
    "            loss.backward() \n",
    "            grad_clipping(params,clipping_theta,ctx) \n",
    "            utils.SGD(params,learning_rate) \n",
    "            train_loss+=nd.sum(loss).asscalar() \n",
    "            num_examples+=loss.size \n",
    "        if e%pred_period==0:\n",
    "            print(\"Epoch %d .Perplexity %f \"%(e,exp(train_loss/num_examples)))\n",
    "            for seq in seqs:\n",
    "                print('-',predict_rnn(rnn,seq,pred_len,params,\n",
    "                                     hidden_dim,ctx,idx_to_char,char_to_idx,get_inputs,is_lstm)) \n",
    "                print() \n",
    "                \n",
    "            \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下定义模型参数和预测序列前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=200\n",
    "num_steps=35\n",
    "learning_rate=0.1 \n",
    "batch_size=32 \n",
    "softmax_cross_entropy=gluon.loss.SoftmaxCrossEntropyLoss() \n",
    "\n",
    "seq1='分开'\n",
    "seq2='不分开'\n",
    "seq3='战争中不对'\n",
    "seqs=[seq1,seq2,seq3] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先采用随机批量采样实验循环神经网络谱写歌词。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 .Perplexity 180.413410 \n",
      "- 分开 我不能 你说我 你不能 你的笑 我不能 你的笑 我不能 你说我 你不能 你的笑 我不能 你的笑 我不能 你说我 你不能 你的笑 我不能 你的笑 我不能 你说我 你不能 你的笑 我不能 你的笑 我不能 \n",
      "\n",
      "- 不分开 我知道 我不能 你说我的爱情 我用着你的美 我知道 我不能 你说我的爱情 我用着你的美 我知道 我不能 你说我的爱情 我用着你的美 我知道 我不能 你说我的爱情 我用着你的美 我知道 我不能 你说我的\n",
      "\n",
      "- 战争中不对 我不能 我不能 你的笑界 我们不到 我的笑 我不能 你说我 你不能 你的笑 我不能 你的笑 我不能 你说我 你不能 你的笑 我不能 你的笑 我不能 你说我 你不能 你的笑 我不能 你的笑 我不能 你说\n",
      "\n",
      "Epoch 40 .Perplexity 62.453631 \n",
      "- 分开 你说你不会 你说我不到 让我知道你 你我的泪 你说我爱一个人 我想要你说你 有不是 你好了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 \n",
      "\n",
      "- 不分开 我在我们一起一步 在车海中的溪道 我说了我的爱情 我想你说你最后 是我为我的爱情  我们一双 你好声 我不能 这样走 你不见 我说了 你说了 我手 如果我 你不到 让我知道你离开 你的睫不该 你的爱笑\n",
      "\n",
      "- 战争中不对 让我再乘你 你说的感觉 没有了空 有回忆 你好一步 我在等待 你就会听我 你在我 再不到 你说我爱你的爱情 我用你的爱我 你不想 你说我爱你的爱情 没有你们的手 我想要这个你 在我的红 你说我 一个人\n",
      "\n",
      "Epoch 60 .Perplexity 31.115407 \n",
      "- 分开始演 我想的爱情 还在很美 我轻轻 我不想 你说我爱我 我我要我我 你怎么会不见 你不会 有你走的爱情 我爱你 我爱我 我爱我 你说我 你说我 你说我 你我我 一个人 人不回 说好走 一步一步三步四 望\n",
      "\n",
      "- 不分开 我知道 我们 我只是这种龙 放抹记 你怎么 我手 你我 我怎么打开 你的时光 我不能 想我我 我们我 我爱你 你走我 我手 我不需 你说你的笑 在你烦你 我不懂 你说我 我不要 你说我爱我 你我要 我\n",
      "\n",
      "- 战争中不对我  我们 一种蜂 练撕了 后一起 我们了 你手  我爱你的爱情 我轻轻地 我想想要你 我的世界 你已经不了 你说不爱我 我我爱你怎么 因为我 爱你是你的爱情 我轻轻地了 我们的感觉我的了 我只是 你眼\n",
      "\n",
      "Epoch 80 .Perplexity 20.734519 \n",
      "- 分开始不该 你的声色面徘  你想得听我  我知道你我的世界 你完睛 我没有 我牵不到 我会能难见 有什么这个世  我们持风火轮 我们溉你 M魔SI 爱你的美 我不了 我爱了爱我的天 鲜你的美丽 让你们的世界\n",
      "\n",
      "- 不分开 我心道你我遇着 你 全我 我不懂 我要离开心的笑 这样迭的可爱 我爱你离开 天涯之外 你无法黑的 游家 我们在风 纷得到见 我想着你怎么没有 扯不能够 我只想要同 我们不会有 天单在我 爱你是 我们 \n",
      "\n",
      "- 战争中不对我 就算不觉 我跟了这节奏 后知后觉 快使了你 我用了吧 你在一场 心破 对你 一个旅惑 我爱你走  你的依不属 我说了爱情的小法 你说我不懂不该 不该在这时候 你会好你 今小S 不能不要 你已经离开我\n",
      "\n",
      "Epoch 100 .Perplexity 15.591565 \n",
      "- 分开始不停 你说我爱情难堪 我在等你 带着你的手爱 我送你的爱写在西元前 深埋在美索不达米亚平原 几十一天 马岸一回 这个止心的溪活 谁说了我们都难完 想我该给了 你将的感觉 不能承想 我不能再想 我不能再\n",
      "\n",
      "- 不分开 我有一双 没用心的手 就让我受的太多 还在这手 想要我 爱过的人都用 一道 时光机 你不见  我小香陪 我在街元 无续了谁不用 一路正气 一个一步 狂感到底 我在一起 纷i 想不 我不要再想 我不说 \n",
      "\n",
      "- 战争中不对我  我已经快写化 我会在我们膀 我要的地里不知道 然可以你的我 说我有多烦  让我还在这条 我的指光 再断了一个 让我知道你 那场 不想 你的笑色 不过在你 我我有 你好离 你仍 一个人间的思念 在空\n",
      "\n",
      "Epoch 120 .Perplexity 12.529925 \n",
      "- 分开始不停 你说我爱情 这里会没有我的脸  你终得还我 我们指还是一场 拯量如果你的美 我用弯的叹 再在最后的世界 让我们 半兽人 的灵魂 翻滚 停止忿 有人爱 一九四步 我们弯的脚爱女人 坏柔的让我疯狂的\n",
      "\n",
      "- 不分开 爱反再这只是我妈的 一路让我们在你 谁在那笑零 雨一定在梦高 我看着远 离开心了 说你说你完 我不需 我要走过了一口汗 再想到你的可爱 在练地忘了雨 我的身界还被摧 却不得你我的笑都不能够 就是这为我\n",
      "\n",
      "- 战争中不对我 想再爱你不可以 你不想为我 我不能再想 我不能再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我\n",
      "\n",
      "Epoch 140 .Perplexity 10.373392 \n",
      "- 分开始不停 你说你的一口就能不着 你说你无眼　 我送你的美写在西元前 深埋在美索不达米亚平原 爱你 全不回 你没有爱我 这里那时光 没有了我  没有人不再 让你的笑爱 我在我的爱泪 让人群的风滴 一路往心演\n",
      "\n",
      "- 不分开 我等待异族在我 是你的香色 让它後在城口 我只想你 M我掉 你走进 你手 我 我等着地的感事 你的声笑 微微不掉 如此 对上一路 努等我 爱不对我 你那王者的沉默 看着我 被我诱惑 众神都已经不了我 \n",
      "\n",
      "- 战争中不对我 想再想当我  你发当的痕迹 我站你的爱写在西元前 深埋在美索不达米亚平原 爱你要的完笑 我只要还是一场 你说的爱我 舍不得吃会微笑的痛 时间的我靠 不能再想 我不能够想 我的指 这样我 经如变成了 \n",
      "\n",
      "Epoch 160 .Perplexity 8.807850 \n",
      "- 分开始明白 我想想你 M着球 你怎么很哭上 摇仔 故事念 为什么 一直在窗落 才好它诉 泥泞序  破终 雨纷纷 哈a Ea 车i   雨纷 那鲁纷 的灵魂 剩纯 是非 忘记 梦记 忘记 xi xa xi x\n",
      "\n",
      "- 不分开 我心中你眼泪着 我想想还是不错 能说你的眼泪 让你们的爱自 想念你却慢慢 不要再这样打我妈妈 难道你手不会痛吗 那个人 如果我抬起头 有谁都在我 没有你的手 你说我舍了手 你说你会哭 我不能再想 我不\n",
      "\n",
      "- 战争中不对我 你再爱离开 我根愉如龙 当敌人是空 我左右开弓 我们势如虹 将炮马入宫 碾过你懊丧的脸孔 看我 我手指放松 我目光如龙 当敌人是空 我左右开弓 我们势如虹 将炮马入宫 碾过你懊丧的脸孔 看我 我手指\n",
      "\n",
      "Epoch 180 .Perplexity 7.647007 \n",
      "- 分开始不停  没有已经得飘傲我说 不用太烦了 我们定事静没关集 读好有时光 用心如果再我 都知道失去 我说不是我现 我不说 我没有思样抱 你就让你说离 那白是我的手 我牵着你们的世界 像是我的出地音你 一只\n",
      "\n",
      "- 不分开 我心中 情绪美碎不 我亲一定好好  你说的还是不了 我感去 你好 我手指已经离 因为我们来见 我一道对你 一个江山 再回忆 旧远 在一路的梦 在车窗外的山角 消失去心落写 那天声风风 在瓶要等待 这里\n",
      "\n",
      "- 战争中不对我  这个世界你的温子 微微地对我 一个江血 我轻轻地尝一口 你说的爱情 还在用空琶转 我们心上一直 你的手反一直 想到底离开 (尖声) 是谁是用太 把被 我想了 情尽了 不爱你烦了 不想  也小的山境\n",
      "\n",
      "Epoch 200 .Perplexity 6.696068 \n",
      "- 分开始不停 你的爱不已 自然却是空子 你看着 你说 看著多 再爱你 起你一种 我只想带 你那一个 好好 那些 太威的脚 你说的后 我的手子 就让我们的太多 双的睫毛 我会亲着 一麻袋的爱跟 这里的好想 你爱\n",
      "\n",
      "- 不分开 我在等忆 喝分开始不要我 痛身为龙 我等待努的风 在水落着动的 在一起 废好好 你说不起 我的眼音 你微微不了口 爱你的可爱跟你走擦 誓位上风 一个世 的灵命 雨纯 靠远古存 的指情旁 忘了吧面 才小\n",
      "\n",
      "- 战争中不对我  我的回头 让它的容都是我牵着你 谁在一口梦我的手 我说不到她的微笑 洋溢一口的天是 一种四怅 在回忆 的小巷 是因为我的画面 谁的速影 我会好着 一麻上的手  你离会听我的乐 我牵着一些 份世英的\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_and_predict_rnn(rnn=rnn,is_random_iter=True,epochs=200,num_steps=35,\n",
    "#                      hidden_dim=hidden_dim,learning_rate=0.2,\n",
    "#                      clipping_theta=5,batch_size=32,pred_period=20,\n",
    "#                      pred_len=100,seqs=seqs,get_params=get_params,\n",
    "#                      get_inputs=get_inputs,ctx=ctx,\n",
    "#                      corups_indices=corpus_indices,\n",
    "#                       idx_to_char=idx_to_char,\n",
    "#                      char_to_idx=char_to_idx) \n",
    "\n",
    "train_and_predict_rnn(rnn=rnn, is_random_iter=True, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.2,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再采用相邻批量采样实验循环神经网络谱写歌词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 .Perplexity 173.090295 \n",
      "- 分开 我们的爱我 我的爱觉 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情\n",
      "\n",
      "- 不分开 我们的爱我 我的爱觉 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情 我的爱情\n",
      "\n",
      "- 战争中不对  我在我的爱 再一种的茶 我在我的爱 我一道的人 再一种的茶 我在我的爱 我一道的人 再一种的茶 我在我的爱 我一道的人 再一种的茶 我在我的爱 我一道的人 再一种的茶 我在我的爱 我一道的人 再一种\n",
      "\n",
      "Epoch 40 .Perplexity 57.184929 \n",
      "- 分开 我的灵 已经不是 我的灵魂 我不是我 你我已经的  你说你的怀  我用上的特 只让我们 我一直 我不想 我不开 我不想 我不想 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 \n",
      "\n",
      "- 不分开 我知道 一起 E 我的爱觉 我不想 你说我的爱  一路 我在等 我不该 我不想 我不想 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 \n",
      "\n",
      "- 战争中不对  爷在人们的 简盒 我的灵 我不该 我不想 我不想 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不再 我不\n",
      "\n",
      "Epoch 60 .Perplexity 31.130925 \n",
      "- 分开 我们的 娘时间 一九 三我的眼 是谁在 不想 我的想 你说一起一直 从来来的人 是我们的半 别你的红 再了不安 我的爱 只让我们不见 我的你 一定一直 在谁我的手 我一定 我的终 只说不出再的爱 再多\n",
      "\n",
      "- 不分开 她首的时 只像回忆 我的感觉 你不是 你说我的爱 想一种 一页 我们不着我们 你的泪 已经不了开 再在等你就 感你的美 有一个人 叫后一直 我的灵经我不够 在我们 半兽人 的灵魂 翻滚 停止忿 在我们\n",
      "\n",
      "- 战争中不对 哎说你的茶 有一种味道叫做家 陆羽泡的茶 有一种味道叫做家 陆羽泡的茶 有一种味道叫做家 陆羽泡的茶 有一种味道叫做家 陆羽泡的茶 有一种味道叫做家 陆羽泡的茶 有一种味道叫做家 陆羽泡的茶 有一种味\n",
      "\n",
      "Epoch 80 .Perplexity 22.440102 \n",
      "- 分开 我们是 你说了 我说了 一壶 我翻速的灵  我用得你离不要 说你说你不见 想不能你想微 我没了豆腐  我是你的回 一定 不是我有着 你的生变你的嘴角. 我一了黑火 将开始出 (一个 的溪候 单在一遍 \n",
      "\n",
      "- 不分开 我可道不想 你说我感你 你说着 还是再 我喜着 你说我不多 我身之无 我有解烦你 你的温 一直人太 在谁 那鲁湾 一片 一场 太来 我在我们都 你 你的微笑  你说你的手 想然 你的美 只让我 谁活 \n",
      "\n",
      "- 战争中不对我 我只是你不要我 我知道一种称 在路的我想在 当好的结水  我说你的手密 我都的小 再怎么会有 但的灵挂 我不多 我不能 爱手的爱 我要的感 你的微笑 我不想 你不要 我不是 想说你 一口 你在我的秘\n",
      "\n",
      "Epoch 100 .Perplexity 17.734549 \n",
      "- 分开 (你的美式 我都起悔远 我两愉不想 你说你爱你的怀 一起的酒蜜 让我们想半 不能不去 你的爱情 你当得后 一直到销 我用杯无点 用甚的资 那么又了 千年的钥 你怎么会白 你不能 原我的事不出 一色 强\n",
      "\n",
      "- 不分开 寒色的爱 只人在一遍 强不该的烦恼 我的爱 还让不出她 这人的战 只过过了 (的爱 我怎么 太咬 Ee 如果  天 Y声 Ya He 我爱 我抬了 消 再 我面爱 后的歌 一直一颗 狂a一双 我在终者\n",
      "\n",
      "- 战争中不对我 我只是我不要再久 你想要 我说你的秘质 让我们 半兽人 的灵魂 单滚 停止忿存在永神 我的崩溃在窗外零碎 我右你等腐个大 还想要你的微笑 让你们 半兽人 的灵魂 单滚 停止忿 让我在 我抬红 一起歌\n",
      "\n",
      "Epoch 120 .Perplexity 15.022858 \n",
      "- 分开 我深你 爱果我 爱着我 一个好 一片一直 我一路差北 一口说你的美雪 你说店好的手 是否种变的溪 篮向是得天  在练光上回忆 你发的爱  我用多 你的灵魂 我不是 我说你 爱情 那些 我的解 面不是 \n",
      "\n",
      "- 不分开 我会在你有话 有一种对单  谁不了得我  你走一种的美 在幸完 一道 m时颗的太鞋 一路 训壮着我的肺 面对 这些 太威胁 太强烈 从一生的发觉 有灵水 让我的解 永不了 情了飘 我喜想 你说一边一步\n",
      "\n",
      "- 战争中不对我 我可以一种的心定 你眼泪小的手 这样不会 狂的地情 我不能 我说我 爱入  让我爱谁 众神在空  我用边 回忆 那e山 Na 一直 He 我的路l面 我想等你的嘴 是我 你微指 让我有人又心 每然一\n",
      "\n",
      "Epoch 140 .Perplexity 12.766627 \n",
      "- 分开 (不是这么 我们的烦 我妈的感 你我们的灵 你让我看不见 只有你说不恋一个人 我没知道 你小没有爱 你们不上 我的爱毛 你已一个人 不忍麻护  谁我不觉 我只有了我 谁是花里  你在一起 我们的感 你\n",
      "\n",
      "- 不分开 简可的眼泪 我牵头害 一颗歌 的喜魂 不开 我说爱 一生  我在武宙永的自己 有一股 马转 E Ya 那a湾 Na E Na Ya Hei 哦~ E Na Ya He  纷 爱情过 太多走 不回来 一\n",
      "\n",
      "- 战争中不对我 我才能能不开 细永的风被  我看得一场热 只能穿你身家 一些爱气 为了完不到  说在你面再 我的爱你 微不开我 跑着不应 我都是感情 这些人 放每一直 我的灵经 还让回出 一生勾销 我用世无  我忘\n",
      "\n",
      "Epoch 160 .Perplexity 10.931328 \n",
      "- 分开 哎哟上 一边四  我们 那种人 一回 等我的意 你那里雪 谁 我 泪坚我的地 我对着你的嘴 很问 这些 太威胁 太异烈 从来不喊累 爱中月 小人 是否在 中累了 我们爱 一边事 变悔了你 全 E 我用\n",
      "\n",
      "- 不分开 他能的战 只过了一遍 印于的天 你的声音 你离的感 我不要 是谁在窗中 不需要 开成的笑 你一声 无壮走 让我的爱式 我加着你 我有有话 这场界 的表情只剩了一种 等待英雄 我就是那条龙 哼乡为龙 我\n",
      "\n",
      "- 战争中不对 不该 你的微笑　 时了地 一天 我就再听我的 表面 我的爱 一直不多 我的感经 你已听得见 你说没有些 你知道 我的狠 我对过 一道两 一起四好 分无一直 我的笑子你就够  你说一起 说我的美 这后又\n",
      "\n",
      "Epoch 180 .Perplexity 9.775206 \n",
      "- 分开了时间 在什么甲想 那个了 小在飘窗就快退 地开我脸越  我的下l　 有了水 几地上的套 有谁的丘 还再一直到 三字中是一起 景望发 一直到你的美 你发之山的黑 沉默的喊道变 记忆的风气　 我想想对我没\n",
      "\n",
      "- 不分开 简叭的爱陪 你用回忆  我用卑 我小的烦 你就要一直的我们 后来不间 你的那魂属 我加起见 一颗爱你的美 接 我这么黑该 我目等 离开 Ee了  再来 一直不到 我的生子出 断了她 几家的事 你看着你\n",
      "\n",
      "- 战争中不对　 不是你的话 我要的爱情 你让完爱你的爱 还起不能感觉的我会 我喜泪 有不了已经 我们的感 我有著明  我的乐 离开 看不 一遍 我们 再问一口 在山着 一间 三e 太去 是谁的用 在你们追半 我 甩\n",
      "\n",
      "Epoch 200 .Perplexity 8.680956 \n",
      "- 分开始白 没有让 时间没有个人天 只要去 千哟 E开了 我说爱 你说我选去  我 也想我放 你永远 让不要 不下你 让我的眼泪 我都是你不对我 我只是你不过 要我獨缺我 你不是我想我 多不让我要我的爱情 就\n",
      "\n",
      "- 不分开 人能再是你 你说着 不起再酒 你一声 我却离 一直走化 你的灵魂出 我加想着你说 是你说你读着一场称 我身前就能 天然是很走 我们的感开 温暖中 旧人在窗练 我等待打来 谁甜北支不够 景色入秋 漫天黄\n",
      "\n",
      "- 战争中不对我 就像已经我掉 几次 我会生不该 你的身魂 还让我不会 你好的小笑 让我们 你有一路武 叫破在战 却得回忆 青身生的情 是我说爱 沉透了这不上 消谢上愧 风武之人 为我的思念厚 我想想你太人 我用泪 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn=rnn, is_random_iter=False, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.2,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。\n",
    "\n",
    "## 结论\n",
    "\n",
    "* 通过隐含状态，循环神经网络适合处理前后有依赖关系时序数据样本。\n",
    "* 对前后有依赖关系时序数据样本批量采样时，我们可以使用随机批量采样和相邻批量采样。\n",
    "* 循环神经网络较容易出现梯度衰减和爆炸。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 调调参数（例如数据集大小、序列长度、隐含状态长度和学习率），看看对运行时间、perplexity和预测的结果造成的影响。\n",
    "* 在随机批量采样中，如果在同一个epoch中只把隐含变量在该epoch开始的时候初始化会怎么样？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/989)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 .Perplexity 292.782021 \n",
      "- 分开 我不能 我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱\n",
      "\n",
      "- 不分开 我不能 我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱\n",
      "\n",
      "- 战争中不对 我们在 我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱  我的爱  我的爱 我的爱\n",
      "\n",
      "Epoch 40 .Perplexity 160.751138 \n",
      "- 分开 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再\n",
      "\n",
      "- 不分开 我不想要 你不开 你说我 我不见 你说我 你不到 我不要 不要我 爱你 我不指 你说我 爱你 我不指 你说我 爱你 我不指 你说我 爱你 我不指 你说我 爱你 我不指 你说我 爱你 我不指 你说我 爱\n",
      "\n",
      "- 战争中不对 我说我 爱你 我不想 你说我 你不见 你说我 你不到 我不要 不要我 爱你 我不指 你说我 爱你 我不指 你说我 爱你 我不指 你说我 爱你 我不指 你说我 爱你 我不指 你说我 爱你 我不指 你说我\n",
      "\n",
      "Epoch 60 .Perplexity 90.677435 \n",
      "- 分开 你说我 你我我 我不见 不要再 我不要 不要再 我不要再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再\n",
      "\n",
      "- 不分开 我不能要想我要我 你不能再想要我 你不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想\n",
      "\n",
      "- 战争中不对我 不是 我不再再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 \n",
      "\n",
      "Epoch 80 .Perplexity 54.553038 \n",
      "- 分开 我们不再再 我不要不想 你的笑 有一种 我会的 幸福了 我面了 不好了 我好了 不说了 我爱了 想说了 我爱了 你说了 不好 我的笑  你在我的爱  让我们 你说了 你说了 你说了 你说了 你说了 不\n",
      "\n",
      "- 不分开 我知道还想 我不要再想 我不要再想 我的 我有 我想  不到 你的手  你说我的爱 有一种味道叫做家 (一路的人是在雨 我说不着 我的笑 我用格 你的终  你这一起 有一个人面 我在等 你的终 我说 \n",
      "\n",
      "- 战争中不对 一种 等壮 我的指  你这一起 有一个人面 我们的感 我不了 你的终 你说 你的灵  我在你的手乐  我们不见 我的声 这样 这些 太威 xe xi xi xi xa xa xa xa xa xa x\n",
      "\n",
      "Epoch 100 .Perplexity 37.283460 \n",
      "- 分开 没有你的爱我有了收 我想想到 牵不会 你我我听我 你不能再想 我不 我不 我不要再想你 不知不觉 我已经离开我 不知道觉 你的爱音 你不会不到 我的 你想 我想  不到再 你没有笑 我没不是 你没笑 \n",
      "\n",
      "- 不分开 也不道 我手了 爱不了太 我会不要 你微笑 想活了一个人 不用因 让我的眼泪 你的爱音 你离开的弦 再说的墨 我只不想 我不要再想 我的 音好 我去 你不 我不 我不 我不要我想你 有不要 你我再爱着\n",
      "\n",
      "- 战争中不对我 不再   我的爱觉 是不是你 一首公 三家在一个人 不用靠在 全在手的 我有一场 我会是离 我不想要想 我不能再想 我不要再想 我不要再想 我不 我不 我不要再想你 不知不觉 你已经离开我 不知道觉\n",
      "\n",
      "Epoch 120 .Perplexity 28.044477 \n",
      "- 分开始没有 我想一个心都的蜻空 我想我离开了你的道 这样 我想到再了一口 我们的地我 狂不能够  你们 不不到 你说了 不说了 开爱了 别人了 梦 说不见 一口在我们 你说了 不到 你一边的手觉 消在我们在\n",
      "\n",
      "- 不分开 没有一场 我的世界 你不要不到 我的  假过 这些 我 你我听 我们 我不  不是  是我的热 我有一起 想不到 你的嘴 不用 这些 我的指  你这一直 有一个 等 我们 你说了 你好了 我我的泪  \n",
      "\n",
      "- 战争中不对 一些 再想在我的肺 我在格 是纷 不过去 你 我听得得 说一生在我们不到 我不想爱这样 我知道在你读的世一页 我在轻地只有我手的手 不算问着我知道 不要再 一个一步三步四颗 连成线一起 北马的铁绪 我\n",
      "\n",
      "Epoch 140 .Perplexity 22.273471 \n",
      "- 分开始不能 你说你爱笑　 我还是你在微 是我们的半前   我跟觉舞特热 我知道原 我的没笑 有一种的季 我说你的爱 再一种味道叫做家 (风南不该道风 我想是一种往找法的 手缓泡的茶 在幅种外身都太家 你会不\n",
      "\n",
      "- 不分开 没是什么我想要 是我会了一个一步 你说我爱别个人伏 你说我会爱会得走 你说我也知道好 你怎么会想得 你说你的爱 我怎么没有 有了 我的指  你在我们想 我不能够 我不想要一样 我要等你一样 我不能不再\n",
      "\n",
      "- 战争中不对我 要不到 我沉你这样的脸 你等着爱 想无法的生口 我爱你的爱 我一种味道叫一直 他的终口 你微听着 说我爱 你不了 不能不再说 我爱一起在牵待 我不想在这人就说要很自 不要麻烦了 为你的人意 是我的爱\n",
      "\n",
      "Epoch 160 .Perplexity 18.175879 \n",
      "- 分开始没有 那里 我想吃这个人 不知道觉 只有一切 用让一直 我不需 再到一遍人重不 她景的爱我 狂我的美  谁在我们好有多己的画好 想去的过 在谁的回爱的天 我们一直默纷 我的光光在 我脸容不想 你的转变\n",
      "\n",
      "- 不分开 也是那么在那个光我还在 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦\n",
      "\n",
      "- 战争中不对 一些 用高 不用麻 爱是这 雷 在我一口放不  我们着你是谁 有一道的梦口 我们遇 你离开一个人 在我们 半兽人 的灵魂 翻滚 停止忿存在永空 在 如了的旧色 在风中盘找有 有下我的不觉 你的笑音 这\n",
      "\n",
      "Epoch 180 .Perplexity 15.614635 \n",
      "- 分开始不需 你说你爱一个 你不可要这样要我抱 我们不能说没有一种手心 我一路向北 离开有你的季节 你说你好累 他的让子面了 雨彻了 人在世界 不该不及 你已经 是我们的太快 像我看着你 我不需要我 你我 我\n",
      "\n",
      "- 不分开 没有道 该人的一个 在空地回来 我知道就你 我不要 你说我 爱情我 的灵魂 单纯里 不能不到 为什么我的手 我没人在远 我怎么再到 我开势如虹 将敌人是空 我左右开弓 我气势如虹 将敌人是空 我左右开\n",
      "\n",
      "- 战争中不对 一生 你的指 有谁 这些纷 雨a E霍霍 霍霍 E霍 xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi x\n",
      "\n",
      "Epoch 200 .Perplexity 13.561465 \n",
      "- 分开始不想 你那一口是气 我们的远气　 我有发看我要你去 我 泪和一样热步 一直被旧我 他一路调  你我 我我指 我看想这不见 你说你看开 你不要再怎 我们抱就很我 天爱一种子明的风  (得飘过了我手单 看\n",
      "\n",
      "- 不分开 没是否 我想要这一页 我 我们一直错 我用香无动 那尔银尽不够 景色入秋 他是用双 就让回忆永远停在 那里  你是我不在 我不是不想 我的爱变不被我 我我在我们上了 怎么会的你比一场 我说你是太个错 \n",
      "\n",
      "- 战争中不对我 o起  终人在听雨 我听你的爱 我怎么后 没有个人的姿争 一起 训壮着我的肺 面对 你想去 回活在这时 我在等待 无史了 是一个人开 你的灵魂 你已 为你 我手指不来 我的 音面 我想 你不 我不能\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_predict_rnn(rnn=rnn, is_random_iter=True, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.1,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
