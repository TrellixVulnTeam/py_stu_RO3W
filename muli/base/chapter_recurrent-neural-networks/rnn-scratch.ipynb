{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络 --- 从0开始\n",
    "\n",
    "前面的教程里我们使用的网络都属于**前馈神经网络**。之所以叫前馈，是因为整个网络是一条链（回想下`gluon.nn.Sequential`），每一层的结果都是反馈给下一层。这一节我们介绍**循环神经网络**，这里每一层不仅输出给下一层，同时还输出一个**隐含状态**，给当前层在处理下一个样本时使用。下图展示这两种网络的区别。\n",
    "\n",
    "![](../img/rnn_1.png)\n",
    "\n",
    "循环神经网络的这种结构使得它适合处理前后有依赖关系数据样本。我们拿语言模型举个例子来解释这个是怎么工作的。语言模型的任务是给定句子的前*t*个字符，然后预测第*t+1*个字符。假设我们的句子是“你好世界”，使用前馈神经网络来预测的一个做法是，在时间1输入“你”，预测”好“，时间2向同一个网络输入“好”预测“世”。下图左边展示了这个过程。\n",
    "\n",
    "![](../img/rnn_2.png)\n",
    "\n",
    "注意到一个问题是，当我们预测“世”的时候只给了“好”这个输入，而完全忽略了“你”。直觉上“你”这个词应该对这次的预测比较重要。虽然这个问题通常可以通过**n-gram**来缓解，就是说预测第*t+1*个字符的时候，我们输入前*n*个字符。如果*n=1*，那就是我们这里用的。我们可以增大*n*来使得输入含有更多信息。但我们不能任意增大*n*，因为这样通常带来模型复杂度的增加从而导致需要大量数据和计算来训练模型。\n",
    "\n",
    "循环神经网络使用一个隐含状态来记录前面看到的数据来帮助当前预测。上图右边展示了这个过程。在预测“好”的时候，我们输出一个隐含状态。我们用这个状态和新的输入“好”来一起预测“世”，然后同时输出一个更新过的隐含状态。我们希望前面的信息能够保存在这个隐含状态里，从而提升预测效果。\n",
    "\n",
    "## 循环神经网络\n",
    "\n",
    "在对输入输出数据有了解后，我们来正式介绍循环神经网络。\n",
    "\n",
    "首先回忆一下单隐含层的前馈神经网络的定义，例如[多层感知机](../chapter_supervised-learning/mlp-scratch.md)。假设隐含层的激活函数是$\\phi$，对于一个样本数为$n$特征向量维度为$x$的批量数据$\\mathbf{X} \\in \\mathbb{R}^{n \\times x}$（$\\mathbf{X}$是一个$n$行$x$列的实数矩阵）来说，那么这个隐含层的输出就是\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h)$$\n",
    "\n",
    "假定隐含层长度为$h$，其中的$\\mathbf{W}_{xh} \\in \\mathbb{R}^{x \\times h}$是权重参数。偏移参数 $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$在与前一项$\\mathbf{X} \\mathbf{W}_{xh} \\in \\mathbb{R}^{n \\times h}$ 相加时使用了[广播](../chapter_crashcourse/ndarray.md)。这个隐含层的输出的尺寸为$\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$。\n",
    "\n",
    "把隐含层的输出$\\mathbf{H}$作为输出层的输入，最终的输出\n",
    "\n",
    "$$\\hat{\\mathbf{Y}} = \\text{softmax}(\\mathbf{H} \\mathbf{W}_{hy} + \\mathbf{b}_y)$$\n",
    "\n",
    "假定每个样本对应的输出向量维度为$y$，其中 $\\hat{\\mathbf{Y}} \\in \\mathbb{R}^{n \\times y}, \\mathbf{W}_{hy} \\in \\mathbb{R}^{h \\times y}, \\mathbf{b}_y \\in \\mathbb{R}^{1 \\times y}$且两项相加使用了[广播](../chapter_crashcourse/ndarray.md)。\n",
    "\n",
    "\n",
    "将上面网络改成循环神经网络，我们首先对输入输出加上时间戳$t$。假设$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$是序列中的第$t$个批量输入（样本数为$n$，每个样本的特征向量维度为$x$），对应的隐含层输出是隐含状态$\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$（隐含层长度为$h$），而对应的最终输出是$\\hat{\\mathbf{Y}}_t \\in \\mathbb{R}^{n \\times y}$（每个样本对应的输出向量维度为$y$）。在计算隐含层的输出的时候，循环神经网络只需要在前馈神经网络基础上加上跟前一时间$t-1$输入隐含层$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$的加权和。为此，我们引入一个新的可学习的权重$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$：\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h)$$\n",
    "\n",
    "输出的计算跟前面一致：\n",
    "\n",
    "$$\\hat{\\mathbf{Y}}_t = \\text{softmax}(\\mathbf{H}_t \\mathbf{W}_{hy}  + \\mathbf{b}_y)$$\n",
    "\n",
    "一开始我们提到过，隐含状态可以认为是这个网络的记忆。该网络中，时刻$t$的隐含状态就是该时刻的隐含层变量$\\mathbf{H}_t$。它存储前面时间里面的信息。我们的输出是只基于这个状态。最开始的隐含状态里的元素通常会被初始化为0。\n",
    "\n",
    "\n",
    "## 周杰伦歌词数据集\n",
    "\n",
    "\n",
    "为了实现并展示循环神经网络，我们使用周杰伦歌词数据集来训练模型作词。该数据集里包含了著名创作型歌手周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》所有歌曲的歌词。\n",
    "\n",
    "![](../img/jay.jpg)\n",
    "\n",
    "\n",
    "下面我们读取这个数据并看看前面49个字符（char）是什么样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每天在想想想想著你\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile \n",
    "with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip','r') as zin:\n",
    "    zin.extractall('../data/') \n",
    "with open('../data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars=f.read()\n",
    "print(corpus_chars[0:49]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64925"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_chars) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们稍微处理下数据集。为了打印方便，我们把换行符替换成空格，然后截去后面一段使得接下来的训练会快一点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chars=corpus_chars.replace('\\n',' ').replace('\\r',' ') \n",
    "# corpus_chars=corpus_chars[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每天在想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_chars[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 字符的数值表示\n",
    "\n",
    "先把数据里面所有不同的字符拿出来做成一个字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2613\n"
     ]
    }
   ],
   "source": [
    "idx_to_char=list(set(corpus_chars)) \n",
    "char_to_idx=dict([(char,i) for i,char in enumerate(idx_to_char)]) \n",
    "vocab_size=len(char_to_idx) \n",
    "print('vocab size:',vocab_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后可以把每个字符转成从0开始的索引(index)来方便之后的使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: \n",
      " 想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每\n",
      "\n",
      " indices: \n",
      " [533, 626, 956, 734, 1593, 353, 529, 533, 626, 1219, 541, 1872, 759, 566, 2344, 2436, 529, 533, 626, 1219, 541, 2347, 2482, 578, 585, 1577, 529, 2347, 2482, 578, 566, 2344, 2116, 529, 1635, 914, 1932, 914, 1932, 914]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices=[char_to_idx[char] for char in corpus_chars] \n",
    "sample=corpus_indices[:40]\n",
    "print('chars: \\n',''.join([idx_to_char[idx] for idx in sample])) \n",
    "print('\\n indices: \\n',sample)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 时序数据的批量采样\n",
    "\n",
    "同之前一样我们需要每次随机读取一些（`batch_size`个）样本和其对用的标号。这里的样本跟前面有点不一样，这里一个样本通常包含一系列连续的字符（前馈神经网络里可能每个字符作为一个样本）。\n",
    "\n",
    "如果我们把序列长度（`num_steps`）设成5，那么一个可能的样本是“想要有直升”。其对应的标号仍然是长为5的序列，每个字符是对应的样本里字符的后面那个。例如前面样本的标号就是“要有直升机”。\n",
    "\n",
    "\n",
    "### 随机批量采样\n",
    "下面代码每次从数据里随机采样一个批量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from mxnet import nd\n",
    "def data_iter_random(corpus_indices,batch_size,num_steps,ctx=None):\n",
    "    # \n",
    "    num_examples=(len(corpus_indices)-1)//num_steps \n",
    "    epoch_size=num_examples//batch_size \n",
    "    # 随机化样本 \n",
    "    example_indices=list(range(num_examples)) \n",
    "    random.shuffle(example_indices) \n",
    "    # 返回num_steps 个数据 \n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos:pos+num_steps]\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size 个随机样本。\n",
    "        i=i*batch_size \n",
    "        batch_indices=example_indices[i:i+batch_size] \n",
    "        data=nd.array(\n",
    "            [_data(j*num_steps) for j in batch_indices],ctx=ctx)\n",
    "        label=nd.array(\n",
    "            [_data(j*num_steps+1) for j in batch_indices],ctx=ctx) \n",
    "        yield data,label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了便于理解时序数据上的随机批量采样，让我们输入一个从0到29的人工序列，看下读出来长什么样：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      "[[ 0.  1.  2.]\n",
      " [18. 19. 20.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 1.  2.  3.]\n",
      " [19. 20. 21.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[ 6.  7.  8.]\n",
      " [ 9. 10. 11.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[12. 13. 14.]\n",
      " [24. 25. 26.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[13. 14. 15.]\n",
      " [25. 26. 27.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[15. 16. 17.]\n",
      " [21. 22. 23.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[16. 17. 18.]\n",
      " [22. 23. 24.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq=list(range(30)) \n",
    "for data,label in data_iter_random(my_seq,batch_size=2,num_steps=3):\n",
    "    print('data:',data,'\\nlabel:',label,'\\n') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "由于各个采样在原始序列上的位置是随机的时序长度为`num_steps`的连续数据点，相邻的两个随机批量在原始序列上的位置不一定相毗邻。因此，在训练模型时，读取每个随机时序批量前需要重新初始化隐含状态。\n",
    "\n",
    "\n",
    "### 相邻批量采样\n",
    "\n",
    "除了对原序列做随机批量采样之外，我们还可以使相邻的两个随机批量在原始序列上的位置相毗邻。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    indices = corpus_indices[0: batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    # 减一是因为label的索引是相应data的索引加一\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        data = indices[:, i: i + num_steps]\n",
    "        label = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield data, label\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相同地，为了便于理解时序数据上的相邻批量采样，让我们输入一个从0到29的人工序列，看下读出来长什么样：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      "[[ 0.  1.  2.]\n",
      " [15. 16. 17.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 1.  2.  3.]\n",
      " [16. 17. 18.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[ 3.  4.  5.]\n",
      " [18. 19. 20.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 4.  5.  6.]\n",
      " [19. 20. 21.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[ 6.  7.  8.]\n",
      " [21. 22. 23.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 7.  8.  9.]\n",
      " [22. 23. 24.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data: \n",
      "[[ 9. 10. 11.]\n",
      " [24. 25. 26.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[10. 11. 12.]\n",
      " [25. 26. 27.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq=list(range(30)) \n",
    "for data,label in data_iter_consecutive(my_seq,batch_size=2,num_steps=3):\n",
    "    print('data:',data,'\\nlabel:',label,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "由于各个采样在原始序列上的位置是毗邻的时序长度为`num_steps`的连续数据点，因此，使用相邻批量采样训练模型时，读取每个时序批量前，我们需要将该批量最开始的隐含状态设为上个批量最终输出的隐含状态。在同一个epoch中，隐含状态只需要在该epoch开始的时候初始化。\n",
    "\n",
    "\n",
    "## One-hot向量\n",
    "\n",
    "注意到每个字符现在是用一个整数来表示，而输入进网络我们需要一个定长的向量。一个常用的办法是使用one-hot来将其表示成向量。也就是说，如果一个字符的整数值是$i$, 那么我们创建一个全0的长为`vocab_size`的向量，并将其第$i$位设成1。该向量就是对原字符的one-hot向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 1. ... 0. 0. 0.]]\n",
       "<NDArray 2x2613 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.one_hot(nd.array([0,2]),vocab_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记得前面我们每次得到的数据是一个`batch_size * num_steps`的批量。下面这个函数将其转换成`num_steps`个可以输入进网络的`batch_size * vocab_size`的矩阵。对于一个长度为`num_steps`的序列，每个批量输入$\\mathbf{X} \\in \\mathbb{R}^{n \\times x}$，其中$n=$ `batch_size`，而$x=$`vocab_size`（onehot编码向量维度）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 3\n",
      "input[0] shape: (2, 2613)\n"
     ]
    }
   ],
   "source": [
    "def get_inputs(data):\n",
    "    return [nd.one_hot(X,vocab_size) for X in data.T] \n",
    "\n",
    "inputs=get_inputs(data) \n",
    "print('Input length:',len(inputs)) \n",
    "print('input[0] shape:',inputs[0].shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 初始化模型参数\n",
    "\n",
    "对于序列中任意一个时间戳，一个字符的输入是维度为`vocab_size`的one-hot向量，对应输出是预测下一个时间戳为词典中任意字符的概率，因而该输出是维度为`vocab_size`的向量。\n",
    "\n",
    "当序列中某一个时间戳的输入为一个样本数为`batch_size`（对应模型定义中的$n$）的批量，每个时间戳上的输入和输出皆为尺寸`batch_size * vocab_size`（对应模型定义中的$n \\times x$）的矩阵。假设每个样本对应的隐含状态的长度为`hidden_dim`（对应模型定义中隐含层长度$h$），根据矩阵乘法定义，我们可以推断出模型隐含层和输出层中各个参数的尺寸。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaa\n",
      "will use gpu(0)\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..') \n",
    "import utils \n",
    "ctx=utils.try_gpu() \n",
    "print('will use',ctx) \n",
    "\n",
    "input_dim=vocab_size \n",
    "# 隐含状态长度 \n",
    "hidden_dim=256\n",
    "output_dim=vocab_size \n",
    "std=0.01\n",
    "\n",
    "def get_params():\n",
    "    # 隐含层\n",
    "    W_xh=nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx) \n",
    "    W_hh=nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx) \n",
    "    b_h=nd.zeros(hidden_dim,ctx=ctx) \n",
    "    \n",
    "    # 输出层 \n",
    "    W_hy=nd.random_normal(scale=std,shape=(hidden_dim,output_dim),ctx=ctx) \n",
    "    b_y=nd.zeros(output_dim,ctx=ctx) \n",
    "    params=[W_xh,W_hh,b_h,W_hy,b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad() \n",
    "    return params \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 定义模型\n",
    "\n",
    "当序列中某一个时间戳的输入为一个样本数为`batch_size`的批量，而整个序列长度为`num_steps`时，以下`rnn`函数的`inputs`和`outputs`皆为`num_steps` 个尺寸为`batch_size * vocab_size`的矩阵，隐含变量$\\mathbf{H}$是一个尺寸为`batch_size * hidden_dim`的矩阵。该隐含变量$\\mathbf{H}$也是循环神经网络的隐含状态`state`。\n",
    "\n",
    "我们将前面的模型公式翻译成代码。这里的激活函数使用了按元素操作的双曲正切函数\n",
    "\n",
    "$$\\text{tanh}(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}$$\n",
    "\n",
    "需要注意的是，双曲正切函数的值域是$[-1, 1]$。如果自变量均匀分布在整个实域，该激活函数输出的均值为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs,state,*params):\n",
    "    # inputs: num_steps 个尺寸为batch_size*vocab_size 矩阵。\n",
    "    # H:尺寸为 batch_size * hidden_dim 矩阵 \n",
    "    # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵。\n",
    "    H=state \n",
    "    W_xh,W_hh,b_h,W_hy,b_y=params \n",
    "    outputs=[]\n",
    "    for X in inputs:\n",
    "        H=nd.tanh(nd.dot(X,W_xh)+nd.dot(H,W_hh)+b_h) \n",
    "        Y=nd.dot(H,W_hy)+b_y \n",
    "        outputs.append(Y) \n",
    "    return (outputs,H) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做个简单的测试：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length: 3\n",
      "output[0] shape: (2, 2613)\n",
      "state shape: (2, 256)\n"
     ]
    }
   ],
   "source": [
    "state=nd.zeros(shape=(data.shape[0],hidden_dim),ctx=ctx) \n",
    "params=get_params() \n",
    "outputs,state_new=rnn(get_inputs(data.as_in_context(ctx)),state,*params) \n",
    "print('output length:',len(outputs))\n",
    "print('output[0] shape:',outputs[0].shape) \n",
    "print('state shape:',state_new.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 预测序列\n",
    "\n",
    "在做预测时我们只需要给定时间0的输入和起始隐含变量。然后我们每次将上一个时间的输出作为下一个时间的输入。\n",
    "\n",
    "![](../img/rnn_3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(rnn,prefix,num_chars,params,hidden_dim,ctx,idx_to_char,char_to_idx,get_inputs,is_lstm=False):\n",
    "    # 预测 以 prefix 开始的接下来的 num_chars 个字符 \n",
    "    prefix=prefix.lower()\n",
    "    state_h=nd.zeros(shape=(1,hidden_dim),ctx=ctx) \n",
    "    if is_lstm:\n",
    "        # 当 RNN 使用LSTM时才会用到，这里可以忽略。\n",
    "        state_C=nd.zeros(shape=(1,hidden_dim),ctx=ctx) \n",
    "    output=[char_to_idx[prefix[0]]]\n",
    "    for i in range(num_chars+len(prefix)):\n",
    "        X=nd.array([output[-1]],ctx=ctx) \n",
    "        # 在序列中循环迭代隐含变量。\n",
    "        if is_lstm:\n",
    "            # 当 RNN使用LSTM时才会用到，这里可以忽略。\n",
    "            Y,state_h,state_c=rnn(get_inputs(X),state_h,state_c,*params)\n",
    "        else:\n",
    "            Y,state_h=rnn(get_inputs(X),state_h,*params) \n",
    "        if i<len(prefix)-1:\n",
    "            next_input=char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            next_input=int(Y[0].argmax(axis=1).asscalar()) \n",
    "        output.append(next_input) \n",
    "    return ''.join([idx_to_char[i] for i in output]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 梯度剪裁\n",
    "\n",
    "我们在[正向传播和反向传播](../chapter_supervised-learning/backprop.md)中提到，\n",
    "训练神经网络往往需要依赖梯度计算的优化算法，例如我们之前介绍的[随机梯度下降](../chapter_supervised-learning/linear-regression-scratch.md)。\n",
    "而在循环神经网络的训练中，当每个时序训练数据样本的时序长度`num_steps`较大或者时刻$t$较小，目标函数有关$t$时刻的隐含层变量梯度较容易出现衰减（valishing）或爆炸（explosion）。我们会在[下一节](bptt.md)详细介绍出现该现象的原因。\n",
    "\n",
    "为了应对梯度爆炸，一个常用的做法是如果梯度特别大，那么就投影到一个比较小的尺度上。假设我们把所有梯度接成一个向量 $\\boldsymbol{g}$，假设剪裁的阈值是$\\theta$，那么我们这样剪裁使得$\\|\\boldsymbol{g}\\|$不会超过$\\theta$：\n",
    "\n",
    "$$ \\boldsymbol{g} = \\min\\left(\\frac{\\theta}{\\|\\boldsymbol{g}\\|}, 1\\right)\\boldsymbol{g}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params,theta,ctx):\n",
    "    if theta is not None:\n",
    "        norm=nd.array([0.0],ctx) \n",
    "        for p in params:\n",
    "            norm+=nd.sum(p.grad**2)\n",
    "        norm=nd.sqrt(norm).asscalar() \n",
    "        if norm>theta:\n",
    "            for p in params:\n",
    "                p.grad[:]*=theta/norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 训练模型\n",
    "\n",
    "下面我们可以还是训练模型。跟前面前置网络的教程比，这里有以下几个不同。\n",
    "\n",
    "1. 通常我们使用困惑度（Perplexity）这个指标。\n",
    "2. 在更新前我们对梯度做剪裁。\n",
    "3. 在训练模型时，对时序数据采用不同批量采样方法将导致隐含变量初始化的不同。\n",
    "\n",
    "### 困惑度（Perplexity）\n",
    "\n",
    "回忆以下我们之前介绍的[交叉熵损失函数](../chapter_supervised-learning/softmax-regression-scratch.md)。在语言模型中，该损失函数即被预测字符的对数似然平均值的相反数：\n",
    "\n",
    "$$\\text{loss} = -\\frac{1}{N} \\sum_{i=1}^N \\log p_{\\text{target}_i}$$\n",
    "\n",
    "其中$N$是预测的字符总数，$p_{\\text{target}_i}$是在第$i$个预测中真实的下个字符被预测的概率。\n",
    "\n",
    "而这里的困惑度可以简单的认为就是对交叉熵做exp运算使得数值更好读。\n",
    "\n",
    "为了解释困惑度的意义，我们先考虑一个完美结果：模型总是把真实的下个字符的概率预测为1。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 1$。这种完美情况下，困惑度值为1。\n",
    "\n",
    "我们再考虑一个基线结果：给定不重复的字符集合$W$及其字符总数$|W|$，模型总是预测下个字符为集合$W$中任一字符的概率都相同。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 1/|W|$。这种基线情况下，困惑度值为$|W|$。\n",
    "\n",
    "最后，我们可以考虑一个最坏结果：模型总是把真实的下个字符的概率预测为0。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 0$。这种最坏情况下，困惑度值为正无穷。\n",
    "\n",
    "任何一个有效模型的困惑度值必须小于预测集中元素的数量。在本例中，困惑度必须小于字典中的字符数$|W|$。如果一个模型可以取得较低的困惑度的值（更靠近1），通常情况下，该模型预测更加准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd \n",
    "from mxnet import gluon \n",
    "from math import exp \n",
    "\n",
    "def train_and_predict_rnn(rnn,is_random_iter,epochs,num_steps,hidden_dim,\n",
    "                         learning_rate,clipping_theta,batch_size,\n",
    "                        pred_period,pred_len,seqs,get_params,get_inputs,\n",
    "                         ctx,corpus_indices,idx_to_char,char_to_idx,\n",
    "                         is_lstm=False):\n",
    "    if is_random_iter:\n",
    "        data_iter=data_iter_random\n",
    "    else:\n",
    "        data_iter=data_iter_consecutive \n",
    "    params=get_params() \n",
    "    softmax_cross_entropy=gluon.loss.SoftmaxCrossEntropyLoss() \n",
    "    for e in range(1,epochs+1):\n",
    "        # 如使用相邻批量采样，在同一个epoch中，隐含变量只需要在该epoch 开始的时候初始化 \n",
    "        if not is_random_iter:\n",
    "            state_h=nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx) \n",
    "            if is_lstm:\n",
    "                state_c=nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx) \n",
    "        train_loss,num_examples=0,0 \n",
    "        for data,label in data_iter(corpus_indices,batch_size,num_steps,ctx):\n",
    "            # 如果使用随机批量采样，处理每个随机小批量前都需要初始化隐含变量。\n",
    "            if is_random_iter:\n",
    "                state_h=nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx) \n",
    "                if is_lstm:\n",
    "                    # 当RNN使用 LSTM 时才会用到，这里可以忽略 。\n",
    "                    state_c=nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx) \n",
    "            with autograd.record():\n",
    "                # outputs 尺寸：（batch_size,vocab_size) \n",
    "                if is_lstm:\n",
    "                    # 当RNN 使用LSTM时才会用到，这里可以忽略 \n",
    "                    outputs,state_h,state_c=rnn(get_inputs(data),state_h,\n",
    "                                               state_c,*params)\n",
    "                else:\n",
    "                    outputs,state_h=rnn(get_inputs(data),state_h,*params) \n",
    "                # 设 t_ib_j 为 i时间批量中的j元素 \n",
    "                # label尺寸：（batch_size * num_steps \n",
    "                label=label.T.reshape((-1,))\n",
    "                # 拼接outpus，尺寸：(batch_size * num_steps,vocab_size) .\n",
    "                outputs=nd.concat(*outputs,dim=0) \n",
    "                #经过上述操作，outputs 和label已经对齐 \n",
    "                loss=softmax_cross_entropy(outputs,label) \n",
    "            loss.backward() \n",
    "            grad_clipping(params,clipping_theta,ctx) \n",
    "            utils.SGD(params,learning_rate) \n",
    "            train_loss+=nd.sum(loss).asscalar() \n",
    "            num_examples+=loss.size \n",
    "        if e%pred_period==0:\n",
    "            print(\"Epoch %d .Perplexity %f \"%(e,exp(train_loss/num_examples)))\n",
    "            for seq in seqs:\n",
    "                print('-',predict_rnn(rnn,seq,pred_len,params,\n",
    "                                     hidden_dim,ctx,idx_to_char,char_to_idx,get_inputs,is_lstm)) \n",
    "                print() \n",
    "                \n",
    "            \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下定义模型参数和预测序列前缀。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=200\n",
    "num_steps=35\n",
    "learning_rate=0.1 \n",
    "batch_size=32 \n",
    "softmax_cross_entropy=gluon.loss.SoftmaxCrossEntropyLoss() \n",
    "\n",
    "seq1='分开'\n",
    "seq2='不分开'\n",
    "seq3='战争中不对'\n",
    "seqs=[seq1,seq2,seq3] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先采用随机批量采样实验循环神经网络谱写歌词。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 .Perplexity 170.175456 \n",
      "- 分开 我说你的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的\n",
      "\n",
      "- 不分开 我们的爱 你的爱 你说我的爱 我在我们的  我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 我在我的爱 \n",
      "\n",
      "- 战争中不对  我们的爱 你的爱 你说我的爱 我在我们的泪 我知道 我们的 爱你的人 我不是 你的爱 只是我的爱 我在我们的泪 我知道 我们的 爱你的人 我不是 你的爱 只是我的爱 我在我们的泪 我知道 我们的 爱\n",
      "\n",
      "Epoch 40 .Perplexity 55.309534 \n",
      "- 分开 一些人 后着你的手 我不能 我们一口 说你的笑  你不懂 我一口 北人一口 我的灵界 你不开 你的爱 只是我的感觉 我不想 我的爱 我怎么我 我不需感我 你不该不到 我们 如兽一口 强后一直 等你一起\n",
      "\n",
      "- 不分开 她不得你 你是你的手 我不能 你身一口 我的爱界 你不开 你的一口 你不会 你不再 我怎么 我手 你不开 我不再 这样一口 我的灵魂 你不懂 我不再 你说一口 我的爱界 你不开 你不再 你不再 我怎么\n",
      "\n",
      "- 战争中不对  我在爱你的模 我有一起 我的爱 只是我的感觉 我们的感 再不用 你说我的爱 这么地 你不想 我一口 北人一口 心的灵 你不是我 你的世界 我们的爱 再着了 的时魂 有着我的爱 我说不开 我的爱 只是\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_and_predict_rnn(rnn=rnn,is_random_iter=True,epochs=200,num_steps=35,\n",
    "#                      hidden_dim=hidden_dim,learning_rate=0.2,\n",
    "#                      clipping_theta=5,batch_size=32,pred_period=20,\n",
    "#                      pred_len=100,seqs=seqs,get_params=get_params,\n",
    "#                      get_inputs=get_inputs,ctx=ctx,\n",
    "#                      corups_indices=corpus_indices,\n",
    "#                       idx_to_char=idx_to_char,\n",
    "#                      char_to_idx=char_to_idx) \n",
    "\n",
    "\n",
    "\n",
    "train_and_predict_rnn(rnn=rnn, is_random_iter=False, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.2,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再采用相邻批量采样实验循环神经网络谱写歌词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_predict_rnn(rnn=rnn, is_random_iter=False, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.2,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。\n",
    "\n",
    "## 结论\n",
    "\n",
    "* 通过隐含状态，循环神经网络适合处理前后有依赖关系时序数据样本。\n",
    "* 对前后有依赖关系时序数据样本批量采样时，我们可以使用随机批量采样和相邻批量采样。\n",
    "* 循环神经网络较容易出现梯度衰减和爆炸。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 调调参数（例如数据集大小、序列长度、隐含状态长度和学习率），看看对运行时间、perplexity和预测的结果造成的影响。\n",
    "* 在随机批量采样中，如果在同一个epoch中只把隐含变量在该epoch开始的时候初始化会怎么样？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/989)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
