{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络 --- 使用Gluon\n",
    "\n",
    "本节介绍如何使用`Gluon`训练循环神经网络。\n",
    "\n",
    "\n",
    "## Penn Tree Bank (PTB) 数据集\n",
    "\n",
    "我们以单词为基本元素来训练语言模型。[Penn Tree Bank](https://catalog.ldc.upenn.edu/ldc99t42)（PTB）是一个标准的文本序列数据集。它包括训练集、验证集和测试集。\n",
    "\n",
    "下面我们载入数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import os \n",
    "import time \n",
    "import numpy as np \n",
    "import mxnet as mx \n",
    "from mxnet import gluon,autograd \n",
    "from mxnet.gluon import nn,rnn \n",
    "import zipfile\n",
    "with zipfile.ZipFile('../data/ptb.zip','r') as zin:\n",
    "    zin.extractall('../data/') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立词语索引\n",
    "\n",
    "下面定义了`Dictionary`类来映射词语和索引。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_idx:\n",
    "            self.idx_to_word.append(word)\n",
    "            self.word_to_idx[word] = len(self.idx_to_word) - 1\n",
    "        return self.word_to_idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下的`Corpus`类按照读取的文本数据集建立映射词语和索引的词典，并将文本转换成词语索引的序列。这样，每个文本数据集就变成了`NDArray`格式的整数序列。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path + 'train.txt')\n",
    "        self.valid = self.tokenize(path + 'valid.txt')\n",
    "        self.test = self.tokenize(path + 'test.txt')\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        assert os.path.exists(path)\n",
    "        # 将词语添加至词典。\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        # 将文本转换成词语索引的序列（NDArray格式）。\n",
    "        with open(path, 'r') as f:\n",
    "            indices = np.zeros((tokens,), dtype='int32')\n",
    "            idx = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    indices[idx] = self.dictionary.word_to_idx[word]\n",
    "                    idx += 1\n",
    "        return mx.nd.array(indices, dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data='../data/ptb/ptb.'\n",
    "corpus=Corpus(data) \n",
    "vocab_size=len(corpus.dictionary)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以定义一个循环神经网络模型库。这样就可以支持各种不同的循环神经网络模型了 。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(gluon.Block):\n",
    "    \"\"\"循环神经网络模型库\"\"\"\n",
    "    def __init__(self, mode, vocab_size, embed_dim, hidden_dim,\n",
    "                 num_layers, dropout=0.5, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, embed_dim,\n",
    "                                        weight_initializer=mx.init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(hidden_dim, num_layers, activation='relu',\n",
    "                                   dropout=dropout, input_size=embed_dim)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(hidden_dim, num_layers, dropout=dropout,\n",
    "                                   input_size=embed_dim)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(hidden_dim, num_layers, dropout=dropout,\n",
    "                                    input_size=embed_dim)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(hidden_dim, num_layers, dropout=dropout,\n",
    "                                   input_size=embed_dim)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)\n",
    "\n",
    "            self.decoder = nn.Dense(vocab_size, in_units=hidden_dim)\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, state = self.rnn(emb, state)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.hidden_dim)))\n",
    "        return decoded, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 定义参数\n",
    "\n",
    "我们接着定义模型参数。我们选择使用ReLU为激活函数的循环神经网络为例。这里我们把`epochs`设为1是为了演示方便。\n",
    "\n",
    "\n",
    "## 多层循环神经网络\n",
    "\n",
    "我们通过`num_layers`设置循环神经网络隐含层的层数，例如2。\n",
    "\n",
    "对于一个多层循环神经网络，当前时刻隐含层的输入来自同一时刻输入层（如果有）或上一隐含层的输出。每一层的隐含状态只沿着同一层传递。\n",
    "\n",
    "把[单层循环神经网络](rnn-scratch.md)中隐含层的每个单元当做一个函数$f$，这个函数在$t$时刻的输入是$\\mathbf{X}_t, \\mathbf{H}_{t-1}$，输出是$\\mathbf{H}_t$：\n",
    "\n",
    "$$f(\\mathbf{X}_t, \\mathbf{H}_{t-1}) = \\mathbf{H}_t$$\n",
    "\n",
    "假设输入为第0层，输出为第$L+1$层，在一共$L$个隐含层的循环神经网络中，上式中可以拓展成以下的函数:\n",
    "\n",
    "$$f(\\mathbf{H}_t^{(l-1)}, \\mathbf{H}_{t-1}^{(l)}) = \\mathbf{H}_t^{(l)}$$\n",
    "\n",
    "如下图所示。\n",
    "\n",
    "![](../img/multi-layer-rnn.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'rnn_relu'\n",
    "\n",
    "embed_dim = 100\n",
    "hidden_dim = 100\n",
    "num_layers = 2\n",
    "lr = 1.0\n",
    "clipping_norm = 0.2\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "num_steps = 5\n",
    "dropout_rate = 0.2\n",
    "eval_period = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 批量采样\n",
    "\n",
    "我们将数据进一步处理为便于相邻批量采样的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaa\n"
     ]
    }
   ],
   "source": [
    "# 尝试使用GPU\n",
    "\n",
    "import sys \n",
    "sys.path.append('..') \n",
    "import utils\n",
    "context=utils.try_gpu()\n",
    "\n",
    "def batchify(data,batch_size):\n",
    "    ''' 数据形状（num_batches,batch_size)'''\n",
    "    num_batches=data.shape[0] //batch_size \n",
    "    data=data[:num_batches*batch_size] \n",
    "    data=data.reshape((batch_size,num_batches)).T \n",
    "    return data \n",
    "\n",
    "train_data=batchify(corpus.train,batch_size).as_in_context(context) \n",
    "val_data=batchify(corpus.valid,batch_size).as_in_context(context) \n",
    "test_data=batchify(corpus.test,batch_size).as_in_context(context) \n",
    "\n",
    "\n",
    "model = RNNModel(model_name, vocab_size, embed_dim, hidden_dim,\n",
    "                       num_layers, dropout_rate)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': lr, 'momentum': 0, 'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def get_batch(source,i):\n",
    "    seq_len = min(num_steps, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target=source[i + 1:i+1+seq_len]\n",
    "    return data,target.reshape((-1,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从计算图分离隐含状态\n",
    "\n",
    "在模型训练的每次迭代中，当前批量序列的初始隐含状态来自上一个相邻批量序列的输出隐含状态。为了使模型参数的梯度计算只依赖当前的批量序列，从而减小每次迭代的计算开销，我们可以使用`detach`函数来将隐含状态从计算图分离出来。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(state):\n",
    "    if isinstance(state,(tuple,list)):\n",
    "        state=[i.detach() for i in state]\n",
    "    else:\n",
    "        state=state.detach() \n",
    "    return state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 训练和评价模型\n",
    "\n",
    "和之前一样，我们定义模型评价函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data_source):\n",
    "    total_L=0.0\n",
    "    ntotal=0 \n",
    "    hidden=model.begin_state(func=mx.nd.zeros,batch_size=batch_size,\n",
    "                            ctx=context) \n",
    "    for i in range(0,data_source.shape[0]-1,num_steps):\n",
    "        data,target=get_batch(data_source,i) \n",
    "        output,hidden=model(data,hidden) \n",
    "        L=loss(output,target) \n",
    "        total_L+=mx.nd.sum(L).asscalar() \n",
    "        ntotal+=L.size\n",
    "    return total_L/ntotal \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以训练模型并在每个epoch评价模型在验证集上的结果。我们可以参考验证集上的结果调参。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = batch_size,\n",
    "                                   ctx = context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, num_steps)):\n",
    "            data, target = get_batch(train_data, i)\n",
    "            # 从计算图分离隐含状态。\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # 梯度裁剪。需要注意的是，这里的梯度是整个批量的梯度。\n",
    "            # 因此我们将clipping_norm乘以num_steps和batch_size。\n",
    "            gluon.utils.clip_global_norm(grads,\n",
    "                                         clipping_norm * num_steps * batch_size)\n",
    "\n",
    "            trainer.step(batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % eval_period == 0 and ibatch > 0:\n",
    "                cur_L = total_L / num_steps / batch_size / eval_period\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        val_L = model_eval(val_data)\n",
    "\n",
    "        print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation ' \n",
    "              'perplexity %.2f' % (epoch + 1, time.time() - start_time, val_L,\n",
    "                                   math.exp(val_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 500] loss 7.27, perplexity 1437.37\n",
      "[Epoch 1 Batch 1000] loss 6.42, perplexity 615.79\n",
      "[Epoch 1 Batch 1500] loss 6.21, perplexity 495.51\n",
      "[Epoch 1 Batch 2000] loss 6.13, perplexity 458.90\n",
      "[Epoch 1 Batch 2500] loss 6.02, perplexity 409.90\n",
      "[Epoch 1 Batch 3000] loss 5.91, perplexity 367.97\n",
      "[Epoch 1 Batch 3500] loss 5.93, perplexity 375.33\n",
      "[Epoch 1 Batch 4000] loss 5.81, perplexity 331.96\n",
      "[Epoch 1 Batch 4500] loss 5.78, perplexity 324.14\n",
      "[Epoch 1 Batch 5000] loss 5.77, perplexity 321.26\n",
      "[Epoch 1 Batch 5500] loss 5.78, perplexity 324.77\n",
      "[Epoch 1] time cost 36.01s, validation loss 5.67, validation perplexity 290.41\n",
      "[Epoch 2 Batch 500] loss 5.76, perplexity 317.50\n",
      "[Epoch 2 Batch 1000] loss 5.69, perplexity 296.42\n",
      "[Epoch 2 Batch 1500] loss 5.66, perplexity 286.31\n",
      "[Epoch 2 Batch 2000] loss 5.68, perplexity 293.89\n",
      "[Epoch 2 Batch 2500] loss 5.64, perplexity 280.63\n",
      "[Epoch 2 Batch 3000] loss 5.55, perplexity 257.87\n",
      "[Epoch 2 Batch 3500] loss 5.60, perplexity 270.75\n",
      "[Epoch 2 Batch 4000] loss 5.51, perplexity 247.03\n",
      "[Epoch 2 Batch 4500] loss 5.50, perplexity 244.43\n",
      "[Epoch 2 Batch 5000] loss 5.52, perplexity 249.58\n",
      "[Epoch 2 Batch 5500] loss 5.55, perplexity 257.15\n",
      "[Epoch 2] time cost 31.69s, validation loss 5.48, validation perplexity 239.45\n",
      "[Epoch 3 Batch 500] loss 5.56, perplexity 259.30\n",
      "[Epoch 3 Batch 1000] loss 5.50, perplexity 244.14\n",
      "[Epoch 3 Batch 1500] loss 5.48, perplexity 239.75\n",
      "[Epoch 3 Batch 2000] loss 5.52, perplexity 250.75\n",
      "[Epoch 3 Batch 2500] loss 5.49, perplexity 241.97\n",
      "[Epoch 3 Batch 3000] loss 5.41, perplexity 223.93\n",
      "[Epoch 3 Batch 3500] loss 5.47, perplexity 236.35\n",
      "[Epoch 3 Batch 4000] loss 5.38, perplexity 216.04\n",
      "[Epoch 3 Batch 4500] loss 5.37, perplexity 214.70\n",
      "[Epoch 3 Batch 5000] loss 5.40, perplexity 222.48\n",
      "[Epoch 3 Batch 5500] loss 5.44, perplexity 229.83\n",
      "[Epoch 3] time cost 32.38s, validation loss 5.40, validation perplexity 221.11\n",
      "[Epoch 4 Batch 500] loss 5.45, perplexity 233.29\n",
      "[Epoch 4 Batch 1000] loss 5.40, perplexity 220.52\n",
      "[Epoch 4 Batch 1500] loss 5.37, perplexity 215.54\n",
      "[Epoch 4 Batch 2000] loss 5.43, perplexity 228.55\n",
      "[Epoch 4 Batch 2500] loss 5.40, perplexity 222.03\n",
      "[Epoch 4 Batch 3000] loss 5.33, perplexity 205.78\n",
      "[Epoch 4 Batch 3500] loss 5.38, perplexity 217.14\n",
      "[Epoch 4 Batch 4000] loss 5.30, perplexity 201.08\n",
      "[Epoch 4 Batch 4500] loss 5.28, perplexity 197.04\n",
      "[Epoch 4 Batch 5000] loss 5.33, perplexity 205.73\n",
      "[Epoch 4 Batch 5500] loss 5.37, perplexity 214.38\n",
      "[Epoch 4] time cost 34.48s, validation loss 5.34, validation perplexity 207.86\n",
      "[Epoch 5 Batch 500] loss 5.38, perplexity 216.87\n",
      "[Epoch 5 Batch 1000] loss 5.33, perplexity 206.47\n",
      "[Epoch 5 Batch 1500] loss 5.31, perplexity 202.41\n",
      "[Epoch 5 Batch 2000] loss 5.37, perplexity 214.78\n",
      "[Epoch 5 Batch 2500] loss 5.35, perplexity 209.76\n",
      "[Epoch 5 Batch 3000] loss 5.27, perplexity 194.97\n",
      "[Epoch 5 Batch 3500] loss 5.32, perplexity 204.68\n",
      "[Epoch 5 Batch 4000] loss 5.25, perplexity 190.14\n",
      "[Epoch 5 Batch 4500] loss 5.23, perplexity 186.65\n",
      "[Epoch 5 Batch 5000] loss 5.28, perplexity 195.69\n",
      "[Epoch 5 Batch 5500] loss 5.32, perplexity 203.58\n",
      "[Epoch 5] time cost 37.41s, validation loss 5.29, validation perplexity 199.09\n",
      "[Epoch 6 Batch 500] loss 5.33, perplexity 206.83\n",
      "[Epoch 6 Batch 1000] loss 5.28, perplexity 196.68\n",
      "[Epoch 6 Batch 1500] loss 5.27, perplexity 194.02\n",
      "[Epoch 6 Batch 2000] loss 5.32, perplexity 204.95\n",
      "[Epoch 6 Batch 2500] loss 5.30, perplexity 200.47\n",
      "[Epoch 6 Batch 3000] loss 5.23, perplexity 186.93\n",
      "[Epoch 6 Batch 3500] loss 5.28, perplexity 196.55\n",
      "[Epoch 6 Batch 4000] loss 5.21, perplexity 182.24\n",
      "[Epoch 6 Batch 4500] loss 5.19, perplexity 179.08\n",
      "[Epoch 6 Batch 5000] loss 5.24, perplexity 187.89\n",
      "[Epoch 6 Batch 5500] loss 5.28, perplexity 196.80\n",
      "[Epoch 6] time cost 35.68s, validation loss 5.27, validation perplexity 194.51\n",
      "[Epoch 7 Batch 500] loss 5.30, perplexity 200.28\n",
      "[Epoch 7 Batch 1000] loss 5.24, perplexity 189.54\n",
      "[Epoch 7 Batch 1500] loss 5.23, perplexity 186.95\n",
      "[Epoch 7 Batch 2000] loss 5.29, perplexity 197.66\n",
      "[Epoch 7 Batch 2500] loss 5.27, perplexity 194.64\n",
      "[Epoch 7 Batch 3000] loss 5.20, perplexity 180.61\n",
      "[Epoch 7 Batch 3500] loss 5.25, perplexity 190.91\n",
      "[Epoch 7 Batch 4000] loss 5.17, perplexity 176.72\n",
      "[Epoch 7 Batch 4500] loss 5.16, perplexity 173.79\n",
      "[Epoch 7 Batch 5000] loss 5.21, perplexity 182.88\n",
      "[Epoch 7 Batch 5500] loss 5.25, perplexity 189.93\n",
      "[Epoch 7] time cost 42.07s, validation loss 5.24, validation perplexity 188.97\n",
      "[Epoch 8 Batch 500] loss 5.26, perplexity 193.00\n",
      "[Epoch 8 Batch 1000] loss 5.22, perplexity 184.37\n",
      "[Epoch 8 Batch 1500] loss 5.20, perplexity 181.67\n",
      "[Epoch 8 Batch 2000] loss 5.26, perplexity 192.86\n",
      "[Epoch 8 Batch 2500] loss 5.24, perplexity 188.40\n",
      "[Epoch 8 Batch 3000] loss 5.16, perplexity 174.65\n",
      "[Epoch 8 Batch 3500] loss 5.22, perplexity 185.53\n",
      "[Epoch 8 Batch 4000] loss 5.15, perplexity 171.78\n",
      "[Epoch 8 Batch 4500] loss 5.12, perplexity 167.98\n",
      "[Epoch 8 Batch 5000] loss 5.18, perplexity 176.83\n",
      "[Epoch 8 Batch 5500] loss 5.23, perplexity 186.49\n",
      "[Epoch 8] time cost 38.89s, validation loss 5.23, validation perplexity 186.63\n",
      "[Epoch 9 Batch 500] loss 5.24, perplexity 188.33\n",
      "[Epoch 9 Batch 1000] loss 5.19, perplexity 179.17\n",
      "[Epoch 9 Batch 1500] loss 5.18, perplexity 177.63\n",
      "[Epoch 9 Batch 2000] loss 5.23, perplexity 187.51\n",
      "[Epoch 9 Batch 2500] loss 5.21, perplexity 183.66\n",
      "[Epoch 9 Batch 3000] loss 5.15, perplexity 172.39\n",
      "[Epoch 9 Batch 3500] loss 5.20, perplexity 181.26\n",
      "[Epoch 9 Batch 4000] loss 5.13, perplexity 168.93\n",
      "[Epoch 9 Batch 4500] loss 5.10, perplexity 164.22\n",
      "[Epoch 9 Batch 5000] loss 5.16, perplexity 174.19\n",
      "[Epoch 9 Batch 5500] loss 5.20, perplexity 181.73\n",
      "[Epoch 9] time cost 36.24s, validation loss 5.21, validation perplexity 183.87\n",
      "[Epoch 10 Batch 500] loss 5.22, perplexity 184.42\n",
      "[Epoch 10 Batch 1000] loss 5.17, perplexity 175.66\n",
      "[Epoch 10 Batch 1500] loss 5.16, perplexity 173.31\n",
      "[Epoch 10 Batch 2000] loss 5.22, perplexity 184.36\n",
      "[Epoch 10 Batch 2500] loss 5.20, perplexity 181.34\n",
      "[Epoch 10 Batch 3000] loss 5.12, perplexity 168.10\n",
      "[Epoch 10 Batch 3500] loss 5.18, perplexity 177.83\n",
      "[Epoch 10 Batch 4000] loss 5.11, perplexity 165.51\n",
      "[Epoch 10 Batch 4500] loss 5.08, perplexity 161.57\n",
      "[Epoch 10 Batch 5000] loss 5.13, perplexity 169.83\n",
      "[Epoch 10 Batch 5500] loss 5.19, perplexity 178.99\n",
      "[Epoch 10] time cost 35.88s, validation loss 5.21, validation perplexity 182.63\n",
      "[Epoch 11 Batch 500] loss 5.20, perplexity 180.67\n",
      "[Epoch 11 Batch 1000] loss 5.16, perplexity 174.20\n",
      "[Epoch 11 Batch 1500] loss 5.14, perplexity 170.46\n",
      "[Epoch 11 Batch 2000] loss 5.20, perplexity 182.05\n",
      "[Epoch 11 Batch 2500] loss 5.18, perplexity 177.49\n",
      "[Epoch 11 Batch 3000] loss 5.11, perplexity 166.02\n",
      "[Epoch 11 Batch 3500] loss 5.16, perplexity 174.84\n",
      "[Epoch 11 Batch 4000] loss 5.09, perplexity 162.80\n",
      "[Epoch 11 Batch 4500] loss 5.07, perplexity 159.34\n",
      "[Epoch 11 Batch 5000] loss 5.12, perplexity 167.72\n",
      "[Epoch 11 Batch 5500] loss 5.17, perplexity 176.21\n",
      "[Epoch 11] time cost 40.16s, validation loss 5.21, validation perplexity 183.59\n",
      "[Epoch 12 Batch 500] loss 5.18, perplexity 178.50\n",
      "[Epoch 12 Batch 1000] loss 5.14, perplexity 171.28\n",
      "[Epoch 12 Batch 1500] loss 5.13, perplexity 168.29\n",
      "[Epoch 12 Batch 2000] loss 5.19, perplexity 178.64\n",
      "[Epoch 12 Batch 2500] loss 5.16, perplexity 174.87\n",
      "[Epoch 12 Batch 3000] loss 5.10, perplexity 163.84\n",
      "[Epoch 12 Batch 3500] loss 5.15, perplexity 171.91\n",
      "[Epoch 12 Batch 4000] loss 5.08, perplexity 160.87\n",
      "[Epoch 12 Batch 4500] loss 5.06, perplexity 157.71\n",
      "[Epoch 12 Batch 5000] loss 5.11, perplexity 164.94\n",
      "[Epoch 12 Batch 5500] loss 5.15, perplexity 173.08\n",
      "[Epoch 12] time cost 38.01s, validation loss 5.20, validation perplexity 180.50\n",
      "[Epoch 13 Batch 500] loss 5.17, perplexity 175.66\n",
      "[Epoch 13 Batch 1000] loss 5.12, perplexity 167.83\n",
      "[Epoch 13 Batch 1500] loss 5.11, perplexity 166.01\n",
      "[Epoch 13 Batch 2000] loss 5.17, perplexity 176.29\n",
      "[Epoch 13 Batch 2500] loss 5.16, perplexity 173.63\n",
      "[Epoch 13 Batch 3000] loss 5.08, perplexity 161.50\n",
      "[Epoch 13 Batch 3500] loss 5.14, perplexity 171.42\n",
      "[Epoch 13 Batch 4000] loss 5.06, perplexity 158.17\n",
      "[Epoch 13 Batch 4500] loss 5.04, perplexity 155.22\n",
      "[Epoch 13 Batch 5000] loss 5.10, perplexity 163.36\n",
      "[Epoch 13 Batch 5500] loss 5.14, perplexity 171.17\n",
      "[Epoch 13] time cost 36.69s, validation loss 5.18, validation perplexity 178.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14 Batch 500] loss 5.16, perplexity 173.84\n",
      "[Epoch 14 Batch 1000] loss 5.11, perplexity 165.18\n",
      "[Epoch 14 Batch 1500] loss 5.09, perplexity 163.10\n",
      "[Epoch 14 Batch 2000] loss 5.16, perplexity 174.14\n",
      "[Epoch 14 Batch 2500] loss 5.13, perplexity 169.73\n",
      "[Epoch 14 Batch 3000] loss 5.07, perplexity 159.06\n",
      "[Epoch 14 Batch 3500] loss 5.13, perplexity 168.58\n",
      "[Epoch 14 Batch 4000] loss 5.05, perplexity 156.28\n",
      "[Epoch 14 Batch 4500] loss 5.04, perplexity 154.03\n",
      "[Epoch 14 Batch 5000] loss 5.09, perplexity 161.64\n",
      "[Epoch 14 Batch 5500] loss 5.13, perplexity 169.73\n",
      "[Epoch 14] time cost 36.38s, validation loss 5.18, validation perplexity 178.52\n",
      "[Epoch 15 Batch 500] loss 5.14, perplexity 171.50\n",
      "[Epoch 15 Batch 1000] loss 5.10, perplexity 164.10\n",
      "[Epoch 15 Batch 1500] loss 5.09, perplexity 161.97\n",
      "[Epoch 15 Batch 2000] loss 5.15, perplexity 172.07\n",
      "[Epoch 15 Batch 2500] loss 5.13, perplexity 169.15\n",
      "[Epoch 15 Batch 3000] loss 5.06, perplexity 157.26\n",
      "[Epoch 15 Batch 3500] loss 5.11, perplexity 166.18\n",
      "[Epoch 15 Batch 4000] loss 5.04, perplexity 153.79\n",
      "[Epoch 15 Batch 4500] loss 5.03, perplexity 152.44\n",
      "[Epoch 15 Batch 5000] loss 5.08, perplexity 160.49\n",
      "[Epoch 15 Batch 5500] loss 5.12, perplexity 167.01\n",
      "[Epoch 15] time cost 35.62s, validation loss 5.18, validation perplexity 177.84\n",
      "[Epoch 16 Batch 500] loss 5.13, perplexity 169.03\n",
      "[Epoch 16 Batch 1000] loss 5.09, perplexity 162.70\n",
      "[Epoch 16 Batch 1500] loss 5.08, perplexity 161.30\n",
      "[Epoch 16 Batch 2000] loss 5.14, perplexity 170.95\n",
      "[Epoch 16 Batch 2500] loss 5.12, perplexity 167.99\n",
      "[Epoch 16 Batch 3000] loss 5.05, perplexity 156.02\n",
      "[Epoch 16 Batch 3500] loss 5.11, perplexity 165.14\n",
      "[Epoch 16 Batch 4000] loss 5.03, perplexity 153.28\n",
      "[Epoch 16 Batch 4500] loss 5.02, perplexity 151.45\n",
      "[Epoch 16 Batch 5000] loss 5.07, perplexity 159.62\n",
      "[Epoch 16 Batch 5500] loss 5.11, perplexity 166.36\n",
      "[Epoch 16] time cost 36.21s, validation loss 5.17, validation perplexity 175.15\n",
      "[Epoch 17 Batch 500] loss 5.12, perplexity 167.99\n",
      "[Epoch 17 Batch 1000] loss 5.09, perplexity 161.81\n",
      "[Epoch 17 Batch 1500] loss 5.07, perplexity 159.04\n",
      "[Epoch 17 Batch 2000] loss 5.14, perplexity 169.90\n",
      "[Epoch 17 Batch 2500] loss 5.11, perplexity 166.24\n",
      "[Epoch 17 Batch 3000] loss 5.04, perplexity 155.18\n",
      "[Epoch 17 Batch 3500] loss 5.10, perplexity 163.58\n",
      "[Epoch 17 Batch 4000] loss 5.02, perplexity 151.65\n",
      "[Epoch 17 Batch 4500] loss 5.01, perplexity 149.50\n",
      "[Epoch 17 Batch 5000] loss 5.06, perplexity 157.48\n",
      "[Epoch 17 Batch 5500] loss 5.10, perplexity 164.80\n",
      "[Epoch 17] time cost 35.26s, validation loss 5.16, validation perplexity 175.00\n",
      "[Epoch 18 Batch 500] loss 5.11, perplexity 166.31\n",
      "[Epoch 18 Batch 1000] loss 5.08, perplexity 160.00\n",
      "[Epoch 18 Batch 1500] loss 5.06, perplexity 157.34\n",
      "[Epoch 18 Batch 2000] loss 5.12, perplexity 167.20\n",
      "[Epoch 18 Batch 2500] loss 5.11, perplexity 164.97\n",
      "[Epoch 18 Batch 3000] loss 5.04, perplexity 154.03\n",
      "[Epoch 18 Batch 3500] loss 5.09, perplexity 162.84\n",
      "[Epoch 18 Batch 4000] loss 5.01, perplexity 150.65\n",
      "[Epoch 18 Batch 4500] loss 5.00, perplexity 148.84\n",
      "[Epoch 18 Batch 5000] loss 5.05, perplexity 156.47\n",
      "[Epoch 18 Batch 5500] loss 5.11, perplexity 165.04\n",
      "[Epoch 18] time cost 35.99s, validation loss 5.18, validation perplexity 177.63\n",
      "[Epoch 19 Batch 500] loss 5.11, perplexity 166.30\n",
      "[Epoch 19 Batch 1000] loss 5.07, perplexity 158.99\n",
      "[Epoch 19 Batch 1500] loss 5.05, perplexity 156.64\n",
      "[Epoch 19 Batch 2000] loss 5.12, perplexity 166.84\n",
      "[Epoch 19 Batch 2500] loss 5.09, perplexity 162.71\n",
      "[Epoch 19 Batch 3000] loss 5.03, perplexity 152.94\n",
      "[Epoch 19 Batch 3500] loss 5.08, perplexity 161.00\n",
      "[Epoch 19 Batch 4000] loss 5.01, perplexity 149.66\n",
      "[Epoch 19 Batch 4500] loss 5.00, perplexity 148.14\n",
      "[Epoch 19 Batch 5000] loss 5.05, perplexity 155.57\n",
      "[Epoch 19 Batch 5500] loss 5.09, perplexity 163.02\n",
      "[Epoch 19] time cost 35.78s, validation loss 5.17, validation perplexity 175.12\n",
      "[Epoch 20 Batch 500] loss 5.10, perplexity 164.64\n",
      "[Epoch 20 Batch 1000] loss 5.07, perplexity 158.72\n",
      "[Epoch 20 Batch 1500] loss 5.05, perplexity 155.84\n",
      "[Epoch 20 Batch 2000] loss 5.11, perplexity 165.94\n",
      "[Epoch 20 Batch 2500] loss 5.09, perplexity 162.35\n",
      "[Epoch 20 Batch 3000] loss 5.03, perplexity 152.92\n",
      "[Epoch 20 Batch 3500] loss 5.08, perplexity 160.65\n",
      "[Epoch 20 Batch 4000] loss 5.00, perplexity 149.06\n",
      "[Epoch 20 Batch 4500] loss 4.99, perplexity 146.73\n",
      "[Epoch 20 Batch 5000] loss 5.04, perplexity 154.27\n",
      "[Epoch 20 Batch 5500] loss 5.09, perplexity 162.04\n",
      "[Epoch 20] time cost 36.20s, validation loss 5.17, validation perplexity 175.81\n",
      "[Epoch 21 Batch 500] loss 5.10, perplexity 163.97\n",
      "[Epoch 21 Batch 1000] loss 5.05, perplexity 156.60\n",
      "[Epoch 21 Batch 1500] loss 5.03, perplexity 153.55\n",
      "[Epoch 21 Batch 2000] loss 5.10, perplexity 164.36\n",
      "[Epoch 21 Batch 2500] loss 5.08, perplexity 160.64\n",
      "[Epoch 21 Batch 3000] loss 5.02, perplexity 151.19\n",
      "[Epoch 21 Batch 3500] loss 5.07, perplexity 159.15\n",
      "[Epoch 21 Batch 4000] loss 5.01, perplexity 149.26\n",
      "[Epoch 21 Batch 4500] loss 4.99, perplexity 146.22\n",
      "[Epoch 21 Batch 5000] loss 5.04, perplexity 154.25\n",
      "[Epoch 21 Batch 5500] loss 5.08, perplexity 160.95\n",
      "[Epoch 21] time cost 35.74s, validation loss 5.17, validation perplexity 175.40\n",
      "[Epoch 22 Batch 500] loss 5.09, perplexity 162.91\n",
      "[Epoch 22 Batch 1000] loss 5.05, perplexity 155.80\n",
      "[Epoch 22 Batch 1500] loss 5.03, perplexity 153.32\n",
      "[Epoch 22 Batch 2000] loss 5.10, perplexity 163.78\n",
      "[Epoch 22 Batch 2500] loss 5.08, perplexity 160.52\n",
      "[Epoch 22 Batch 3000] loss 5.01, perplexity 150.25\n",
      "[Epoch 22 Batch 3500] loss 5.07, perplexity 159.04\n",
      "[Epoch 22 Batch 4000] loss 4.99, perplexity 147.17\n",
      "[Epoch 22 Batch 4500] loss 4.98, perplexity 144.97\n",
      "[Epoch 22 Batch 5000] loss 5.03, perplexity 152.80\n",
      "[Epoch 22 Batch 5500] loss 5.08, perplexity 160.38\n",
      "[Epoch 22] time cost 35.57s, validation loss 5.15, validation perplexity 173.21\n",
      "[Epoch 23 Batch 500] loss 5.09, perplexity 162.04\n",
      "[Epoch 23 Batch 1000] loss 5.05, perplexity 155.66\n",
      "[Epoch 23 Batch 1500] loss 5.03, perplexity 152.61\n",
      "[Epoch 23 Batch 2000] loss 5.09, perplexity 163.01\n",
      "[Epoch 23 Batch 2500] loss 5.08, perplexity 160.53\n",
      "[Epoch 23 Batch 3000] loss 5.01, perplexity 150.29\n",
      "[Epoch 23 Batch 3500] loss 5.06, perplexity 158.23\n",
      "[Epoch 23 Batch 4000] loss 4.99, perplexity 146.92\n",
      "[Epoch 23 Batch 4500] loss 4.97, perplexity 144.33\n",
      "[Epoch 23 Batch 5000] loss 5.02, perplexity 152.05\n",
      "[Epoch 23 Batch 5500] loss 5.07, perplexity 159.43\n",
      "[Epoch 23] time cost 36.83s, validation loss 5.17, validation perplexity 175.08\n",
      "[Epoch 24 Batch 500] loss 5.09, perplexity 161.85\n",
      "[Epoch 24 Batch 1000] loss 5.05, perplexity 155.31\n",
      "[Epoch 24 Batch 1500] loss 5.02, perplexity 151.33\n",
      "[Epoch 24 Batch 2000] loss 5.09, perplexity 161.61\n",
      "[Epoch 24 Batch 2500] loss 5.07, perplexity 159.56\n",
      "[Epoch 24 Batch 3000] loss 5.00, perplexity 148.89\n",
      "[Epoch 24 Batch 3500] loss 5.06, perplexity 157.75\n",
      "[Epoch 24 Batch 4000] loss 4.98, perplexity 146.06\n",
      "[Epoch 24 Batch 4500] loss 4.97, perplexity 143.80\n",
      "[Epoch 24 Batch 5000] loss 5.02, perplexity 151.86\n",
      "[Epoch 24 Batch 5500] loss 5.06, perplexity 158.35\n",
      "[Epoch 24] time cost 36.81s, validation loss 5.17, validation perplexity 175.11\n",
      "[Epoch 25 Batch 500] loss 5.07, perplexity 159.96\n",
      "[Epoch 25 Batch 1000] loss 5.04, perplexity 153.75\n",
      "[Epoch 25 Batch 1500] loss 5.02, perplexity 151.98\n",
      "[Epoch 25 Batch 2000] loss 5.08, perplexity 160.91\n",
      "[Epoch 25 Batch 2500] loss 5.07, perplexity 158.52\n",
      "[Epoch 25 Batch 3000] loss 5.00, perplexity 147.87\n",
      "[Epoch 25 Batch 3500] loss 5.06, perplexity 157.06\n",
      "[Epoch 25 Batch 4000] loss 4.98, perplexity 145.64\n",
      "[Epoch 25 Batch 4500] loss 4.96, perplexity 142.90\n",
      "[Epoch 25 Batch 5000] loss 5.02, perplexity 151.11\n",
      "[Epoch 25 Batch 5500] loss 5.06, perplexity 157.88\n",
      "[Epoch 25] time cost 36.46s, validation loss 5.16, validation perplexity 173.64\n",
      "[Epoch 26 Batch 500] loss 5.07, perplexity 159.12\n",
      "[Epoch 26 Batch 1000] loss 5.03, perplexity 153.04\n",
      "[Epoch 26 Batch 1500] loss 5.02, perplexity 151.06\n",
      "[Epoch 26 Batch 2000] loss 5.07, perplexity 159.55\n",
      "[Epoch 26 Batch 2500] loss 5.06, perplexity 157.19\n",
      "[Epoch 26 Batch 3000] loss 5.00, perplexity 147.97\n",
      "[Epoch 26 Batch 3500] loss 5.05, perplexity 155.86\n",
      "[Epoch 26 Batch 4000] loss 4.98, perplexity 145.22\n",
      "[Epoch 26 Batch 4500] loss 4.96, perplexity 142.95\n",
      "[Epoch 26 Batch 5000] loss 5.01, perplexity 149.66\n",
      "[Epoch 26 Batch 5500] loss 5.06, perplexity 157.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] time cost 37.12s, validation loss 5.15, validation perplexity 173.24\n",
      "[Epoch 27 Batch 500] loss 5.07, perplexity 158.49\n",
      "[Epoch 27 Batch 1000] loss 5.03, perplexity 152.36\n",
      "[Epoch 27 Batch 1500] loss 5.01, perplexity 150.30\n",
      "[Epoch 27 Batch 2000] loss 5.07, perplexity 159.42\n",
      "[Epoch 27 Batch 2500] loss 5.06, perplexity 157.17\n",
      "[Epoch 27 Batch 3000] loss 4.99, perplexity 147.09\n",
      "[Epoch 27 Batch 3500] loss 5.04, perplexity 154.62\n",
      "[Epoch 27 Batch 4000] loss 4.97, perplexity 143.55\n",
      "[Epoch 27 Batch 4500] loss 4.96, perplexity 142.00\n",
      "[Epoch 27 Batch 5000] loss 5.01, perplexity 149.83\n",
      "[Epoch 27 Batch 5500] loss 5.05, perplexity 156.45\n",
      "[Epoch 27] time cost 38.13s, validation loss 5.17, validation perplexity 175.11\n",
      "[Epoch 28 Batch 500] loss 5.07, perplexity 158.65\n",
      "[Epoch 28 Batch 1000] loss 5.03, perplexity 152.20\n",
      "[Epoch 28 Batch 1500] loss 5.00, perplexity 149.15\n",
      "[Epoch 28 Batch 2000] loss 5.07, perplexity 158.81\n",
      "[Epoch 28 Batch 2500] loss 5.05, perplexity 155.75\n",
      "[Epoch 28 Batch 3000] loss 4.99, perplexity 146.24\n",
      "[Epoch 28 Batch 3500] loss 5.04, perplexity 154.21\n",
      "[Epoch 28 Batch 4000] loss 4.97, perplexity 143.77\n",
      "[Epoch 28 Batch 4500] loss 4.95, perplexity 141.85\n",
      "[Epoch 28 Batch 5000] loss 5.00, perplexity 148.47\n",
      "[Epoch 28 Batch 5500] loss 5.04, perplexity 155.03\n",
      "[Epoch 28] time cost 42.56s, validation loss 5.17, validation perplexity 176.76\n",
      "[Epoch 29 Batch 500] loss 5.06, perplexity 157.13\n",
      "[Epoch 29 Batch 1000] loss 5.02, perplexity 151.82\n",
      "[Epoch 29 Batch 1500] loss 5.01, perplexity 149.28\n",
      "[Epoch 29 Batch 2000] loss 5.07, perplexity 158.68\n",
      "[Epoch 29 Batch 2500] loss 5.05, perplexity 156.04\n",
      "[Epoch 29 Batch 3000] loss 4.98, perplexity 145.89\n",
      "[Epoch 29 Batch 3500] loss 5.04, perplexity 153.82\n",
      "[Epoch 29 Batch 4000] loss 4.96, perplexity 142.38\n",
      "[Epoch 29 Batch 4500] loss 4.95, perplexity 140.87\n",
      "[Epoch 29 Batch 5000] loss 5.00, perplexity 147.74\n",
      "[Epoch 29 Batch 5500] loss 5.04, perplexity 155.18\n",
      "[Epoch 29] time cost 36.04s, validation loss 5.14, validation perplexity 171.24\n",
      "[Epoch 30 Batch 500] loss 5.06, perplexity 157.68\n",
      "[Epoch 30 Batch 1000] loss 5.02, perplexity 151.29\n",
      "[Epoch 30 Batch 1500] loss 5.00, perplexity 148.60\n",
      "[Epoch 30 Batch 2000] loss 5.06, perplexity 157.77\n",
      "[Epoch 30 Batch 2500] loss 5.05, perplexity 155.54\n",
      "[Epoch 30 Batch 3000] loss 4.98, perplexity 145.08\n",
      "[Epoch 30 Batch 3500] loss 5.03, perplexity 152.91\n",
      "[Epoch 30 Batch 4000] loss 4.96, perplexity 141.95\n",
      "[Epoch 30 Batch 4500] loss 4.95, perplexity 140.81\n",
      "[Epoch 30 Batch 5000] loss 4.99, perplexity 146.93\n",
      "[Epoch 30 Batch 5500] loss 5.04, perplexity 154.87\n",
      "[Epoch 30] time cost 36.28s, validation loss 5.16, validation perplexity 173.43\n",
      "[Epoch 31 Batch 500] loss 5.06, perplexity 157.02\n",
      "[Epoch 31 Batch 1000] loss 5.02, perplexity 150.81\n",
      "[Epoch 31 Batch 1500] loss 5.00, perplexity 148.07\n",
      "[Epoch 31 Batch 2000] loss 5.06, perplexity 157.66\n",
      "[Epoch 31 Batch 2500] loss 5.04, perplexity 154.57\n",
      "[Epoch 31 Batch 3000] loss 4.98, perplexity 145.06\n",
      "[Epoch 31 Batch 3500] loss 5.03, perplexity 152.85\n",
      "[Epoch 31 Batch 4000] loss 4.95, perplexity 141.42\n",
      "[Epoch 31 Batch 4500] loss 4.94, perplexity 139.87\n",
      "[Epoch 31 Batch 5000] loss 5.00, perplexity 148.53\n",
      "[Epoch 31 Batch 5500] loss 5.04, perplexity 153.97\n",
      "[Epoch 31] time cost 37.64s, validation loss 5.16, validation perplexity 174.44\n",
      "[Epoch 32 Batch 500] loss 5.05, perplexity 156.01\n",
      "[Epoch 32 Batch 1000] loss 5.01, perplexity 149.78\n",
      "[Epoch 32 Batch 1500] loss 5.00, perplexity 148.23\n",
      "[Epoch 32 Batch 2000] loss 5.06, perplexity 157.25\n",
      "[Epoch 32 Batch 2500] loss 5.04, perplexity 154.07\n",
      "[Epoch 32 Batch 3000] loss 4.97, perplexity 144.37\n",
      "[Epoch 32 Batch 3500] loss 5.02, perplexity 151.87\n",
      "[Epoch 32 Batch 4000] loss 4.95, perplexity 141.55\n",
      "[Epoch 32 Batch 4500] loss 4.94, perplexity 139.88\n",
      "[Epoch 32 Batch 5000] loss 4.99, perplexity 147.43\n",
      "[Epoch 32 Batch 5500] loss 5.03, perplexity 153.58\n",
      "[Epoch 32] time cost 36.38s, validation loss 5.16, validation perplexity 173.92\n",
      "[Epoch 33 Batch 500] loss 5.05, perplexity 155.52\n",
      "[Epoch 33 Batch 1000] loss 5.01, perplexity 150.50\n",
      "[Epoch 33 Batch 1500] loss 4.99, perplexity 147.20\n",
      "[Epoch 33 Batch 2000] loss 5.06, perplexity 157.21\n",
      "[Epoch 33 Batch 2500] loss 5.04, perplexity 154.47\n",
      "[Epoch 33 Batch 3000] loss 4.97, perplexity 144.28\n",
      "[Epoch 33 Batch 3500] loss 5.02, perplexity 151.82\n",
      "[Epoch 33 Batch 4000] loss 4.95, perplexity 140.64\n",
      "[Epoch 33 Batch 4500] loss 4.94, perplexity 139.69\n",
      "[Epoch 33 Batch 5000] loss 4.98, perplexity 146.12\n",
      "[Epoch 33 Batch 5500] loss 5.03, perplexity 152.71\n",
      "[Epoch 33] time cost 36.40s, validation loss 5.16, validation perplexity 174.10\n",
      "[Epoch 34 Batch 500] loss 5.04, perplexity 154.83\n",
      "[Epoch 34 Batch 1000] loss 5.00, perplexity 148.21\n",
      "[Epoch 34 Batch 1500] loss 4.99, perplexity 146.44\n",
      "[Epoch 34 Batch 2000] loss 5.05, perplexity 156.59\n",
      "[Epoch 34 Batch 2500] loss 5.03, perplexity 153.00\n",
      "[Epoch 34 Batch 3000] loss 4.97, perplexity 144.41\n",
      "[Epoch 34 Batch 3500] loss 5.02, perplexity 151.91\n",
      "[Epoch 34 Batch 4000] loss 4.95, perplexity 141.27\n",
      "[Epoch 34 Batch 4500] loss 4.93, perplexity 138.80\n",
      "[Epoch 34 Batch 5000] loss 4.98, perplexity 146.07\n",
      "[Epoch 34 Batch 5500] loss 5.03, perplexity 153.00\n",
      "[Epoch 34] time cost 35.97s, validation loss 5.16, validation perplexity 173.82\n",
      "[Epoch 35 Batch 500] loss 5.04, perplexity 155.17\n",
      "[Epoch 35 Batch 1000] loss 5.00, perplexity 149.08\n",
      "[Epoch 35 Batch 1500] loss 4.99, perplexity 146.31\n",
      "[Epoch 35 Batch 2000] loss 5.05, perplexity 156.33\n",
      "[Epoch 35 Batch 2500] loss 5.03, perplexity 153.31\n",
      "[Epoch 35 Batch 3000] loss 4.97, perplexity 143.65\n",
      "[Epoch 35 Batch 3500] loss 5.02, perplexity 151.92\n",
      "[Epoch 35 Batch 4000] loss 4.94, perplexity 139.88\n",
      "[Epoch 35 Batch 4500] loss 4.93, perplexity 138.79\n",
      "[Epoch 35 Batch 5000] loss 4.99, perplexity 146.21\n",
      "[Epoch 35 Batch 5500] loss 5.03, perplexity 152.80\n",
      "[Epoch 35] time cost 35.22s, validation loss 5.15, validation perplexity 172.42\n",
      "[Epoch 36 Batch 500] loss 5.04, perplexity 154.86\n",
      "[Epoch 36 Batch 1000] loss 5.01, perplexity 149.17\n",
      "[Epoch 36 Batch 1500] loss 4.99, perplexity 147.10\n",
      "[Epoch 36 Batch 2000] loss 5.04, perplexity 155.18\n",
      "[Epoch 36 Batch 2500] loss 5.03, perplexity 152.84\n",
      "[Epoch 36 Batch 3000] loss 4.96, perplexity 142.81\n",
      "[Epoch 36 Batch 3500] loss 5.02, perplexity 151.20\n",
      "[Epoch 36 Batch 4000] loss 4.94, perplexity 140.00\n",
      "[Epoch 36 Batch 4500] loss 4.93, perplexity 138.73\n",
      "[Epoch 36 Batch 5000] loss 4.97, perplexity 144.43\n",
      "[Epoch 36 Batch 5500] loss 5.03, perplexity 152.48\n",
      "[Epoch 36] time cost 37.14s, validation loss 5.15, validation perplexity 172.89\n",
      "[Epoch 37 Batch 500] loss 5.04, perplexity 153.88\n",
      "[Epoch 37 Batch 1000] loss 5.00, perplexity 148.58\n",
      "[Epoch 37 Batch 1500] loss 4.98, perplexity 145.96\n",
      "[Epoch 37 Batch 2000] loss 5.04, perplexity 155.16\n",
      "[Epoch 37 Batch 2500] loss 5.03, perplexity 152.40\n",
      "[Epoch 37 Batch 3000] loss 4.96, perplexity 142.63\n",
      "[Epoch 37 Batch 3500] loss 5.01, perplexity 150.53\n",
      "[Epoch 37 Batch 4000] loss 4.94, perplexity 139.39\n",
      "[Epoch 37 Batch 4500] loss 4.92, perplexity 137.49\n",
      "[Epoch 37 Batch 5000] loss 4.97, perplexity 144.66\n",
      "[Epoch 37 Batch 5500] loss 5.02, perplexity 151.67\n",
      "[Epoch 37] time cost 34.68s, validation loss 5.15, validation perplexity 172.16\n",
      "[Epoch 38 Batch 500] loss 5.03, perplexity 153.51\n",
      "[Epoch 38 Batch 1000] loss 5.00, perplexity 148.30\n",
      "[Epoch 38 Batch 1500] loss 4.98, perplexity 146.12\n",
      "[Epoch 38 Batch 2000] loss 5.04, perplexity 154.40\n",
      "[Epoch 38 Batch 2500] loss 5.02, perplexity 150.90\n",
      "[Epoch 38 Batch 3000] loss 4.96, perplexity 142.68\n",
      "[Epoch 38 Batch 3500] loss 5.01, perplexity 150.21\n",
      "[Epoch 38 Batch 4000] loss 4.93, perplexity 139.02\n",
      "[Epoch 38 Batch 4500] loss 4.92, perplexity 137.61\n",
      "[Epoch 38 Batch 5000] loss 4.97, perplexity 144.40\n",
      "[Epoch 38 Batch 5500] loss 5.02, perplexity 151.28\n",
      "[Epoch 38] time cost 33.18s, validation loss 5.16, validation perplexity 174.43\n",
      "[Epoch 39 Batch 500] loss 5.03, perplexity 153.51\n",
      "[Epoch 39 Batch 1000] loss 5.00, perplexity 148.24\n",
      "[Epoch 39 Batch 1500] loss 4.98, perplexity 144.85\n",
      "[Epoch 39 Batch 2000] loss 5.04, perplexity 154.58\n",
      "[Epoch 39 Batch 2500] loss 5.02, perplexity 151.59\n",
      "[Epoch 39 Batch 3000] loss 4.96, perplexity 142.13\n",
      "[Epoch 39 Batch 3500] loss 5.01, perplexity 150.07\n",
      "[Epoch 39 Batch 4000] loss 4.93, perplexity 138.81\n",
      "[Epoch 39 Batch 4500] loss 4.92, perplexity 137.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39 Batch 5000] loss 4.98, perplexity 144.95\n",
      "[Epoch 39 Batch 5500] loss 5.02, perplexity 151.50\n",
      "[Epoch 39] time cost 34.50s, validation loss 5.17, validation perplexity 175.14\n",
      "[Epoch 40 Batch 500] loss 5.03, perplexity 152.47\n",
      "[Epoch 40 Batch 1000] loss 4.99, perplexity 146.62\n",
      "[Epoch 40 Batch 1500] loss 4.97, perplexity 143.87\n",
      "[Epoch 40 Batch 2000] loss 5.04, perplexity 154.20\n",
      "[Epoch 40 Batch 2500] loss 5.01, perplexity 150.42\n",
      "[Epoch 40 Batch 3000] loss 4.95, perplexity 141.81\n",
      "[Epoch 40 Batch 3500] loss 5.00, perplexity 148.92\n",
      "[Epoch 40 Batch 4000] loss 4.94, perplexity 139.15\n",
      "[Epoch 40 Batch 4500] loss 4.92, perplexity 137.23\n",
      "[Epoch 40 Batch 5000] loss 4.97, perplexity 144.57\n",
      "[Epoch 40 Batch 5500] loss 5.01, perplexity 150.08\n",
      "[Epoch 40] time cost 34.18s, validation loss 5.15, validation perplexity 172.99\n",
      "[Epoch 41 Batch 500] loss 5.03, perplexity 152.34\n",
      "[Epoch 41 Batch 1000] loss 4.99, perplexity 146.89\n",
      "[Epoch 41 Batch 1500] loss 4.97, perplexity 144.03\n",
      "[Epoch 41 Batch 2000] loss 5.03, perplexity 153.34\n",
      "[Epoch 41 Batch 2500] loss 5.01, perplexity 150.35\n",
      "[Epoch 41 Batch 3000] loss 4.95, perplexity 140.87\n",
      "[Epoch 41 Batch 3500] loss 5.00, perplexity 148.90\n",
      "[Epoch 41 Batch 4000] loss 4.92, perplexity 137.51\n",
      "[Epoch 41 Batch 4500] loss 4.91, perplexity 136.24\n",
      "[Epoch 41 Batch 5000] loss 4.97, perplexity 143.60\n",
      "[Epoch 41 Batch 5500] loss 5.01, perplexity 150.01\n",
      "[Epoch 41] time cost 35.21s, validation loss 5.16, validation perplexity 173.61\n",
      "[Epoch 42 Batch 500] loss 5.03, perplexity 152.24\n",
      "[Epoch 42 Batch 1000] loss 4.99, perplexity 146.56\n",
      "[Epoch 42 Batch 1500] loss 4.97, perplexity 143.67\n",
      "[Epoch 42 Batch 2000] loss 5.04, perplexity 154.16\n",
      "[Epoch 42 Batch 2500] loss 5.01, perplexity 149.95\n",
      "[Epoch 42 Batch 3000] loss 4.95, perplexity 140.83\n",
      "[Epoch 42 Batch 3500] loss 5.00, perplexity 148.93\n",
      "[Epoch 42 Batch 4000] loss 4.93, perplexity 138.17\n",
      "[Epoch 42 Batch 4500] loss 4.92, perplexity 137.24\n",
      "[Epoch 42 Batch 5000] loss 4.96, perplexity 143.27\n",
      "[Epoch 42 Batch 5500] loss 5.01, perplexity 149.93\n",
      "[Epoch 42] time cost 34.72s, validation loss 5.16, validation perplexity 173.63\n",
      "[Epoch 43 Batch 500] loss 5.02, perplexity 151.83\n",
      "[Epoch 43 Batch 1000] loss 4.99, perplexity 146.98\n",
      "[Epoch 43 Batch 1500] loss 4.97, perplexity 143.32\n",
      "[Epoch 43 Batch 2000] loss 5.03, perplexity 152.76\n",
      "[Epoch 43 Batch 2500] loss 5.01, perplexity 149.27\n",
      "[Epoch 43 Batch 3000] loss 4.95, perplexity 140.95\n",
      "[Epoch 43 Batch 3500] loss 5.00, perplexity 148.42\n",
      "[Epoch 43 Batch 4000] loss 4.92, perplexity 137.42\n",
      "[Epoch 43 Batch 4500] loss 4.92, perplexity 137.12\n",
      "[Epoch 43 Batch 5000] loss 4.97, perplexity 143.51\n",
      "[Epoch 43 Batch 5500] loss 5.01, perplexity 149.59\n",
      "[Epoch 43] time cost 36.11s, validation loss 5.16, validation perplexity 173.49\n",
      "[Epoch 44 Batch 500] loss 5.02, perplexity 151.48\n",
      "[Epoch 44 Batch 1000] loss 4.98, perplexity 145.72\n",
      "[Epoch 44 Batch 1500] loss 4.96, perplexity 143.00\n",
      "[Epoch 44 Batch 2000] loss 5.03, perplexity 152.46\n",
      "[Epoch 44 Batch 2500] loss 5.01, perplexity 150.08\n",
      "[Epoch 44 Batch 3000] loss 4.94, perplexity 140.37\n",
      "[Epoch 44 Batch 3500] loss 5.00, perplexity 148.37\n",
      "[Epoch 44 Batch 4000] loss 4.92, perplexity 136.56\n",
      "[Epoch 44 Batch 4500] loss 4.91, perplexity 136.11\n",
      "[Epoch 44 Batch 5000] loss 4.97, perplexity 143.37\n",
      "[Epoch 44 Batch 5500] loss 5.01, perplexity 149.65\n",
      "[Epoch 44] time cost 37.39s, validation loss 5.15, validation perplexity 173.23\n",
      "[Epoch 45 Batch 500] loss 5.02, perplexity 151.10\n",
      "[Epoch 45 Batch 1000] loss 4.98, perplexity 145.29\n",
      "[Epoch 45 Batch 1500] loss 4.96, perplexity 142.50\n",
      "[Epoch 45 Batch 2000] loss 5.03, perplexity 153.11\n",
      "[Epoch 45 Batch 2500] loss 5.01, perplexity 150.37\n",
      "[Epoch 45 Batch 3000] loss 4.94, perplexity 140.08\n",
      "[Epoch 45 Batch 3500] loss 5.00, perplexity 147.89\n",
      "[Epoch 45 Batch 4000] loss 4.92, perplexity 136.97\n",
      "[Epoch 45 Batch 4500] loss 4.91, perplexity 135.22\n",
      "[Epoch 45 Batch 5000] loss 4.97, perplexity 143.33\n",
      "[Epoch 45 Batch 5500] loss 5.01, perplexity 149.35\n",
      "[Epoch 45] time cost 37.48s, validation loss 5.16, validation perplexity 174.38\n",
      "[Epoch 46 Batch 500] loss 5.02, perplexity 151.19\n",
      "[Epoch 46 Batch 1000] loss 4.98, perplexity 145.83\n",
      "[Epoch 46 Batch 1500] loss 4.95, perplexity 141.70\n",
      "[Epoch 46 Batch 2000] loss 5.02, perplexity 152.15\n",
      "[Epoch 46 Batch 2500] loss 5.01, perplexity 149.48\n",
      "[Epoch 46 Batch 3000] loss 4.94, perplexity 139.38\n",
      "[Epoch 46 Batch 3500] loss 4.99, perplexity 147.26\n",
      "[Epoch 46 Batch 4000] loss 4.92, perplexity 136.98\n",
      "[Epoch 46 Batch 4500] loss 4.90, perplexity 134.92\n",
      "[Epoch 46 Batch 5000] loss 4.96, perplexity 143.17\n",
      "[Epoch 46 Batch 5500] loss 5.01, perplexity 149.20\n",
      "[Epoch 46] time cost 36.09s, validation loss 5.15, validation perplexity 173.06\n",
      "[Epoch 47 Batch 500] loss 5.02, perplexity 151.60\n",
      "[Epoch 47 Batch 1000] loss 4.98, perplexity 145.02\n",
      "[Epoch 47 Batch 1500] loss 4.96, perplexity 142.93\n",
      "[Epoch 47 Batch 2000] loss 5.02, perplexity 152.13\n",
      "[Epoch 47 Batch 2500] loss 5.01, perplexity 149.66\n",
      "[Epoch 47 Batch 3000] loss 4.94, perplexity 139.72\n",
      "[Epoch 47 Batch 3500] loss 5.00, perplexity 148.14\n",
      "[Epoch 47 Batch 4000] loss 4.92, perplexity 137.16\n",
      "[Epoch 47 Batch 4500] loss 4.91, perplexity 134.97\n",
      "[Epoch 47 Batch 5000] loss 4.96, perplexity 142.87\n",
      "[Epoch 47 Batch 5500] loss 5.00, perplexity 148.55\n",
      "[Epoch 47] time cost 35.42s, validation loss 5.16, validation perplexity 174.41\n",
      "[Epoch 48 Batch 500] loss 5.02, perplexity 151.05\n",
      "[Epoch 48 Batch 1000] loss 4.98, perplexity 145.49\n",
      "[Epoch 48 Batch 1500] loss 4.96, perplexity 142.69\n",
      "[Epoch 48 Batch 2000] loss 5.02, perplexity 151.75\n",
      "[Epoch 48 Batch 2500] loss 5.01, perplexity 149.41\n",
      "[Epoch 48 Batch 3000] loss 4.94, perplexity 139.30\n",
      "[Epoch 48 Batch 3500] loss 4.99, perplexity 147.05\n",
      "[Epoch 48 Batch 4000] loss 4.92, perplexity 136.79\n",
      "[Epoch 48 Batch 4500] loss 4.91, perplexity 135.32\n",
      "[Epoch 48 Batch 5000] loss 4.96, perplexity 142.74\n",
      "[Epoch 48 Batch 5500] loss 5.00, perplexity 148.90\n",
      "[Epoch 48] time cost 33.48s, validation loss 5.16, validation perplexity 174.24\n",
      "[Epoch 49 Batch 500] loss 5.01, perplexity 150.20\n",
      "[Epoch 49 Batch 1000] loss 4.98, perplexity 144.79\n",
      "[Epoch 49 Batch 1500] loss 4.96, perplexity 141.94\n",
      "[Epoch 49 Batch 2000] loss 5.03, perplexity 152.22\n",
      "[Epoch 49 Batch 2500] loss 5.00, perplexity 149.15\n",
      "[Epoch 49 Batch 3000] loss 4.94, perplexity 139.10\n",
      "[Epoch 49 Batch 3500] loss 4.99, perplexity 146.80\n",
      "[Epoch 49 Batch 4000] loss 4.91, perplexity 135.64\n",
      "[Epoch 49 Batch 4500] loss 4.90, perplexity 133.81\n",
      "[Epoch 49 Batch 5000] loss 4.95, perplexity 141.51\n",
      "[Epoch 49 Batch 5500] loss 5.00, perplexity 148.14\n",
      "[Epoch 49] time cost 34.49s, validation loss 5.16, validation perplexity 173.39\n",
      "[Epoch 50 Batch 500] loss 5.01, perplexity 150.00\n",
      "[Epoch 50 Batch 1000] loss 4.97, perplexity 144.14\n",
      "[Epoch 50 Batch 1500] loss 4.96, perplexity 141.95\n",
      "[Epoch 50 Batch 2000] loss 5.02, perplexity 151.39\n",
      "[Epoch 50 Batch 2500] loss 5.00, perplexity 148.50\n",
      "[Epoch 50 Batch 3000] loss 4.93, perplexity 139.00\n",
      "[Epoch 50 Batch 3500] loss 4.99, perplexity 146.59\n",
      "[Epoch 50 Batch 4000] loss 4.91, perplexity 136.16\n",
      "[Epoch 50 Batch 4500] loss 4.90, perplexity 134.74\n",
      "[Epoch 50 Batch 5000] loss 4.95, perplexity 141.46\n",
      "[Epoch 50 Batch 5500] loss 5.00, perplexity 148.24\n",
      "[Epoch 50] time cost 36.12s, validation loss 5.16, validation perplexity 174.89\n",
      "[Epoch 51 Batch 500] loss 5.01, perplexity 149.94\n",
      "[Epoch 51 Batch 1000] loss 4.97, perplexity 144.04\n",
      "[Epoch 51 Batch 1500] loss 4.95, perplexity 141.83\n",
      "[Epoch 51 Batch 2000] loss 5.01, perplexity 150.32\n",
      "[Epoch 51 Batch 2500] loss 5.00, perplexity 148.57\n",
      "[Epoch 51 Batch 3000] loss 4.94, perplexity 139.86\n",
      "[Epoch 51 Batch 3500] loss 4.99, perplexity 146.65\n",
      "[Epoch 51 Batch 4000] loss 4.91, perplexity 136.27\n",
      "[Epoch 51 Batch 4500] loss 4.91, perplexity 135.03\n",
      "[Epoch 51 Batch 5000] loss 4.95, perplexity 140.68\n",
      "[Epoch 51 Batch 5500] loss 5.00, perplexity 148.14\n",
      "[Epoch 51] time cost 36.26s, validation loss 5.15, validation perplexity 171.61\n",
      "[Epoch 52 Batch 500] loss 5.02, perplexity 150.72\n",
      "[Epoch 52 Batch 1000] loss 4.96, perplexity 143.18\n",
      "[Epoch 52 Batch 1500] loss 4.94, perplexity 140.22\n",
      "[Epoch 52 Batch 2000] loss 5.01, perplexity 150.30\n",
      "[Epoch 52 Batch 2500] loss 4.99, perplexity 147.44\n",
      "[Epoch 52 Batch 3000] loss 4.93, perplexity 138.93\n",
      "[Epoch 52 Batch 3500] loss 4.99, perplexity 146.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 52 Batch 4000] loss 4.91, perplexity 135.33\n",
      "[Epoch 52 Batch 4500] loss 4.90, perplexity 133.95\n",
      "[Epoch 52 Batch 5000] loss 4.95, perplexity 140.93\n",
      "[Epoch 52 Batch 5500] loss 4.99, perplexity 146.88\n",
      "[Epoch 52] time cost 35.39s, validation loss 5.16, validation perplexity 174.55\n",
      "[Epoch 53 Batch 500] loss 5.01, perplexity 149.70\n",
      "[Epoch 53 Batch 1000] loss 4.97, perplexity 143.74\n",
      "[Epoch 53 Batch 1500] loss 4.95, perplexity 141.07\n",
      "[Epoch 53 Batch 2000] loss 5.01, perplexity 149.46\n",
      "[Epoch 53 Batch 2500] loss 4.99, perplexity 147.60\n",
      "[Epoch 53 Batch 3000] loss 4.93, perplexity 138.26\n",
      "[Epoch 53 Batch 3500] loss 4.99, perplexity 146.22\n",
      "[Epoch 53 Batch 4000] loss 4.92, perplexity 136.35\n",
      "[Epoch 53 Batch 4500] loss 4.90, perplexity 133.64\n",
      "[Epoch 53 Batch 5000] loss 4.95, perplexity 140.72\n",
      "[Epoch 53 Batch 5500] loss 4.99, perplexity 146.30\n",
      "[Epoch 53] time cost 32.94s, validation loss 5.16, validation perplexity 174.12\n",
      "[Epoch 54 Batch 500] loss 5.00, perplexity 148.98\n",
      "[Epoch 54 Batch 1000] loss 4.97, perplexity 143.67\n",
      "[Epoch 54 Batch 1500] loss 4.94, perplexity 140.44\n",
      "[Epoch 54 Batch 2000] loss 5.01, perplexity 149.80\n",
      "[Epoch 54 Batch 2500] loss 4.99, perplexity 146.72\n",
      "[Epoch 54 Batch 3000] loss 4.93, perplexity 138.07\n",
      "[Epoch 54 Batch 3500] loss 4.98, perplexity 145.32\n",
      "[Epoch 54 Batch 4000] loss 4.91, perplexity 135.15\n",
      "[Epoch 54 Batch 4500] loss 4.90, perplexity 134.20\n",
      "[Epoch 54 Batch 5000] loss 4.95, perplexity 140.59\n",
      "[Epoch 54 Batch 5500] loss 4.99, perplexity 146.93\n",
      "[Epoch 54] time cost 33.27s, validation loss 5.16, validation perplexity 174.45\n",
      "[Epoch 55 Batch 500] loss 5.01, perplexity 149.22\n",
      "[Epoch 55 Batch 1000] loss 4.97, perplexity 143.66\n",
      "[Epoch 55 Batch 1500] loss 4.95, perplexity 140.94\n",
      "[Epoch 55 Batch 2000] loss 5.01, perplexity 150.04\n",
      "[Epoch 55 Batch 2500] loss 4.99, perplexity 147.40\n",
      "[Epoch 55 Batch 3000] loss 4.93, perplexity 138.79\n",
      "[Epoch 55 Batch 3500] loss 4.98, perplexity 145.12\n",
      "[Epoch 55 Batch 4000] loss 4.91, perplexity 135.58\n",
      "[Epoch 55 Batch 4500] loss 4.89, perplexity 132.81\n",
      "[Epoch 55 Batch 5000] loss 4.95, perplexity 141.53\n",
      "[Epoch 55 Batch 5500] loss 4.99, perplexity 146.67\n",
      "[Epoch 55] time cost 35.70s, validation loss 5.15, validation perplexity 172.20\n",
      "[Epoch 56 Batch 500] loss 5.00, perplexity 148.38\n",
      "[Epoch 56 Batch 1000] loss 4.96, perplexity 143.00\n",
      "[Epoch 56 Batch 1500] loss 4.95, perplexity 141.13\n",
      "[Epoch 56 Batch 2000] loss 5.01, perplexity 149.45\n",
      "[Epoch 56 Batch 2500] loss 4.99, perplexity 147.59\n",
      "[Epoch 56 Batch 3000] loss 4.93, perplexity 137.78\n",
      "[Epoch 56 Batch 3500] loss 4.98, perplexity 145.55\n",
      "[Epoch 56 Batch 4000] loss 4.91, perplexity 135.23\n",
      "[Epoch 56 Batch 4500] loss 4.89, perplexity 133.44\n",
      "[Epoch 56 Batch 5000] loss 4.94, perplexity 140.11\n",
      "[Epoch 56 Batch 5500] loss 4.99, perplexity 146.62\n",
      "[Epoch 56] time cost 36.86s, validation loss 5.16, validation perplexity 174.63\n",
      "[Epoch 57 Batch 500] loss 5.01, perplexity 149.88\n",
      "[Epoch 57 Batch 1000] loss 4.96, perplexity 142.57\n",
      "[Epoch 57 Batch 1500] loss 4.95, perplexity 140.95\n",
      "[Epoch 57 Batch 2000] loss 5.01, perplexity 150.28\n",
      "[Epoch 57 Batch 2500] loss 4.99, perplexity 147.41\n",
      "[Epoch 57 Batch 3000] loss 4.92, perplexity 137.22\n",
      "[Epoch 57 Batch 3500] loss 4.98, perplexity 145.48\n",
      "[Epoch 57 Batch 4000] loss 4.91, perplexity 135.13\n",
      "[Epoch 57 Batch 4500] loss 4.89, perplexity 132.91\n",
      "[Epoch 57 Batch 5000] loss 4.95, perplexity 140.90\n",
      "[Epoch 57 Batch 5500] loss 4.99, perplexity 146.23\n",
      "[Epoch 57] time cost 34.66s, validation loss 5.16, validation perplexity 174.18\n",
      "[Epoch 58 Batch 500] loss 5.01, perplexity 149.49\n",
      "[Epoch 58 Batch 1000] loss 4.96, perplexity 142.50\n",
      "[Epoch 58 Batch 1500] loss 4.94, perplexity 139.91\n",
      "[Epoch 58 Batch 2000] loss 5.01, perplexity 149.81\n",
      "[Epoch 58 Batch 2500] loss 4.99, perplexity 146.30\n",
      "[Epoch 58 Batch 3000] loss 4.92, perplexity 137.67\n",
      "[Epoch 58 Batch 3500] loss 4.98, perplexity 146.04\n",
      "[Epoch 58 Batch 4000] loss 4.90, perplexity 134.53\n",
      "[Epoch 58 Batch 4500] loss 4.89, perplexity 132.44\n",
      "[Epoch 58 Batch 5000] loss 4.94, perplexity 140.21\n",
      "[Epoch 58 Batch 5500] loss 4.99, perplexity 146.45\n",
      "[Epoch 58] time cost 35.29s, validation loss 5.16, validation perplexity 173.58\n",
      "[Epoch 59 Batch 500] loss 5.00, perplexity 148.42\n",
      "[Epoch 59 Batch 1000] loss 4.97, perplexity 143.72\n",
      "[Epoch 59 Batch 1500] loss 4.94, perplexity 140.28\n",
      "[Epoch 59 Batch 2000] loss 5.01, perplexity 149.55\n",
      "[Epoch 59 Batch 2500] loss 4.98, perplexity 145.90\n",
      "[Epoch 59 Batch 3000] loss 4.92, perplexity 137.59\n",
      "[Epoch 59 Batch 3500] loss 4.97, perplexity 144.09\n",
      "[Epoch 59 Batch 4000] loss 4.90, perplexity 134.88\n",
      "[Epoch 59 Batch 4500] loss 4.89, perplexity 133.56\n",
      "[Epoch 59 Batch 5000] loss 4.95, perplexity 140.53\n",
      "[Epoch 59 Batch 5500] loss 4.99, perplexity 146.54\n",
      "[Epoch 59] time cost 35.46s, validation loss 5.16, validation perplexity 174.13\n",
      "[Epoch 60 Batch 500] loss 5.00, perplexity 149.05\n",
      "[Epoch 60 Batch 1000] loss 4.96, perplexity 141.90\n",
      "[Epoch 60 Batch 1500] loss 4.94, perplexity 139.27\n",
      "[Epoch 60 Batch 2000] loss 5.01, perplexity 149.77\n",
      "[Epoch 60 Batch 2500] loss 4.99, perplexity 146.53\n",
      "[Epoch 60 Batch 3000] loss 4.92, perplexity 137.64\n",
      "[Epoch 60 Batch 3500] loss 4.98, perplexity 144.88\n",
      "[Epoch 60 Batch 4000] loss 4.91, perplexity 135.45\n",
      "[Epoch 60 Batch 4500] loss 4.89, perplexity 132.62\n",
      "[Epoch 60 Batch 5000] loss 4.94, perplexity 140.42\n",
      "[Epoch 60 Batch 5500] loss 4.99, perplexity 146.96\n",
      "[Epoch 60] time cost 34.23s, validation loss 5.16, validation perplexity 174.16\n",
      "[Epoch 61 Batch 500] loss 5.00, perplexity 148.52\n",
      "[Epoch 61 Batch 1000] loss 4.96, perplexity 143.22\n",
      "[Epoch 61 Batch 1500] loss 4.94, perplexity 139.71\n",
      "[Epoch 61 Batch 2000] loss 5.00, perplexity 147.85\n",
      "[Epoch 61 Batch 2500] loss 4.99, perplexity 146.51\n",
      "[Epoch 61 Batch 3000] loss 4.93, perplexity 138.25\n",
      "[Epoch 61 Batch 3500] loss 4.97, perplexity 144.73\n",
      "[Epoch 61 Batch 4000] loss 4.90, perplexity 134.82\n",
      "[Epoch 61 Batch 4500] loss 4.89, perplexity 132.64\n",
      "[Epoch 61 Batch 5000] loss 4.94, perplexity 140.31\n",
      "[Epoch 61 Batch 5500] loss 4.98, perplexity 145.59\n",
      "[Epoch 61] time cost 34.56s, validation loss 5.16, validation perplexity 174.08\n",
      "[Epoch 62 Batch 500] loss 4.99, perplexity 147.39\n",
      "[Epoch 62 Batch 1000] loss 4.95, perplexity 141.74\n",
      "[Epoch 62 Batch 1500] loss 4.94, perplexity 139.76\n",
      "[Epoch 62 Batch 2000] loss 5.00, perplexity 147.97\n",
      "[Epoch 62 Batch 2500] loss 4.99, perplexity 146.48\n",
      "[Epoch 62 Batch 3000] loss 4.92, perplexity 137.32\n",
      "[Epoch 62 Batch 3500] loss 4.97, perplexity 143.75\n",
      "[Epoch 62 Batch 4000] loss 4.90, perplexity 133.86\n",
      "[Epoch 62 Batch 4500] loss 4.89, perplexity 132.66\n",
      "[Epoch 62 Batch 5000] loss 4.94, perplexity 139.41\n",
      "[Epoch 62 Batch 5500] loss 4.98, perplexity 145.53\n",
      "[Epoch 62] time cost 36.07s, validation loss 5.16, validation perplexity 173.30\n",
      "[Epoch 63 Batch 500] loss 4.99, perplexity 147.14\n",
      "[Epoch 63 Batch 1000] loss 4.95, perplexity 141.48\n",
      "[Epoch 63 Batch 1500] loss 4.93, perplexity 138.58\n",
      "[Epoch 63 Batch 2000] loss 5.00, perplexity 148.37\n",
      "[Epoch 63 Batch 2500] loss 4.98, perplexity 146.18\n",
      "[Epoch 63 Batch 3000] loss 4.92, perplexity 136.38\n",
      "[Epoch 63 Batch 3500] loss 4.97, perplexity 144.74\n",
      "[Epoch 63 Batch 4000] loss 4.90, perplexity 133.79\n",
      "[Epoch 63 Batch 4500] loss 4.89, perplexity 132.72\n",
      "[Epoch 63 Batch 5000] loss 4.94, perplexity 139.42\n",
      "[Epoch 63 Batch 5500] loss 4.98, perplexity 145.35\n",
      "[Epoch 63] time cost 36.17s, validation loss 5.16, validation perplexity 173.33\n",
      "[Epoch 64 Batch 500] loss 5.00, perplexity 148.02\n",
      "[Epoch 64 Batch 1000] loss 4.95, perplexity 141.13\n",
      "[Epoch 64 Batch 1500] loss 4.95, perplexity 140.64\n",
      "[Epoch 64 Batch 2000] loss 5.00, perplexity 148.05\n",
      "[Epoch 64 Batch 2500] loss 4.98, perplexity 144.76\n",
      "[Epoch 64 Batch 3000] loss 4.92, perplexity 136.77\n",
      "[Epoch 64 Batch 3500] loss 4.97, perplexity 143.71\n",
      "[Epoch 64 Batch 4000] loss 4.90, perplexity 134.02\n",
      "[Epoch 64 Batch 4500] loss 4.89, perplexity 132.40\n",
      "[Epoch 64 Batch 5000] loss 4.93, perplexity 139.04\n",
      "[Epoch 64 Batch 5500] loss 4.98, perplexity 145.53\n",
      "[Epoch 64] time cost 36.64s, validation loss 5.17, validation perplexity 175.50\n",
      "[Epoch 65 Batch 500] loss 4.99, perplexity 147.06\n",
      "[Epoch 65 Batch 1000] loss 4.96, perplexity 142.21\n",
      "[Epoch 65 Batch 1500] loss 4.94, perplexity 139.56\n",
      "[Epoch 65 Batch 2000] loss 5.00, perplexity 148.59\n",
      "[Epoch 65 Batch 2500] loss 4.98, perplexity 145.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 65 Batch 3000] loss 4.92, perplexity 136.87\n",
      "[Epoch 65 Batch 3500] loss 4.97, perplexity 144.35\n",
      "[Epoch 65 Batch 4000] loss 4.90, perplexity 133.87\n",
      "[Epoch 65 Batch 4500] loss 4.88, perplexity 132.05\n",
      "[Epoch 65 Batch 5000] loss 4.93, perplexity 138.73\n",
      "[Epoch 65 Batch 5500] loss 4.98, perplexity 146.13\n",
      "[Epoch 65] time cost 41.36s, validation loss 5.16, validation perplexity 174.84\n",
      "[Epoch 66 Batch 500] loss 5.00, perplexity 147.92\n",
      "[Epoch 66 Batch 1000] loss 4.95, perplexity 141.34\n",
      "[Epoch 66 Batch 1500] loss 4.94, perplexity 139.71\n",
      "[Epoch 66 Batch 2000] loss 5.00, perplexity 148.16\n",
      "[Epoch 66 Batch 2500] loss 4.98, perplexity 144.83\n",
      "[Epoch 66 Batch 3000] loss 4.91, perplexity 135.57\n",
      "[Epoch 66 Batch 3500] loss 4.96, perplexity 142.99\n",
      "[Epoch 66 Batch 4000] loss 4.90, perplexity 133.74\n",
      "[Epoch 66 Batch 4500] loss 4.89, perplexity 132.37\n",
      "[Epoch 66 Batch 5000] loss 4.93, perplexity 139.05\n",
      "[Epoch 66 Batch 5500] loss 4.97, perplexity 144.49\n",
      "[Epoch 66] time cost 35.68s, validation loss 5.16, validation perplexity 174.53\n",
      "[Epoch 67 Batch 500] loss 4.99, perplexity 146.78\n",
      "[Epoch 67 Batch 1000] loss 4.95, perplexity 141.62\n",
      "[Epoch 67 Batch 1500] loss 4.93, perplexity 138.95\n",
      "[Epoch 67 Batch 2000] loss 5.00, perplexity 147.91\n",
      "[Epoch 67 Batch 2500] loss 4.98, perplexity 145.35\n",
      "[Epoch 67 Batch 3000] loss 4.92, perplexity 136.42\n",
      "[Epoch 67 Batch 3500] loss 4.97, perplexity 143.40\n",
      "[Epoch 67 Batch 4000] loss 4.90, perplexity 133.81\n",
      "[Epoch 67 Batch 4500] loss 4.89, perplexity 132.79\n",
      "[Epoch 67 Batch 5000] loss 4.94, perplexity 139.73\n",
      "[Epoch 67 Batch 5500] loss 4.98, perplexity 145.16\n",
      "[Epoch 67] time cost 35.56s, validation loss 5.17, validation perplexity 176.43\n",
      "[Epoch 68 Batch 500] loss 4.99, perplexity 147.42\n",
      "[Epoch 68 Batch 1000] loss 4.95, perplexity 141.55\n",
      "[Epoch 68 Batch 1500] loss 4.94, perplexity 139.68\n",
      "[Epoch 68 Batch 2000] loss 5.00, perplexity 148.13\n",
      "[Epoch 68 Batch 2500] loss 4.98, perplexity 144.79\n",
      "[Epoch 68 Batch 3000] loss 4.91, perplexity 135.85\n",
      "[Epoch 68 Batch 3500] loss 4.96, perplexity 143.16\n",
      "[Epoch 68 Batch 4000] loss 4.90, perplexity 134.26\n",
      "[Epoch 68 Batch 4500] loss 4.88, perplexity 131.73\n",
      "[Epoch 68 Batch 5000] loss 4.93, perplexity 138.73\n",
      "[Epoch 68 Batch 5500] loss 4.98, perplexity 144.83\n",
      "[Epoch 68] time cost 35.17s, validation loss 5.17, validation perplexity 175.26\n",
      "[Epoch 69 Batch 500] loss 4.99, perplexity 147.20\n",
      "[Epoch 69 Batch 1000] loss 4.95, perplexity 140.73\n",
      "[Epoch 69 Batch 1500] loss 4.93, perplexity 138.38\n",
      "[Epoch 69 Batch 2000] loss 5.00, perplexity 148.47\n",
      "[Epoch 69 Batch 2500] loss 4.98, perplexity 144.97\n",
      "[Epoch 69 Batch 3000] loss 4.92, perplexity 136.51\n",
      "[Epoch 69 Batch 3500] loss 4.96, perplexity 142.84\n",
      "[Epoch 69 Batch 4000] loss 4.89, perplexity 133.14\n",
      "[Epoch 69 Batch 4500] loss 4.88, perplexity 131.67\n",
      "[Epoch 69 Batch 5000] loss 4.93, perplexity 138.50\n",
      "[Epoch 69 Batch 5500] loss 4.98, perplexity 145.05\n",
      "[Epoch 69] time cost 34.84s, validation loss 5.17, validation perplexity 175.56\n",
      "[Epoch 70 Batch 500] loss 4.99, perplexity 146.88\n",
      "[Epoch 70 Batch 1000] loss 4.95, perplexity 140.99\n",
      "[Epoch 70 Batch 1500] loss 4.93, perplexity 138.66\n",
      "[Epoch 70 Batch 2000] loss 5.00, perplexity 149.05\n",
      "[Epoch 70 Batch 2500] loss 4.98, perplexity 145.47\n",
      "[Epoch 70 Batch 3000] loss 4.91, perplexity 135.96\n",
      "[Epoch 70 Batch 3500] loss 4.96, perplexity 143.00\n",
      "[Epoch 70 Batch 4000] loss 4.90, perplexity 133.64\n",
      "[Epoch 70 Batch 4500] loss 4.88, perplexity 131.30\n",
      "[Epoch 70 Batch 5000] loss 4.93, perplexity 138.86\n",
      "[Epoch 70 Batch 5500] loss 4.97, perplexity 143.91\n",
      "[Epoch 70] time cost 32.98s, validation loss 5.17, validation perplexity 175.04\n",
      "[Epoch 71 Batch 500] loss 4.99, perplexity 147.11\n",
      "[Epoch 71 Batch 1000] loss 4.94, perplexity 140.11\n",
      "[Epoch 71 Batch 1500] loss 4.93, perplexity 138.50\n",
      "[Epoch 71 Batch 2000] loss 5.00, perplexity 148.08\n",
      "[Epoch 71 Batch 2500] loss 4.97, perplexity 144.70\n",
      "[Epoch 71 Batch 3000] loss 4.91, perplexity 136.12\n",
      "[Epoch 71 Batch 3500] loss 4.96, perplexity 142.86\n",
      "[Epoch 71 Batch 4000] loss 4.89, perplexity 132.89\n",
      "[Epoch 71 Batch 4500] loss 4.88, perplexity 131.55\n",
      "[Epoch 71 Batch 5000] loss 4.93, perplexity 138.52\n",
      "[Epoch 71 Batch 5500] loss 4.97, perplexity 144.03\n",
      "[Epoch 71] time cost 32.89s, validation loss 5.19, validation perplexity 179.06\n",
      "[Epoch 72 Batch 500] loss 4.98, perplexity 146.14\n",
      "[Epoch 72 Batch 1000] loss 4.95, perplexity 140.70\n",
      "[Epoch 72 Batch 1500] loss 4.93, perplexity 138.36\n",
      "[Epoch 72 Batch 2000] loss 5.00, perplexity 148.38\n",
      "[Epoch 72 Batch 2500] loss 4.97, perplexity 144.11\n",
      "[Epoch 72 Batch 3000] loss 4.90, perplexity 134.89\n",
      "[Epoch 72 Batch 3500] loss 4.96, perplexity 142.56\n",
      "[Epoch 72 Batch 4000] loss 4.89, perplexity 132.61\n",
      "[Epoch 72 Batch 4500] loss 4.87, perplexity 130.49\n",
      "[Epoch 72 Batch 5000] loss 4.93, perplexity 138.55\n",
      "[Epoch 72 Batch 5500] loss 4.97, perplexity 144.72\n",
      "[Epoch 72] time cost 34.77s, validation loss 5.16, validation perplexity 174.90\n",
      "[Epoch 73 Batch 500] loss 4.99, perplexity 147.14\n",
      "[Epoch 73 Batch 1000] loss 4.95, perplexity 140.59\n",
      "[Epoch 73 Batch 1500] loss 4.93, perplexity 138.42\n",
      "[Epoch 73 Batch 2000] loss 5.00, perplexity 148.00\n",
      "[Epoch 73 Batch 2500] loss 4.97, perplexity 144.53\n",
      "[Epoch 73 Batch 3000] loss 4.92, perplexity 136.41\n",
      "[Epoch 73 Batch 3500] loss 4.96, perplexity 142.96\n",
      "[Epoch 73 Batch 4000] loss 4.89, perplexity 133.13\n",
      "[Epoch 73 Batch 4500] loss 4.88, perplexity 131.06\n",
      "[Epoch 73 Batch 5000] loss 4.93, perplexity 138.57\n",
      "[Epoch 73 Batch 5500] loss 4.97, perplexity 144.23\n",
      "[Epoch 73] time cost 39.46s, validation loss 5.17, validation perplexity 176.32\n",
      "[Epoch 74 Batch 500] loss 4.98, perplexity 145.99\n",
      "[Epoch 74 Batch 1000] loss 4.95, perplexity 141.83\n",
      "[Epoch 74 Batch 1500] loss 4.93, perplexity 138.51\n",
      "[Epoch 74 Batch 2000] loss 5.00, perplexity 147.92\n",
      "[Epoch 74 Batch 2500] loss 4.97, perplexity 143.98\n",
      "[Epoch 74 Batch 3000] loss 4.91, perplexity 135.69\n",
      "[Epoch 74 Batch 3500] loss 4.97, perplexity 143.61\n",
      "[Epoch 74 Batch 4000] loss 4.89, perplexity 132.45\n",
      "[Epoch 74 Batch 4500] loss 4.87, perplexity 129.87\n",
      "[Epoch 74 Batch 5000] loss 4.93, perplexity 138.01\n",
      "[Epoch 74 Batch 5500] loss 4.97, perplexity 144.12\n",
      "[Epoch 74] time cost 37.30s, validation loss 5.17, validation perplexity 176.43\n",
      "[Epoch 75 Batch 500] loss 4.99, perplexity 146.46\n",
      "[Epoch 75 Batch 1000] loss 4.95, perplexity 140.98\n",
      "[Epoch 75 Batch 1500] loss 4.93, perplexity 138.74\n",
      "[Epoch 75 Batch 2000] loss 4.99, perplexity 147.66\n",
      "[Epoch 75 Batch 2500] loss 4.98, perplexity 145.71\n",
      "[Epoch 75 Batch 3000] loss 4.91, perplexity 135.58\n",
      "[Epoch 75 Batch 3500] loss 4.96, perplexity 142.37\n",
      "[Epoch 75 Batch 4000] loss 4.89, perplexity 132.52\n",
      "[Epoch 75 Batch 4500] loss 4.88, perplexity 131.43\n",
      "[Epoch 75 Batch 5000] loss 4.93, perplexity 138.34\n",
      "[Epoch 75 Batch 5500] loss 4.97, perplexity 144.19\n",
      "[Epoch 75] time cost 35.25s, validation loss 5.17, validation perplexity 175.43\n",
      "[Epoch 76 Batch 500] loss 4.99, perplexity 146.62\n",
      "[Epoch 76 Batch 1000] loss 4.95, perplexity 140.64\n",
      "[Epoch 76 Batch 1500] loss 4.93, perplexity 138.75\n",
      "[Epoch 76 Batch 2000] loss 4.99, perplexity 146.79\n",
      "[Epoch 76 Batch 2500] loss 4.97, perplexity 144.36\n",
      "[Epoch 76 Batch 3000] loss 4.91, perplexity 135.64\n",
      "[Epoch 76 Batch 3500] loss 4.96, perplexity 142.06\n",
      "[Epoch 76 Batch 4000] loss 4.89, perplexity 132.97\n",
      "[Epoch 76 Batch 4500] loss 4.87, perplexity 130.61\n",
      "[Epoch 76 Batch 5000] loss 4.93, perplexity 138.25\n",
      "[Epoch 76 Batch 5500] loss 4.97, perplexity 144.73\n",
      "[Epoch 76] time cost 35.80s, validation loss 5.16, validation perplexity 174.85\n",
      "[Epoch 77 Batch 500] loss 4.99, perplexity 146.30\n",
      "[Epoch 77 Batch 1000] loss 4.94, perplexity 140.41\n",
      "[Epoch 77 Batch 1500] loss 4.93, perplexity 138.56\n",
      "[Epoch 77 Batch 2000] loss 4.99, perplexity 147.46\n",
      "[Epoch 77 Batch 2500] loss 4.97, perplexity 143.80\n",
      "[Epoch 77 Batch 3000] loss 4.90, perplexity 134.91\n",
      "[Epoch 77 Batch 3500] loss 4.96, perplexity 141.97\n",
      "[Epoch 77 Batch 4000] loss 4.89, perplexity 132.82\n",
      "[Epoch 77 Batch 4500] loss 4.88, perplexity 131.30\n",
      "[Epoch 77 Batch 5000] loss 4.92, perplexity 137.26\n",
      "[Epoch 77 Batch 5500] loss 4.97, perplexity 143.80\n",
      "[Epoch 77] time cost 36.23s, validation loss 5.18, validation perplexity 177.76\n",
      "[Epoch 78 Batch 500] loss 4.99, perplexity 146.68\n",
      "[Epoch 78 Batch 1000] loss 4.95, perplexity 140.48\n",
      "[Epoch 78 Batch 1500] loss 4.93, perplexity 138.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 78 Batch 2000] loss 4.99, perplexity 147.02\n",
      "[Epoch 78 Batch 2500] loss 4.97, perplexity 144.17\n",
      "[Epoch 78 Batch 3000] loss 4.91, perplexity 135.14\n",
      "[Epoch 78 Batch 3500] loss 4.96, perplexity 142.09\n",
      "[Epoch 78 Batch 4000] loss 4.89, perplexity 132.71\n",
      "[Epoch 78 Batch 4500] loss 4.88, perplexity 131.40\n",
      "[Epoch 78 Batch 5000] loss 4.92, perplexity 137.69\n",
      "[Epoch 78 Batch 5500] loss 4.97, perplexity 144.59\n",
      "[Epoch 78] time cost 37.09s, validation loss 5.17, validation perplexity 176.27\n",
      "[Epoch 79 Batch 500] loss 4.98, perplexity 146.03\n",
      "[Epoch 79 Batch 1000] loss 4.94, perplexity 139.92\n",
      "[Epoch 79 Batch 1500] loss 4.92, perplexity 137.63\n",
      "[Epoch 79 Batch 2000] loss 4.99, perplexity 146.66\n",
      "[Epoch 79 Batch 2500] loss 4.97, perplexity 143.85\n",
      "[Epoch 79 Batch 3000] loss 4.91, perplexity 135.52\n",
      "[Epoch 79 Batch 3500] loss 4.95, perplexity 141.85\n",
      "[Epoch 79 Batch 4000] loss 4.89, perplexity 132.41\n",
      "[Epoch 79 Batch 4500] loss 4.87, perplexity 130.77\n",
      "[Epoch 79 Batch 5000] loss 4.93, perplexity 138.25\n",
      "[Epoch 79 Batch 5500] loss 4.97, perplexity 143.50\n",
      "[Epoch 79] time cost 35.18s, validation loss 5.16, validation perplexity 174.94\n",
      "[Epoch 80 Batch 500] loss 4.98, perplexity 145.04\n",
      "[Epoch 80 Batch 1000] loss 4.94, perplexity 139.41\n",
      "[Epoch 80 Batch 1500] loss 4.92, perplexity 137.13\n",
      "[Epoch 80 Batch 2000] loss 4.99, perplexity 146.51\n",
      "[Epoch 80 Batch 2500] loss 4.97, perplexity 143.58\n",
      "[Epoch 80 Batch 3000] loss 4.91, perplexity 135.31\n",
      "[Epoch 80 Batch 3500] loss 4.96, perplexity 142.32\n",
      "[Epoch 80 Batch 4000] loss 4.88, perplexity 131.48\n",
      "[Epoch 80 Batch 4500] loss 4.87, perplexity 130.28\n",
      "[Epoch 80 Batch 5000] loss 4.92, perplexity 137.69\n",
      "[Epoch 80 Batch 5500] loss 4.97, perplexity 143.57\n",
      "[Epoch 80] time cost 36.74s, validation loss 5.17, validation perplexity 176.70\n",
      "[Epoch 81 Batch 500] loss 4.98, perplexity 145.23\n",
      "[Epoch 81 Batch 1000] loss 4.94, perplexity 139.86\n",
      "[Epoch 81 Batch 1500] loss 4.92, perplexity 137.44\n",
      "[Epoch 81 Batch 2000] loss 4.99, perplexity 147.03\n",
      "[Epoch 81 Batch 2500] loss 4.97, perplexity 144.01\n",
      "[Epoch 81 Batch 3000] loss 4.90, perplexity 134.51\n",
      "[Epoch 81 Batch 3500] loss 4.95, perplexity 141.23\n",
      "[Epoch 81 Batch 4000] loss 4.88, perplexity 132.24\n",
      "[Epoch 81 Batch 4500] loss 4.87, perplexity 129.97\n",
      "[Epoch 81 Batch 5000] loss 4.92, perplexity 137.58\n",
      "[Epoch 81 Batch 5500] loss 4.97, perplexity 143.86\n",
      "[Epoch 81] time cost 39.03s, validation loss 5.17, validation perplexity 176.75\n",
      "[Epoch 82 Batch 500] loss 4.98, perplexity 145.56\n",
      "[Epoch 82 Batch 1000] loss 4.94, perplexity 139.90\n",
      "[Epoch 82 Batch 1500] loss 4.92, perplexity 137.30\n",
      "[Epoch 82 Batch 2000] loss 4.99, perplexity 146.72\n",
      "[Epoch 82 Batch 2500] loss 4.97, perplexity 144.08\n",
      "[Epoch 82 Batch 3000] loss 4.91, perplexity 135.17\n",
      "[Epoch 82 Batch 3500] loss 4.96, perplexity 142.03\n",
      "[Epoch 82 Batch 4000] loss 4.88, perplexity 131.92\n",
      "[Epoch 82 Batch 4500] loss 4.87, perplexity 130.78\n",
      "[Epoch 82 Batch 5000] loss 4.92, perplexity 137.13\n",
      "[Epoch 82 Batch 5500] loss 4.96, perplexity 143.28\n",
      "[Epoch 82] time cost 39.99s, validation loss 5.16, validation perplexity 174.70\n",
      "[Epoch 83 Batch 500] loss 4.98, perplexity 145.64\n",
      "[Epoch 83 Batch 1000] loss 4.94, perplexity 139.99\n",
      "[Epoch 83 Batch 1500] loss 4.93, perplexity 138.37\n",
      "[Epoch 83 Batch 2000] loss 4.99, perplexity 146.32\n",
      "[Epoch 83 Batch 2500] loss 4.97, perplexity 144.09\n",
      "[Epoch 83 Batch 3000] loss 4.90, perplexity 134.28\n",
      "[Epoch 83 Batch 3500] loss 4.96, perplexity 142.50\n",
      "[Epoch 83 Batch 4000] loss 4.88, perplexity 132.27\n",
      "[Epoch 83 Batch 4500] loss 4.87, perplexity 130.86\n",
      "[Epoch 83 Batch 5000] loss 4.92, perplexity 137.24\n",
      "[Epoch 83 Batch 5500] loss 4.97, perplexity 143.88\n",
      "[Epoch 83] time cost 41.42s, validation loss 5.17, validation perplexity 175.58\n",
      "[Epoch 84 Batch 500] loss 4.98, perplexity 145.45\n",
      "[Epoch 84 Batch 1000] loss 4.94, perplexity 140.42\n",
      "[Epoch 84 Batch 1500] loss 4.93, perplexity 138.13\n",
      "[Epoch 84 Batch 2000] loss 4.98, perplexity 145.81\n",
      "[Epoch 84 Batch 2500] loss 4.97, perplexity 143.63\n",
      "[Epoch 84 Batch 3000] loss 4.90, perplexity 134.43\n",
      "[Epoch 84 Batch 3500] loss 4.95, perplexity 141.85\n",
      "[Epoch 84 Batch 4000] loss 4.88, perplexity 132.28\n",
      "[Epoch 84 Batch 4500] loss 4.87, perplexity 129.81\n",
      "[Epoch 84 Batch 5000] loss 4.92, perplexity 137.52\n",
      "[Epoch 84 Batch 5500] loss 4.97, perplexity 144.07\n",
      "[Epoch 84] time cost 44.49s, validation loss 5.17, validation perplexity 176.39\n",
      "[Epoch 85 Batch 500] loss 4.98, perplexity 146.09\n",
      "[Epoch 85 Batch 1000] loss 4.94, perplexity 139.12\n",
      "[Epoch 85 Batch 1500] loss 4.92, perplexity 137.34\n",
      "[Epoch 85 Batch 2000] loss 4.99, perplexity 146.27\n",
      "[Epoch 85 Batch 2500] loss 4.97, perplexity 143.67\n",
      "[Epoch 85 Batch 3000] loss 4.91, perplexity 135.05\n",
      "[Epoch 85 Batch 3500] loss 4.96, perplexity 142.04\n",
      "[Epoch 85 Batch 4000] loss 4.88, perplexity 132.03\n",
      "[Epoch 85 Batch 4500] loss 4.87, perplexity 130.46\n",
      "[Epoch 85 Batch 5000] loss 4.93, perplexity 138.67\n",
      "[Epoch 85 Batch 5500] loss 4.97, perplexity 143.49\n",
      "[Epoch 85] time cost 39.15s, validation loss 5.18, validation perplexity 177.87\n",
      "[Epoch 86 Batch 500] loss 4.98, perplexity 146.05\n",
      "[Epoch 86 Batch 1000] loss 4.94, perplexity 139.20\n",
      "[Epoch 86 Batch 1500] loss 4.93, perplexity 138.16\n",
      "[Epoch 86 Batch 2000] loss 4.98, perplexity 145.98\n",
      "[Epoch 86 Batch 2500] loss 4.97, perplexity 143.95\n",
      "[Epoch 86 Batch 3000] loss 4.90, perplexity 134.81\n",
      "[Epoch 86 Batch 3500] loss 4.95, perplexity 141.87\n",
      "[Epoch 86 Batch 4000] loss 4.88, perplexity 131.43\n",
      "[Epoch 86 Batch 4500] loss 4.87, perplexity 129.92\n",
      "[Epoch 86 Batch 5000] loss 4.93, perplexity 138.16\n",
      "[Epoch 86 Batch 5500] loss 4.97, perplexity 143.37\n",
      "[Epoch 86] time cost 37.12s, validation loss 5.18, validation perplexity 178.03\n",
      "[Epoch 87 Batch 500] loss 4.98, perplexity 145.47\n",
      "[Epoch 87 Batch 1000] loss 4.94, perplexity 139.22\n",
      "[Epoch 87 Batch 1500] loss 4.92, perplexity 137.32\n",
      "[Epoch 87 Batch 2000] loss 4.98, perplexity 145.72\n",
      "[Epoch 87 Batch 2500] loss 4.96, perplexity 143.02\n",
      "[Epoch 87 Batch 3000] loss 4.90, perplexity 134.17\n",
      "[Epoch 87 Batch 3500] loss 4.96, perplexity 142.80\n",
      "[Epoch 87 Batch 4000] loss 4.89, perplexity 132.78\n",
      "[Epoch 87 Batch 4500] loss 4.87, perplexity 130.24\n",
      "[Epoch 87 Batch 5000] loss 4.92, perplexity 136.49\n",
      "[Epoch 87 Batch 5500] loss 4.96, perplexity 143.26\n",
      "[Epoch 87] time cost 37.26s, validation loss 5.17, validation perplexity 175.37\n",
      "[Epoch 88 Batch 500] loss 4.98, perplexity 145.32\n",
      "[Epoch 88 Batch 1000] loss 4.93, perplexity 138.72\n",
      "[Epoch 88 Batch 1500] loss 4.92, perplexity 136.97\n",
      "[Epoch 88 Batch 2000] loss 4.98, perplexity 145.42\n",
      "[Epoch 88 Batch 2500] loss 4.96, perplexity 143.23\n",
      "[Epoch 88 Batch 3000] loss 4.90, perplexity 133.78\n",
      "[Epoch 88 Batch 3500] loss 4.95, perplexity 141.24\n",
      "[Epoch 88 Batch 4000] loss 4.87, perplexity 130.94\n",
      "[Epoch 88 Batch 4500] loss 4.87, perplexity 130.18\n",
      "[Epoch 88 Batch 5000] loss 4.92, perplexity 136.75\n",
      "[Epoch 88 Batch 5500] loss 4.96, perplexity 143.21\n",
      "[Epoch 88] time cost 36.70s, validation loss 5.17, validation perplexity 176.79\n",
      "[Epoch 89 Batch 500] loss 4.98, perplexity 144.76\n",
      "[Epoch 89 Batch 1000] loss 4.94, perplexity 139.72\n",
      "[Epoch 89 Batch 1500] loss 4.92, perplexity 137.18\n",
      "[Epoch 89 Batch 2000] loss 4.98, perplexity 146.05\n",
      "[Epoch 89 Batch 2500] loss 4.96, perplexity 142.68\n",
      "[Epoch 89 Batch 3000] loss 4.90, perplexity 134.11\n",
      "[Epoch 89 Batch 3500] loss 4.95, perplexity 141.25\n",
      "[Epoch 89 Batch 4000] loss 4.87, perplexity 130.71\n",
      "[Epoch 89 Batch 4500] loss 4.87, perplexity 130.30\n",
      "[Epoch 89 Batch 5000] loss 4.92, perplexity 137.43\n",
      "[Epoch 89 Batch 5500] loss 4.97, perplexity 143.79\n",
      "[Epoch 89] time cost 37.51s, validation loss 5.17, validation perplexity 175.57\n",
      "[Epoch 90 Batch 500] loss 4.97, perplexity 144.55\n",
      "[Epoch 90 Batch 1000] loss 4.93, perplexity 138.91\n",
      "[Epoch 90 Batch 1500] loss 4.92, perplexity 136.89\n",
      "[Epoch 90 Batch 2000] loss 4.98, perplexity 145.82\n",
      "[Epoch 90 Batch 2500] loss 4.96, perplexity 142.77\n",
      "[Epoch 90 Batch 3000] loss 4.90, perplexity 134.82\n",
      "[Epoch 90 Batch 3500] loss 4.95, perplexity 141.69\n",
      "[Epoch 90 Batch 4000] loss 4.88, perplexity 131.65\n",
      "[Epoch 90 Batch 4500] loss 4.86, perplexity 129.50\n",
      "[Epoch 90 Batch 5000] loss 4.92, perplexity 137.36\n",
      "[Epoch 90 Batch 5500] loss 4.96, perplexity 142.45\n",
      "[Epoch 90] time cost 36.35s, validation loss 5.17, validation perplexity 176.41\n",
      "[Epoch 91 Batch 500] loss 4.97, perplexity 144.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 91 Batch 1000] loss 4.94, perplexity 139.31\n",
      "[Epoch 91 Batch 1500] loss 4.92, perplexity 137.64\n",
      "[Epoch 91 Batch 2000] loss 4.98, perplexity 145.77\n",
      "[Epoch 91 Batch 2500] loss 4.96, perplexity 143.26\n",
      "[Epoch 91 Batch 3000] loss 4.90, perplexity 134.54\n",
      "[Epoch 91 Batch 3500] loss 4.95, perplexity 140.61\n",
      "[Epoch 91 Batch 4000] loss 4.88, perplexity 131.95\n",
      "[Epoch 91 Batch 4500] loss 4.86, perplexity 129.36\n",
      "[Epoch 91 Batch 5000] loss 4.92, perplexity 136.57\n",
      "[Epoch 91 Batch 5500] loss 4.96, perplexity 142.28\n",
      "[Epoch 91] time cost 34.43s, validation loss 5.17, validation perplexity 175.58\n",
      "[Epoch 92 Batch 500] loss 4.98, perplexity 145.39\n",
      "[Epoch 92 Batch 1000] loss 4.93, perplexity 138.87\n",
      "[Epoch 92 Batch 1500] loss 4.92, perplexity 137.37\n",
      "[Epoch 92 Batch 2000] loss 4.99, perplexity 146.25\n",
      "[Epoch 92 Batch 2500] loss 4.97, perplexity 144.38\n",
      "[Epoch 92 Batch 3000] loss 4.90, perplexity 134.16\n",
      "[Epoch 92 Batch 3500] loss 4.95, perplexity 141.51\n",
      "[Epoch 92 Batch 4000] loss 4.88, perplexity 131.11\n",
      "[Epoch 92 Batch 4500] loss 4.86, perplexity 129.12\n",
      "[Epoch 92 Batch 5000] loss 4.92, perplexity 137.13\n",
      "[Epoch 92 Batch 5500] loss 4.96, perplexity 141.98\n",
      "[Epoch 92] time cost 33.50s, validation loss 5.17, validation perplexity 176.32\n",
      "[Epoch 93 Batch 500] loss 4.98, perplexity 144.93\n",
      "[Epoch 93 Batch 1000] loss 4.93, perplexity 139.04\n",
      "[Epoch 93 Batch 1500] loss 4.92, perplexity 136.51\n",
      "[Epoch 93 Batch 2000] loss 4.98, perplexity 144.88\n",
      "[Epoch 93 Batch 2500] loss 4.96, perplexity 143.18\n",
      "[Epoch 93 Batch 3000] loss 4.90, perplexity 134.18\n",
      "[Epoch 93 Batch 3500] loss 4.95, perplexity 141.42\n",
      "[Epoch 93 Batch 4000] loss 4.88, perplexity 131.66\n",
      "[Epoch 93 Batch 4500] loss 4.87, perplexity 130.14\n",
      "[Epoch 93 Batch 5000] loss 4.92, perplexity 136.61\n",
      "[Epoch 93 Batch 5500] loss 4.96, perplexity 142.34\n",
      "[Epoch 93] time cost 36.13s, validation loss 5.18, validation perplexity 178.14\n",
      "[Epoch 94 Batch 500] loss 4.98, perplexity 145.25\n",
      "[Epoch 94 Batch 1000] loss 4.93, perplexity 138.84\n",
      "[Epoch 94 Batch 1500] loss 4.92, perplexity 137.67\n",
      "[Epoch 94 Batch 2000] loss 4.98, perplexity 145.83\n",
      "[Epoch 94 Batch 2500] loss 4.96, perplexity 143.03\n",
      "[Epoch 94 Batch 3000] loss 4.90, perplexity 134.24\n",
      "[Epoch 94 Batch 3500] loss 4.95, perplexity 141.02\n",
      "[Epoch 94 Batch 4000] loss 4.88, perplexity 131.67\n",
      "[Epoch 94 Batch 4500] loss 4.86, perplexity 129.54\n",
      "[Epoch 94 Batch 5000] loss 4.92, perplexity 136.57\n",
      "[Epoch 94 Batch 5500] loss 4.96, perplexity 142.84\n",
      "[Epoch 94] time cost 35.65s, validation loss 5.17, validation perplexity 175.68\n",
      "[Epoch 95 Batch 500] loss 4.97, perplexity 144.71\n",
      "[Epoch 95 Batch 1000] loss 4.93, perplexity 138.40\n",
      "[Epoch 95 Batch 1500] loss 4.92, perplexity 136.57\n",
      "[Epoch 95 Batch 2000] loss 4.98, perplexity 146.09\n",
      "[Epoch 95 Batch 2500] loss 4.96, perplexity 142.11\n",
      "[Epoch 95 Batch 3000] loss 4.90, perplexity 134.05\n",
      "[Epoch 95 Batch 3500] loss 4.95, perplexity 141.08\n",
      "[Epoch 95 Batch 4000] loss 4.87, perplexity 130.68\n",
      "[Epoch 95 Batch 4500] loss 4.86, perplexity 129.33\n",
      "[Epoch 95 Batch 5000] loss 4.92, perplexity 136.35\n",
      "[Epoch 95 Batch 5500] loss 4.96, perplexity 143.20\n",
      "[Epoch 95] time cost 36.15s, validation loss 5.17, validation perplexity 176.32\n",
      "[Epoch 96 Batch 500] loss 4.97, perplexity 144.28\n",
      "[Epoch 96 Batch 1000] loss 4.93, perplexity 137.84\n",
      "[Epoch 96 Batch 1500] loss 4.91, perplexity 136.23\n",
      "[Epoch 96 Batch 2000] loss 4.98, perplexity 145.33\n",
      "[Epoch 96 Batch 2500] loss 4.96, perplexity 142.24\n",
      "[Epoch 96 Batch 3000] loss 4.89, perplexity 133.36\n",
      "[Epoch 96 Batch 3500] loss 4.95, perplexity 141.30\n",
      "[Epoch 96 Batch 4000] loss 4.88, perplexity 131.36\n",
      "[Epoch 96 Batch 4500] loss 4.87, perplexity 129.91\n",
      "[Epoch 96 Batch 5000] loss 4.92, perplexity 136.94\n",
      "[Epoch 96 Batch 5500] loss 4.96, perplexity 142.33\n",
      "[Epoch 96] time cost 37.91s, validation loss 5.18, validation perplexity 177.75\n",
      "[Epoch 97 Batch 500] loss 4.97, perplexity 144.52\n",
      "[Epoch 97 Batch 1000] loss 4.93, perplexity 137.76\n",
      "[Epoch 97 Batch 1500] loss 4.92, perplexity 137.30\n",
      "[Epoch 97 Batch 2000] loss 4.98, perplexity 145.93\n",
      "[Epoch 97 Batch 2500] loss 4.97, perplexity 143.40\n",
      "[Epoch 97 Batch 3000] loss 4.90, perplexity 134.89\n",
      "[Epoch 97 Batch 3500] loss 4.95, perplexity 140.84\n",
      "[Epoch 97 Batch 4000] loss 4.88, perplexity 131.40\n",
      "[Epoch 97 Batch 4500] loss 4.86, perplexity 129.42\n",
      "[Epoch 97 Batch 5000] loss 4.92, perplexity 136.46\n",
      "[Epoch 97 Batch 5500] loss 4.96, perplexity 142.15\n",
      "[Epoch 97] time cost 36.10s, validation loss 5.17, validation perplexity 175.07\n",
      "[Epoch 98 Batch 500] loss 4.97, perplexity 143.88\n",
      "[Epoch 98 Batch 1000] loss 4.93, perplexity 139.04\n",
      "[Epoch 98 Batch 1500] loss 4.92, perplexity 136.83\n",
      "[Epoch 98 Batch 2000] loss 4.97, perplexity 144.52\n",
      "[Epoch 98 Batch 2500] loss 4.96, perplexity 142.52\n",
      "[Epoch 98 Batch 3000] loss 4.89, perplexity 133.09\n",
      "[Epoch 98 Batch 3500] loss 4.94, perplexity 140.13\n",
      "[Epoch 98 Batch 4000] loss 4.87, perplexity 130.37\n",
      "[Epoch 98 Batch 4500] loss 4.87, perplexity 129.98\n",
      "[Epoch 98 Batch 5000] loss 4.92, perplexity 136.77\n",
      "[Epoch 98 Batch 5500] loss 4.96, perplexity 142.20\n",
      "[Epoch 98] time cost 36.82s, validation loss 5.18, validation perplexity 178.54\n",
      "[Epoch 99 Batch 500] loss 4.97, perplexity 144.74\n",
      "[Epoch 99 Batch 1000] loss 4.94, perplexity 139.35\n",
      "[Epoch 99 Batch 1500] loss 4.92, perplexity 137.41\n",
      "[Epoch 99 Batch 2000] loss 4.98, perplexity 145.47\n",
      "[Epoch 99 Batch 2500] loss 4.96, perplexity 142.80\n",
      "[Epoch 99 Batch 3000] loss 4.90, perplexity 133.74\n",
      "[Epoch 99 Batch 3500] loss 4.94, perplexity 139.98\n",
      "[Epoch 99 Batch 4000] loss 4.87, perplexity 130.44\n",
      "[Epoch 99 Batch 4500] loss 4.86, perplexity 129.15\n",
      "[Epoch 99 Batch 5000] loss 4.91, perplexity 135.97\n",
      "[Epoch 99 Batch 5500] loss 4.95, perplexity 141.56\n",
      "[Epoch 99] time cost 36.65s, validation loss 5.18, validation perplexity 178.15\n",
      "[Epoch 100 Batch 500] loss 4.97, perplexity 143.61\n",
      "[Epoch 100 Batch 1000] loss 4.93, perplexity 138.16\n",
      "[Epoch 100 Batch 1500] loss 4.92, perplexity 136.43\n",
      "[Epoch 100 Batch 2000] loss 4.97, perplexity 144.71\n",
      "[Epoch 100 Batch 2500] loss 4.96, perplexity 142.93\n",
      "[Epoch 100 Batch 3000] loss 4.89, perplexity 133.52\n",
      "[Epoch 100 Batch 3500] loss 4.95, perplexity 140.50\n",
      "[Epoch 100 Batch 4000] loss 4.87, perplexity 130.97\n",
      "[Epoch 100 Batch 4500] loss 4.86, perplexity 129.55\n",
      "[Epoch 100 Batch 5000] loss 4.92, perplexity 136.41\n",
      "[Epoch 100 Batch 5500] loss 4.95, perplexity 141.81\n",
      "[Epoch 100] time cost 36.56s, validation loss 5.18, validation perplexity 177.91\n",
      "Test loss 5.11, test perplexity 166.36\n"
     ]
    }
   ],
   "source": [
    "train()\n",
    "test_L = model_eval(test_data)\n",
    "print('Test loss %.2f, test perplexity %.2f' % (test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 结论\n",
    "\n",
    "* 我们可以使用Gluon轻松训练各种不同的循环神经网络，并设置网络参数，例如网络的层数。\n",
    "* 训练迭代中需要将隐含状态从计算图中分离，使模型参数梯度计算只依赖当前的时序数据批量采样。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 调调参数（例如epochs、隐含层的层数、序列长度、隐含状态长度和学习率），看看对运行时间、训练集、验证集和测试集上perplexity造成的影响。\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/4089)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
