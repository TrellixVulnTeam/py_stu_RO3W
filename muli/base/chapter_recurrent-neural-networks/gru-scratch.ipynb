{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 门控循环单元（GRU）--- 从0开始\n",
    "\n",
    "[上一节](bptt.md)中，我们介绍了循环神经网络中的梯度计算方法。我们发现，循环神经网络的隐含层变量梯度可能会出现衰减或爆炸。虽然[梯度裁剪](rnn-scratch.md)可以应对梯度爆炸，但无法解决梯度衰减的问题。因此，给定一个时间序列，例如文本序列，循环神经网络在实际中其实较难捕捉两个时刻距离较大的文本元素（字或词）之间的依赖关系。\n",
    "\n",
    "门控循环神经网络（gated recurrent neural networks）的提出，是为了更好地捕捉时序数据中间隔较大的依赖关系。其中，门控循环单元（gated recurrent unit，简称GRU）是一种常用的门控循环神经网络。它由Cho、van Merrienboer、 Bahdanau和Bengio在2014年被提出。\n",
    "\n",
    "\n",
    "## 门控循环单元\n",
    "\n",
    "我们先介绍门控循环单元的构造。它比循环神经网络中的隐含层构造稍复杂一点。\n",
    "\n",
    "### 重置门和更新门\n",
    "\n",
    "门控循环单元的隐含状态只包含隐含层变量$\\mathbf{H}$。假定隐含状态长度为$h$，给定时刻$t$的一个样本数为$n$特征向量维度为$x$的批量数据$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$和上一时刻隐含状态$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$，重置门（reset gate）$\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$和更新门（update gate）$\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$的定义如下：\n",
    "\n",
    "$$\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r)$$\n",
    "\n",
    "$$\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z)$$\n",
    "\n",
    "其中的$\\mathbf{W}_{xr}, \\mathbf{W}_{xz} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hr}, \\mathbf{W}_{hz} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_r, \\mathbf{b}_z \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。函数$\\sigma$自变量中的三项相加使用了[广播](../chapter_crashcourse/ndarray.md)。\n",
    "\n",
    "需要注意的是，重置门和更新门使用了值域为$[0, 1]$的函数$\\sigma(x) = 1/(1+\\text{exp}(-x))$。因此，重置门$\\mathbf{R}_t$和更新门$\\mathbf{Z}_t$中每个元素的值域都是$[0, 1]$。\n",
    "\n",
    "\n",
    "### 候选隐含状态\n",
    "\n",
    "我们可以通过元素值域在$[0, 1]$的更新门和重置门来控制隐含状态中信息的流动：这通常可以应用按元素乘法符$\\odot$。门控循环单元中的候选隐含状态$\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$使用了值域在$[-1, 1]$的双曲正切函数tanh做激活函数：\n",
    "\n",
    "$$\\tilde{\\mathbf{H}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{R}_t \\odot \\mathbf{H}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h)$$\n",
    "\n",
    "其中的$\\mathbf{W}_{xh} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。\n",
    "\n",
    "需要注意的是，候选隐含状态使用了重置门来控制包含过去时刻信息的上一个隐含状态的流入。如果重置门近似0，上一个隐含状态将被丢弃。因此，重置门提供了丢弃与未来无关的过去隐含状态的机制。\n",
    "\n",
    "\n",
    "### 隐含状态\n",
    "\n",
    "隐含状态$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$的计算使用更新门$\\mathbf{Z}_t$来对上一时刻的隐含状态$\\mathbf{H}_{t-1}$和当前时刻的候选隐含状态$\\tilde{\\mathbf{H}}_t$做组合，公式如下：\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t$$\n",
    "\n",
    "需要注意的是，更新门可以控制过去的隐含状态在当前时刻的重要性。如果更新门一直近似1，过去的隐含状态将一直通过时间保存并传递至当前时刻。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时序数据中间隔较大的依赖关系。\n",
    "\n",
    "我们对门控循环单元的设计稍作总结：\n",
    "\n",
    "* 重置门有助于捕捉时序数据中短期的依赖关系。\n",
    "* 更新门有助于捕捉时序数据中长期的依赖关系。\n",
    "\n",
    "\n",
    "输出层的设计可参照[循环神经网络](rnn-scratch.md)中的描述。\n",
    "\n",
    "\n",
    "## 实验\n",
    "\n",
    "\n",
    "为了实现并展示门控循环单元，我们依然使用周杰伦歌词数据集来训练模型作词。这里除门控循环单元以外的实现已在[循环神经网络](rnn-scratch.md)中介绍。\n",
    "\n",
    "\n",
    "### 数据处理\n",
    "\n",
    "我们先读取并对数据集做简单处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 1465\n"
     ]
    }
   ],
   "source": [
    "import zipfile \n",
    "with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip','r') as zin:\n",
    "    zin.extractall('../data/') \n",
    "\n",
    "with open('../data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars=f.read() \n",
    "    \n",
    "    \n",
    "corpus_chars=corpus_chars.replace('\\n',' ').replace('\\r',' ') \n",
    "corpus_chars=corpus_chars[0:20000]\n",
    "\n",
    "idx_to_char=list(set(corpus_chars)) \n",
    "char_to_idx=dict([(char,i) for i,char in enumerate(idx_to_char)]) \n",
    "corpus_indices=[char_to_idx[char] for char in corpus_chars] \n",
    "\n",
    "vocab_size=len(char_to_idx)\n",
    "print(\"vocab size:\",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用onehot 来将字符索引表时成向量。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(data):\n",
    "    return [nd.one_hot(X,vocab_size) for X in data.T] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数 \n",
    "以下部分对模型参数就行初始化。参数hidden_dim定义了隐含状态的长度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaa\n",
      "Will use  gpu(0)\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx \n",
    "import sys \n",
    "sys.path.append('..') \n",
    "from mxnet import nd\n",
    "import utils\n",
    "ctx=utils.try_gpu() \n",
    "print('Will use ',ctx) \n",
    "input_dim=vocab_size \n",
    "# 隐含状态长度 \n",
    "hidden_dim=256\n",
    "output_dim=vocab_size \n",
    "std=.01 \n",
    "def get_params():\n",
    "    # 隐含层 \n",
    "    W_xz=nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx)\n",
    "    W_hz=nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx) \n",
    "    b_z=nd.zeros(hidden_dim,ctx=ctx) \n",
    "    \n",
    "    W_xr=nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx) \n",
    "    W_hr=nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx) \n",
    "    b_r=nd.zeros(hidden_dim,ctx=ctx) \n",
    "    \n",
    "    W_xh=nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx) \n",
    "    W_hh=nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx) \n",
    "    b_h=nd.zeros(hidden_dim,ctx=ctx) \n",
    "    \n",
    "    # 输出层 \n",
    "    W_hy=nd.random_normal(scale=std,shape=(hidden_dim,output_dim),ctx=ctx) \n",
    "    b_y=nd.zeros(output_dim,ctx=ctx) \n",
    "    \n",
    "    params=[W_xz,W_hz,b_z, W_xr,W_hr,b_r,  W_xh,W_hh,b_h,  W_hy,b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad() \n",
    "    return params "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型 \n",
    "我们将前面的模型公式翻译成代码。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_rnn(inputs,H,*params):\n",
    "    # inputs: num_steps 个尺寸 batch_size * vocab_size 矩阵 \n",
    "    # H：尺寸为 batch_size * hidden_dim 矩阵 \n",
    "    # output : num_steps 个尺寸为 batch_size * vocab_size 矩阵 \n",
    "    W_xz,W_hz,b_z, W_xr,W_hr,b_r,  W_xh,W_hh,b_h,  W_hy,b_y=params\n",
    "    outputs=[]\n",
    "    for X in inputs:\n",
    "        Z=nd.sigmoid(nd.dot(X,W_xz)+nd.dot(H,W_hz)+b_z) \n",
    "        R=nd.sigmoid(nd.dot(X,W_xr)+nd.dot(H,W_hr)+b_r) \n",
    "        H_tilda=nd.tanh(nd.dot(X,W_xh)+R*nd.dot(H,W_hh)+b_h) \n",
    "        H=Z*H+(1-Z)*H_tilda \n",
    "        Y=nd.dot(H,W_hy)+b_y\n",
    "        outputs.append(Y) \n",
    "    return (outputs,H) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型 \n",
    "下面我们开始训练模型。我们假定谱写歌词的前缀分别为\"分开“，”不分开“和”战争中部队“。这里采用的是相邻批量采样实验门控循环单元谱写歌词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Training perplexity 271.140978\n",
      " -  分开 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你\n",
      " -  不分开 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你\n",
      " -  战争中部队 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你的我的 我不的你\n",
      "\n",
      "Epoch 40. Training perplexity 100.480900\n",
      " -  分开 我想要你想想你的爱爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可\n",
      " -  不分开 你想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我\n",
      " -  战争中部队 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我想要你的爱爱人 我\n",
      "\n",
      "Epoch 60. Training perplexity 27.770895\n",
      " -  分开 不想就这样 我不能再想 我不 我不 我不要再想 我不 我不 我不 我不 我不 我不 我不 我不 我不要再想 我不 我不 我不 我不 我不 我不 我不 我不 我不要再想 我不 我不 我不 我不 我不 我\n",
      " -  不分开 为我的手不是我 不要我的想 我不能再想 我不 我不 我不 我不 我不 我不 我不 我不 我不要再想 我不 我不 我不 我不 我不 我不 我不 我不 我不要再想 我不 我不 我不 我不 我不 我不 我不\n",
      " -  战争中部队 我想要的想 我不能再想 我不 我不 我不 我不 我不 我不 我不 我不 我不要再想 我不 我不 我不 我不 我不 我不 我不 我不 我不要再想 我不 我不 我不 我不 我不 我不 我不 我不 我不要再\n",
      "\n",
      "Epoch 80. Training perplexity 7.838467\n",
      " -  分开 还是那人在那里 我想就这样牵着你的手不放开 爱可不可以你简单单没有你 我不能再想 我不能再想 我不 我不 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我的想要的天  单悔的风索 不要再回忆 我\n",
      " -  不分开 不要再这样 我不能再想 我不 我不 我不 我不 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我的想要的天  单悔的风索 不要再回忆 我没有这样 我不能再想你 陪我的 不知好了太家 (难不要的爱\n",
      " -  战争中部队 他的那界被我妈动 一直令它的手 一切莫叫叫 它在小的我 你说着努忆 加乡走 让我连恨了吧 我   简！简！单！！！ 爱~~~~~~~~~~~ 我想想你的微笑 想你在你不起  不想再想你的微笑 我想揍你\n",
      "\n",
      "Epoch 100. Training perplexity 3.068070\n",
      " -  分开了口 还要在抽脉 我的世界将被摧毁 也许颓废也是另一种美 也许颓废也是谁 夜越黑　梦违背 想要去河南嵩山 我用耍地拆封将离开水掏空 人在古老 不代将发现 泪街事的羞叫我找何能够够的画 说过 你怎么依不能\n",
      " -  不分开 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活 不知不觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活 不知\n",
      " -  战争中部队 没有了空 我马将将生活 我该好好生活 不知不觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活 不知不觉 你已经离开我 不知不觉 我跟了这节奏 后知\n",
      "\n",
      "Epoch 120. Training perplexity 1.674061\n",
      " -  分开始做难道 没人帮着你走才快乐 爷爷泡的茶 有一种味道叫做家 他满头白发 喝茶时不准说话 陆羽泡的茶 像幅泼墨的山水画 唐朝千年的风沙 现在还在刮 (千年 那个山 阳属 不o搁存ㄟㄟ甜 放抹 故事到这为止\n",
      " -  不分开 我没有这样打 静静悄悄默默离开 陷入了危险边缘Baby~ 我的世界已狂风暴雨~~~~ 想 简！简！单！单！ 爱~~~~~~~~~~ 我想就这样牵着你的手不放开 爱能不能够永远单没没有伤害 我 想带你骑\n",
      " -  战争中部队 不懂得返来巢看历边 为啥咪铁支路直直 火车叨位去 为啥咪铁支路直直 火车叨位去 为啥咪铁支路直直 火车叨位去 为啥咪铁支路直直 火车叨位去 为啥咪铁支路直直 火车叨位去 为啥咪铁支路直直 火车叨位去 \n",
      "\n",
      "Epoch 140. Training perplexity 1.237015\n",
      " -  分开始做难道 没人帮着你走才快乐 爷爷泡的茶 有一种味道叫做家 没法挑剔它 口感味觉还不差 陆羽泡的茶 听说名和利都不拿 他牵着一匹瘦马在走天涯 爷爷泡的茶 有一种味道叫做家 他满头白发 喝茶时不准说话 陆\n",
      " -  不分开就走 把手慢慢交给我 放下心中的困惑 雨点从两旁划过 割开两种精神的我 经过老伯的家 篮框变得好高 爬过的那棵树 又何时变得渺小 这样也好 开始没人注意到我 等雨变强之前 我们将会分化软弱 趁时间没发觉\n",
      " -  战争中部队 不要再返了休了 马容就回到你那个子天 没人帮过去 试着一天天打 渴满了我的手 也放莹 干什么调 神底 沉箩酸 恨不了下避 没有悲空 你不想再活 我不能 你不要再想你 爱你走的泪面就像龙卷风 离不开暴风\n",
      "\n",
      "Epoch 160. Training perplexity 1.104505\n",
      " -  分开了躺板还要想 你和我怕多的可爱我 说不再这为简 不太担沉 又没有一个秋 后知后觉 我该好好生活 我该好好生活 静静悄悄默默离开 陷入了危险边缘Baby~ 我的世界已狂风暴雨 Wu~ 爱情来的太快就像龙卷\n",
      " -  不分开就走 把手慢慢交给我 放下心中的困惑 雨点从两旁的寻内  一定是我又的香净的美空 一点一直我叫属于我的天 我要一直一步往上爬 等峡谷的风 在小泼中发飘 真说 苦地烟 sㄡu 不o忿搁着你一种空 它一枝杨\n",
      " -  战争中部队 不要再返了休了 但是一起被酒 你的完美主义摧太太能走 这个个人反过难己多得安钮 但回到过去 试以让故事 是你的客 作面的战惠 又何水射声全人想要在 在小角千著水鹿的骨头 秃鹰盘旋死盯着腐肉 草原上两只\n",
      "\n",
      "Epoch 180. Training perplexity 1.065493\n",
      " -  分开了躺板还要停止 为什么这样子这一个ㄟ气的眼 蒙古高大南约的东写 什么有天十千圈 你会想要我的微笑 想要走过一直是开始还来远 还是怕怕我试无处来一起好演 就世共鸣来阻来就像 走过了一阵地方在的黑萨弥漫 我\n",
      " -  不分开就走 把手慢慢交给我 放下心中的困惑 雨点从两旁划过 割开两种精神的我 经过老伯的家 篮框变得好高 爬过的那棵树 又何时变得渺小 这样也好 开始没人注意到你我 等雨变强之前 我们将会分化软弱 趁时间没发\n",
      " -  战争中部队 不要再返一遍 马撕到回忆) 日色病夫的招牌 我说店小二 三两银够不够 景色入秋 漫天黄沙凉过 塞北的客栈人多 牧草有没有 我马儿有些瘦 天涯尽头 满脸风霜落寞 近乡情怯的我 相思寄红豆 相思寄红豆无能\n",
      "\n",
      "Epoch 200. Training perplexity 1.052483\n",
      " -  分开没躺难不要) 难不放 一颗两颗三颗 这领族人写下祈祷 连ー 但ㄚ咪间阳 但字久很久以前 我右拳引拆封将 化身为龙 把山河重新移动 填平裂缝 将东方 的日出调整了时空 回到洪荒 去支配去操纵 我右拳打开了\n",
      " -  不分开就走 把手慢慢交给我 放下心中的困惑 雨点从两旁划过 割开两种精神的我 经过老伯的家 篮框变得好高 爬过的那棵树 又何时变得渺小 这样也好 开始没人注意到你我 等雨变强之前 我们将会分化软弱 趁时间没发\n",
      " -  战争中部队 不要再返重之了 沉容一只天要我进头 我一定会呵护著你 趁时间没发觉 让我带着你离开 没有了证明 没有了空虚 基于两种立场我会罩着你 趁时间没发觉 让我带着你离开 这不是顽固 这不是逃避 没人帮着你走才\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq1='分开'\n",
    "seq2='不分开'\n",
    "seq3='战争中部队'\n",
    "seqs=[seq1,seq2,seq3]\n",
    "\n",
    "utils.train_and_predict_rnn(rnn=gru_rnn,is_random_iter=False,epochs=200,\n",
    "                       num_steps=35,hidden_dim=hidden_dim,\n",
    "                       learning_rate=0.2,clipping_norm=5,\n",
    "                       batch_size=32,pred_period=20,pred_len=100,\n",
    "                       seqs=seqs,get_params=get_params,\n",
    "                       get_inputs=get_inputs,ctx=ctx,\n",
    "                       corpus_indices=corpus_indices,\n",
    "                       idx_to_char=idx_to_char,char_to_idx=char_to_idx) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。\n",
    "\n",
    "## 结论\n",
    "\n",
    "* 门控循环单元的提出是为了更好地捕捉时序数据中间隔较大的依赖关系。\n",
    "* 重置门有助于捕捉时序数据中短期的依赖关系。\n",
    "* 更新门有助于捕捉时序数据中长期的依赖关系。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 调调参数（例如数据集大小、序列长度、隐含状态长度和学习率），看看对运行时间、perplexity和预测的结果造成的影响。\n",
    "* 在相同条件下，比较门控循环单元和循环神经网络的运行效率。\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/4042)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
