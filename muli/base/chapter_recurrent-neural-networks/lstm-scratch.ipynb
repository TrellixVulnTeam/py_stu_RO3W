{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 长短期记忆（LSTM）--- 从0开始\n",
    "\n",
    "[上一节](bptt.md)中，我们介绍了循环神经网络中的梯度计算方法。我们发现，循环神经网络的隐含层变量梯度可能会出现衰减或爆炸。虽然[梯度裁剪](rnn-scratch.md)可以应对梯度爆炸，但无法解决梯度衰减的问题。因此，给定一个时间序列，例如文本序列，循环神经网络在实际中其实较难捕捉两个时刻距离较大的文本元素（字或词）之间的依赖关系。\n",
    "\n",
    "为了更好地捕捉时序数据中间隔较大的依赖关系，我们介绍了一种常用的门控循环神经网络，叫做[门控循环单元](gru-scratch.md)。本节将介绍另一种常用的门控循环神经网络，长短期记忆（long short-term memory，简称LSTM）。它由Hochreiter和Schmidhuber在1997年被提出。事实上，它比门控循环单元的结构稍微更复杂一点。\n",
    "\n",
    "\n",
    "## 长短期记忆\n",
    "\n",
    "我们先介绍长短期记忆的构造。长短期记忆的隐含状态包括隐含层变量$\\mathbf{H}$和细胞$\\mathbf{C}$（也称记忆细胞）。它们形状相同。\n",
    "\n",
    "\n",
    "### 输入门、遗忘门和输出门\n",
    "\n",
    "\n",
    "假定隐含状态长度为$h$，给定时刻$t$的一个样本数为$n$特征向量维度为$x$的批量数据$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$和上一时刻隐含状态$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$，输入门（input gate）$\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$、遗忘门（forget gate）$\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$和输出门（output gate）$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$的定义如下：\n",
    "\n",
    "$$\\mathbf{I}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i)$$\n",
    "\n",
    "$$\\mathbf{F}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)$$\n",
    "\n",
    "$$\\mathbf{O}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o)$$\n",
    "\n",
    "其中的$\\mathbf{W}_{xi}, \\mathbf{W}_{xf}, \\mathbf{W}_{xo} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hi}, \\mathbf{W}_{hf}, \\mathbf{W}_{ho} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_i, \\mathbf{b}_f, \\mathbf{b}_o \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。函数$\\sigma$自变量中的三项相加使用了[广播](../chapter_crashcourse/ndarray.md)。\n",
    "\n",
    "和[门控循环单元](gru-scratch.md)中的重置门和更新门一样，这里的输入门、遗忘门和输出门中每个元素的值域都是$[0, 1]$。\n",
    "\n",
    "\n",
    "### 候选细胞\n",
    "\n",
    "和[门控循环单元](gru-scratch.md)中的候选隐含状态一样，长短期记忆中的候选细胞$\\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h}$也使用了值域在$[-1, 1]$的双曲正切函数tanh做激活函数：\n",
    "\n",
    "$$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$$\n",
    "\n",
    "其中的$\\mathbf{W}_{xc} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hc} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_c \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。\n",
    "\n",
    "\n",
    "### 细胞\n",
    "\n",
    "我们可以通过元素值域在$[0, 1]$的输入门、遗忘门和输出门来控制隐含状态中信息的流动：这通常可以应用按元素乘法符$\\odot$。当前时刻细胞$\\mathbf{C}_t \\in \\mathbb{R}^{n \\times h}$的计算组合了上一时刻细胞和当前时刻候选细胞的信息，并通过遗忘门和输入门来控制信息的流动：\n",
    "\n",
    "$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$$\n",
    "\n",
    "需要注意的是，如果遗忘门一直近似1且输入门一直近似0，过去的细胞将一直通过时间保存并传递至当前时刻。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时序数据中间隔较大的依赖关系。\n",
    "\n",
    "\n",
    "### 隐含状态\n",
    "\n",
    "有了细胞以后，接下来我们还可以通过输出门来控制从细胞到隐含层变量$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$的信息的流动：\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\text{tanh}(\\mathbf{C}_t)$$\n",
    "\n",
    "需要注意的是，当输出门近似1，细胞信息将传递到隐含层变量；当输出门近似0，细胞信息只自己保留。\n",
    "\n",
    "\n",
    "\n",
    "输出层的设计可参照[循环神经网络](rnn-scratch.md)中的描述。\n",
    "\n",
    "\n",
    "## 实验\n",
    "\n",
    "\n",
    "为了实现并展示门控循环单元，我们依然使用周杰伦歌词数据集来训练模型作词。这里除长短期记忆以外的实现已在[循环神经网络](rnn-scratch.md)中介绍。\n",
    "\n",
    "\n",
    "### 数据处理\n",
    "\n",
    "我们先读取并对数据集做简单处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size : 1465\n"
     ]
    }
   ],
   "source": [
    "import zipfile \n",
    "with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip','r') as zin:\n",
    "    zin.extractall('../data/') \n",
    "    \n",
    "with open('../data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars=f.read() \n",
    "\n",
    "corpus_chars=corpus_chars.replace('\\n',' ').replace('\\r',' ') \n",
    "corpus_chars=corpus_chars[0:20000]\n",
    "\n",
    "idx_to_char=list(set(corpus_chars))\n",
    "char_to_idx=dict([(char,i) for i,char in enumerate(idx_to_char)])\n",
    "corpus_indices=[char_to_idx[char] for char in corpus_chars] \n",
    "\n",
    "vocab_size=len(char_to_idx) \n",
    "print('vocab_size :',vocab_size) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用onehot 来将字符索引表示成向量。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_inputs(data):\n",
    "#     return [nd.one_hot(X,vocab_size) for X in data.T] \n",
    "def get_inputs(data):\n",
    "    return [nd.one_hot(X, vocab_size) for X in data.T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数 \n",
    "以下部分对模型参数进行初始化。参数\"hidden_dim\"定义了隐含状态的长度。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaa\n",
      "Will use gpu(0)\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx \n",
    "# 尝试使用GPU \n",
    "import sys \n",
    "sys.path.append('..') \n",
    "from mxnet import nd \n",
    "import utils \n",
    "ctx=utils.try_gpu() \n",
    "print('Will use ',ctx) \n",
    "input_dim=vocab_size \n",
    "# 隐含状态长度 \n",
    "hidden_dim=256\n",
    "output_dim=vocab_size \n",
    "std=0.01 \n",
    "def get_params():\n",
    "    # 输入参数 \n",
    "    W_xi=nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx) \n",
    "    W_hi=nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx) \n",
    "    b_i=nd.zeros(hidden_dim,ctx=ctx) \n",
    "    \n",
    "    # 遗忘门参数 \n",
    "    W_xf=nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx) \n",
    "    W_hf=nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx) \n",
    "    b_f=nd.zeros(hidden_dim,ctx=ctx) \n",
    "    \n",
    "    # 输出门参数 \n",
    "    W_xo=nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx) \n",
    "    W_ho=nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx) \n",
    "    b_o=nd.zeros(hidden_dim,ctx=ctx) \n",
    "    \n",
    "    # 候选细胞参数 \n",
    "    W_xc=nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx) \n",
    "    W_hc=nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx) \n",
    "    b_c=nd.zeros(hidden_dim,ctx=ctx) \n",
    "    \n",
    "    # 输出层\n",
    "    W_hy=nd.random_normal(scale=std,shape=(hidden_dim,output_dim) ,ctx=ctx) \n",
    "    b_y=nd.zeros(output_dim,ctx=ctx) \n",
    "    params=[W_xi,W_hi,b_i,  W_xf,W_hf,b_f,  W_xo,W_ho,b_o,  W_xc,W_hc,b_c,  W_hy,b_y] \n",
    "    for param in params:\n",
    "        param.attach_grad() \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型 \n",
    "我们将前面的模型公式翻译成代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_rnn(inputs, state_h, state_c, *params):\n",
    "    # inputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵\n",
    "    # H: 尺寸为 batch_size * hidden_dim 矩阵\n",
    "    # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hy, b_y] = params\n",
    "\n",
    "    H = state_h\n",
    "    C = state_c\n",
    "    outputs = []\n",
    "    for X in inputs:        \n",
    "        I = nd.sigmoid(nd.dot(X, W_xi) + nd.dot(H, W_hi) + b_i)\n",
    "        F = nd.sigmoid(nd.dot(X, W_xf) + nd.dot(H, W_hf) + b_f)\n",
    "        O = nd.sigmoid(nd.dot(X, W_xo) + nd.dot(H, W_ho) + b_o)\n",
    "        C_tilda = nd.tanh(nd.dot(X, W_xc) + nd.dot(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * nd.tanh(C)\n",
    "        Y = nd.dot(H, W_hy) + b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs, H, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型 \n",
    "下面我们开始训练模型。我们假定谱写歌词的前缀分别为‘分开’，‘不分开’和‘战争中部队’。这里\n",
    "采用的是相邻批量采样实验门控循环单元谱写歌词 。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Training perplexity 320.252024\n",
      " -  分开 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 \n",
      " -  不分开 我我的我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我\n",
      " -  战争中部队 我我的我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我的 我我的 我不的 我我\n",
      "\n",
      "Epoch 40. Training perplexity 179.518659\n",
      " -  分开 我想的你你不要 我想的你你你爱爱 我想的你你爱爱爱 爱不的你你不爱你的可爱女 我想的让我想要的可爱女人 我想的让我想要的可爱女人 我想的让我想要的可爱女人 我想的让我想要的可爱女人 我想的让我想要的可\n",
      " -  不分开 我想的你你你的可爱女 我想的你你想你的可爱女 我想的你你想你的可爱女 我想的你你想你的可爱女 我想的你你想你的可爱女 我想的你你想你的可爱女 我想的你你想你的可爱女 我想的你你想你的可爱女 我想的你你\n",
      " -  战争中部队 我想的你你想你的可爱女 我想的让我想要的可爱女人 我想的让我想要的可爱女人 我想的让我想要的可爱女人 我想的让我想要的可爱女人 我想的让我想要的可爱女人 我想的让我想要的可爱女人 我想的让我想要的可爱\n",
      "\n",
      "Epoch 60. Training perplexity 71.335562\n",
      " -  分开 我想要你的爱我 想想你 你想我 想你 别！ 别！ 单！ 单！ xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi \n",
      " -  不分开 我想要你的爱我 想想你 你不我 想你 别！ 别！ 单！ 单！ xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi \n",
      " -  战争中部队 我有你 一兽人 的灵魂 三滚 xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi x\n",
      "\n",
      "Epoch 80. Training perplexity 26.446387\n",
      " -  分开 我不要再想你 你的手是你 我不能 爱爱的你 快后了这样 这不的没爱 我不能再你 我不能再你 我不了这生 我的能好 我不能这生 我不能觉 我不能这生 我不能觉生 我不能觉你 我不能好你 我不能这生 我的\n",
      " -  不分开 我不要这想 你不着你想 我不要 爱不了 你不了这太我 不不着 你不了这太我 不知去 你不了这太了 你想好 别不了你不想 说你去 别不开 别不了 别不了 别对了 别对 停对 停止 停止 停i xi x去\n",
      " -  战争中部队的脸 用时的风 有一种的叫叫 一道在着 我想再再我 再不着觉的你 我想会好你 你的不是我 不不着这生 我的能好 我不能再生 我不能觉 我不能再你 我不能觉你 我不了这生 我不能好生 我不能觉你 我不能这\n",
      "\n",
      "Epoch 100. Training perplexity 11.025155\n",
      " -  分开 不是我的爱我 一直两 一直走 我想就这样牵着你的手不放开 爱可不可我 可以你有 说你要这样我妈妈 我不要 你想想 别我 却杰伦 单不了停！留！ 爱爱去 爱爱后的口面 放去好好气活 一直到我 你爱了这球\n",
      " -  不分开 我要要陪我 你着着人走 这不事停走 你不不要 你我了这样走 不知不觉 你已了人节我 后知不觉 我该了这节活 后知后觉 又过了一个秋 后知后觉 我该好好生活 我知好觉生活 不知不觉 你已经离开我 不知不\n",
      " -  战争中部队 我想要你已经没人 不要你 说你眼睛的快? 我想  别笑我的想头 你说  想想再不想我 说你去 别给我的想球 说你去 别想我 想不了 别怎么么太么 我说啊 别不有 别不了 别不球 别对你么停着 我说着 \n",
      "\n",
      "Epoch 120. Training perplexity 5.210170\n",
      " -  分开 是你么我不多 干着 没有这人我的想想你牵牵... 只多 不不到 看杰  是是 该谁伦 周我的该有重重过的暴淡淡的泪我的爱  你来的旧神 如果 想很很 你怎的一起我 甩知伦 的让人 的眼怎 一直停停留着\n",
      " -  不分开去 为什么这样 我说多定 你不着你爱爱有能能来熬 我没有这种天份 你也你也你离开 不不到不不是你 不是来慢 我爱一些节奏 后知后觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好\n",
      " -  战争中部队的天 沉过的风我已狂的可爱女) 轻果放这 你将我感到更加沮丧 (难的这不是 我想想到你 没没的让我面了你可你不多 我有道这心天 你会着你不离 我不要你 你不是我想 不不能陪你我的爱场 就你走陪你会会止 \n",
      "\n",
      "Epoch 140. Training perplexity 3.111478\n",
      " -  分开 是我面人多走 我说不会你离开 夜静为悄默默背 我也会慢慢封开 你什么好开 我已着着开走 又静了我 全你一口节秋 你知起看游我 一场抢觉一友 我想你 你分人的我留 说你去 让你怎么口留 你说啊对开药箱 \n",
      " -  不分开去过的甜样来你走走 最许到有不是再你你 爱你不了了你没会往你 印上中的爱情好像像晰住练 我给你的爱写在西元前 深埋在美索不达米亚平原 几楔形文字刻下了永远 那已风化千年的誓言 一切又重演 我很往很天恼 \n",
      " -  战争中部队的脸戏 用愿古的风后好好中 是被开了它始的你 老念老老我来过于我的你 用古安斑鸠 会感那开的那里 我把能白静静欣赏你你那远 我的世界我狂狂 你在在已 他来一起热粥 配上几斤 我想怕这子功 那水水壁 快人\n",
      "\n",
      "Epoch 160. Training perplexity 2.101260\n",
      " -  分开 是我有真的手面 残忍莹温柔出现 脆弱时间到 我们一起来祷告 仁慈的父我已坠入 看不见罪的国度 请原谅我的自负 没人能说没人可说 好难承受 荣耀的背后刻着一道孤独 闭慈双眼我已坠见 当年那梦的画面 天空\n",
      " -  不分开去我的想手好你走到 心在每层 在你有发 听乡的话有我有一定 你在在乐 我想能努熬 (没有你在我有多烦我 难着你陪的注份 这次会抱得更紧 这样挽也不知还来不来来及 想回到过去 试着不事 挡挡的侧有我的回道\n",
      " -  战争中部队的黑 想要你的爱界一种 然次的梦一天一片斑鸠 印地安老斑鸠 腿短毛不多 几天都没有喝水也能活 脑袋瓜有一点秀逗 猎物死了它比谁都难过 印地安斑鸠 会学人开口 仙人掌怕羞 蜥蝪横著走 这里什么奇怪的事都有\n",
      "\n",
      "Epoch 180. Training perplexity 1.603193\n",
      " -  分开 是我有真的忧面 残忍莹温的出 现感成得忆 小人的美丽 你的完美主义 太彻底 让我连恨都难以下笔 将真心抽离写成日记 像是一场默剧 你的完美主义 太彻底 分手的话像语言暴力 我已无能为力再提起 决定中断\n",
      " -  不分开去过 没么你慢 我已已难开我 你不不觉生离知知 风风你心不护你才也道 爷爷着 小是开著陪快实我 泪不开没发起的黄谣 我想著你想牵著 一身为梦 你不要有多我 不知悔觉 又过我没太难 后知后觉 快过了一切秋\n",
      " -  战争中部队的路延 用人的木猫大大的喊想 孤有的阵我不见我可亚不着 我要一声是你一点的天不在天 专后 我又表 的让让 三变 是真的从的喜面下道 等不休 别洛克 娘子她人夫江日折 一枝杨柳的书面在在在 他很的在我不见\n",
      "\n",
      "Epoch 200. Training perplexity 1.381580\n",
      " -  分开 是我面的话 有一种美主做静在蔓延 爷爷的抽爱 一切它不道 如平岁的羞人在我了会 你一着着你 那天抱不满 一子一朵在我而香 却著你夕的外 随一种叫斑岛 平常你不多 除非是乌鸦抢了它 ㄒ在床愿愿外 在我们\n",
      " -  不分开就走 把手慢慢交给我 放下心中的困惑 雨点从两旁划过 割开两种精神的我 经过老伯的家 篮框的得好 爬平的那丽过 又后变变得渺的争 纪过过过去将瞒 开也的天笑的过 让谁著牵的画征 然留的没人人过开 好难承\n",
      " -  战争中部队的脸 还著古看前 试视碑 满里的字 一一直很热密家唱页序 我也远我都前的口 一些在天流那都的老斑鸠 印地安老斑 再着我 你字字口典 查下下很庄会 只容 失在角角 印地在它球 还下在一枝 你只于它球球 我\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq1='分开'\n",
    "seq2='不分开'\n",
    "seq3='战争中部队'\n",
    "seqs=[seq1,seq2,seq3]\n",
    "\n",
    "utils.train_and_predict_rnn(rnn=lstm_rnn,is_random_iter=False,epochs=200,\n",
    "                           num_steps=35,hidden_dim=hidden_dim,\n",
    "                           learning_rate=0.2,clipping_norm=5,\n",
    "                           batch_size =32,pred_period=20,pred_len=100,\n",
    "                           seqs=seqs,get_params=get_params,\n",
    "                           get_inputs=get_inputs,ctx=ctx,\n",
    "                           corpus_indices=corpus_indices,\n",
    "                           idx_to_char=idx_to_char,char_to_idx=char_to_idx,\n",
    "                           is_lstm=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。\n",
    "\n",
    "## 结论\n",
    "\n",
    "* 长短期记忆的提出是为了更好地捕捉时序数据中间隔较大的依赖关系。\n",
    "* 长短期记忆的结构比门控循环单元的结构较复杂。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 调调参数（例如数据集大小、序列长度、隐含状态长度和学习率），看看对运行时间、perplexity和预测的结果造成的影响。\n",
    "* 在相同条件下，比较长短期记忆和门控循环单元以及循环神经网络的运行效率。\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/4042)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
