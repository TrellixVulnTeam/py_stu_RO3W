{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 使用 Gluon的线性回归 \n",
    " \n",
    " 前一章使用ndarray和autograd 来实现线性回归，这一章我们仍然实现相同的模型，\n",
    " 但是使用高层抽象包 gluon . \n",
    " \n",
    " ## 创建数据集 \n",
    " \n",
    " 我们生成同样的数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd \n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=2 \n",
    "num_examples=1000 \n",
    "true_w=[2,-3.4]\n",
    "true_b=4.2 \n",
    "\n",
    "X = nd.random_normal(shape=(num_examples,num_inputs)) \n",
    "y=true_w[0]* X[:,0] + true_w[1]*X[:,1]+true_b \n",
    "y+=0.01 * nd.random_normal(shape=y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据读取 \n",
    "但这里使用data 模块来读取数据 。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10 \n",
    "dataset=gluon.data.ArrayDataset(X,y) \n",
    "data_iter=gluon.data.DataLoader(dataset,batch_size,shuffle=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取和前面一致： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 8.1348163e-01 -5.6710786e-01]\n",
      " [ 2.5984043e-01  8.9015400e-01]\n",
      " [-2.4894042e-02  6.3270587e-01]\n",
      " [ 3.0029500e-01  7.3225945e-01]\n",
      " [ 1.0161712e+00  5.0922304e-01]\n",
      " [-3.4465104e-01 -4.1924685e-01]\n",
      " [-3.9304093e-01  4.5682105e-01]\n",
      " [ 2.4472828e-01  6.6647071e-01]\n",
      " [ 6.7431526e-04 -7.7242523e-01]\n",
      " [ 1.0347279e-01  1.3004491e-01]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[7.776431  1.6905847 2.0036626 2.3162487 4.498421  4.9339895 1.867064\n",
      " 2.4410765 6.8383746 3.9475276]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for data,label in data_iter:\n",
    "    print(data,label) \n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型 \n",
    "当我们手写模型的时候，我们需要先声明模型参数，然后再使用它们来构建模型。但gluon提供大量\n",
    "的提前定制好的层，使得我们只需要主要关注使用哪些层来构建模型。\n",
    "例如线性模型就是使用对应的Dense层 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=gluon.nn.Sequential() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们加入一个Dense层，它唯一必须要定义的参数就是输出节点的个数，在线性模型里面是1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add(gluon.nn.Dense(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dense(None -> 1, linear)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(注意这里我们并没有定义说这个层的输入节点是多少，在这之后真正给数据的时候系统会自动赋值。\n",
    "我们之后会详细介绍这个特性是如何工作的。） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化模型参数 \n",
    "在使用前，net我们必须要初始化模型权重，这里我们使用默认随机初始化方法\n",
    "(之后我们会介绍更多的初始化方法) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数 \n",
    "gluon 提供了平方误差函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_loss=gluon.loss.L2Loss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化 \n",
    "我们同样无需手动实现随机梯度下降，我们可以用创建一个Trainer的实例，并且将\n",
    "模型参数传递给它就行 。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=gluon.Trainer(\n",
    "    net.collect_params(),'sgd',{'learning_rate':0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.gluon.trainer.Trainer at 0x7faf18aecac8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练 \n",
    "这里的训练跟前面没有太多区别，唯一的就是我们不再是调用SGD，而是trainer,step来攻心模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , average loss : 7.415231 \n",
      "Epoch 1 , average loss : 0.970896 \n",
      "Epoch 2 , average loss : 0.127767 \n",
      "Epoch 3 , average loss : 0.016934 \n",
      "Epoch 4 , average loss : 0.002291 \n",
      "Epoch 5 , average loss : 0.000346 \n",
      "Epoch 6 , average loss : 0.000088 \n",
      "Epoch 7 , average loss : 0.000054 \n",
      "Epoch 8 , average loss : 0.000049 \n",
      "Epoch 9 , average loss : 0.000048 \n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for e in range(epochs):\n",
    "    total_loss=0 \n",
    "    for data,label in data_iter: \n",
    "        with autograd.record():\n",
    "            output=net(data) \n",
    "            loss=square_loss(output,label) \n",
    "        loss.backward()\n",
    "        trainer.step(batch_size) \n",
    "        total_loss+=nd.sum(loss).asscalar()\n",
    "    print(\"Epoch %d , average loss : %f \"% (e,total_loss/num_examples)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4],\n",
       " \n",
       " [[ 1.9996634 -3.400032 ]]\n",
       " <NDArray 1x2 @cpu(0)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense=net[0]\n",
    "true_w,dense.weight.data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2,\n",
       " \n",
       " [4.2001705]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b,dense.bias.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论\n",
    "可以看到gluon可以帮助我们更快更干净地实现模型 \n",
    "\n",
    "# 练习 \n",
    "\n",
    "在训练的时候，为什么我们用了比前面要大10倍的学习率呢\n",
    "（提示：可以尝试运行help(train.step)来查找答案) \n",
    "如何拿到weight的梯度呢?(提示：尝试help(dense.weight) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Parameter in module mxnet.gluon.parameter object:\n",
      "\n",
      "class Parameter(builtins.object)\n",
      " |  A Container holding parameters (weights) of Blocks.\n",
      " |  \n",
      " |  :py:class:`Parameter` holds a copy of the parameter on each :py:class:`Context` after\n",
      " |  it is initialized with ``Parameter.initialize(...)``. If :py:attr:`grad_req` is\n",
      " |  not ``'null'``, it will also hold a gradient array on each :py:class:`Context`::\n",
      " |  \n",
      " |      ctx = mx.gpu(0)\n",
      " |      x = mx.nd.zeros((16, 100), ctx=ctx)\n",
      " |      w = mx.gluon.Parameter('fc_weight', shape=(64, 100), init=mx.init.Xavier())\n",
      " |      b = mx.gluon.Parameter('fc_bias', shape=(64,), init=mx.init.Zero())\n",
      " |      w.initialize(ctx=ctx)\n",
      " |      b.initialize(ctx=ctx)\n",
      " |      out = mx.nd.FullyConnected(x, w.data(ctx), b.data(ctx), num_hidden=64)\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  name : str\n",
      " |      Name of this parameter.\n",
      " |  grad_req : {'write', 'add', 'null'}, default 'write'\n",
      " |      Specifies how to update gradient to grad arrays.\n",
      " |  \n",
      " |      - ``'write'`` means everytime gradient is written to grad :py:class:`NDArray`.\n",
      " |      - ``'add'`` means everytime gradient is added to the grad :py:class:`NDArray`. You need\n",
      " |        to manually call ``zero_grad()`` to clear the gradient buffer before each\n",
      " |        iteration when using this option.\n",
      " |      - 'null' means gradient is not requested for this parameter. gradient arrays\n",
      " |        will not be allocated.\n",
      " |  shape : int or tuple of int, default None\n",
      " |      Shape of this parameter. By default shape is not specified. Parameter with\n",
      " |      unknown shape can be used for :py:class:`Symbol` API, but ``init`` will throw an error\n",
      " |      when using :py:class:`NDArray` API.\n",
      " |  dtype : numpy.dtype or str, default 'float32'\n",
      " |      Data type of this parameter. For example, ``numpy.float32`` or ``'float32'``.\n",
      " |  lr_mult : float, default 1.0\n",
      " |      Learning rate multiplier. Learning rate will be multiplied by lr_mult\n",
      " |      when updating this parameter with optimizer.\n",
      " |  wd_mult : float, default 1.0\n",
      " |      Weight decay multiplier (L2 regularizer coefficient). Works similar to lr_mult.\n",
      " |  init : Initializer, default None\n",
      " |      Initializer of this parameter. Will use the global initializer by default.\n",
      " |  stype: {'default', 'row_sparse', 'csr'}, defaults to 'default'.\n",
      " |      The storage type of the parameter.\n",
      " |  grad_stype: {'default', 'row_sparse', 'csr'}, defaults to 'default'.\n",
      " |      The storage type of the parameter's gradient.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  grad_req : {'write', 'add', 'null'}\n",
      " |      This can be set before or after initialization. Setting ``grad_req`` to ``'null'``\n",
      " |      with ``x.grad_req = 'null'`` saves memory and computation when you don't\n",
      " |      need gradient w.r.t x.\n",
      " |  lr_mult : float\n",
      " |      Local learning rate multiplier for this Parameter. The actual learning rate\n",
      " |      is calculated with ``learning_rate * lr_mult``. You can set it with\n",
      " |      ``param.lr_mult = 2.0``\n",
      " |  wd_mult : float\n",
      " |      Local weight decay multiplier for this Parameter.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, name, grad_req='write', shape=None, dtype=<class 'numpy.float32'>, lr_mult=1.0, wd_mult=1.0, init=None, allow_deferred_init=False, differentiable=True, stype='default', grad_stype='default')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  cast(self, dtype)\n",
      " |      Cast data and gradient of this Parameter to a new data type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or numpy.dtype\n",
      " |          The new data type.\n",
      " |  \n",
      " |  data(self, ctx=None)\n",
      " |      Returns a copy of this parameter on one context. Must have been\n",
      " |      initialized on this context before. For sparse parameters, use\n",
      " |      :py:meth:`Parameter.row_sparse_data` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context\n",
      " |          Desired context.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      NDArray on ctx\n",
      " |  \n",
      " |  grad(self, ctx=None)\n",
      " |      Returns a gradient buffer for this parameter on one context.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context\n",
      " |          Desired context.\n",
      " |  \n",
      " |  initialize(self, init=None, ctx=None, default_init=<mxnet.initializer.Uniform object at 0x7faf1888d908>, force_reinit=False)\n",
      " |      Initializes parameter and gradient arrays. Only used for :py:class:`NDArray` API.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      init : Initializer\n",
      " |          The initializer to use. Overrides :py:meth:`Parameter.init` and default_init.\n",
      " |      ctx : Context or list of Context, defaults to :py:meth:`context.current_context()`.\n",
      " |          Initialize Parameter on given context. If ctx is a list of Context, a\n",
      " |          copy will be made for each context.\n",
      " |      \n",
      " |          .. note::\n",
      " |              Copies are independent arrays. User is responsible for keeping\n",
      " |              their values consistent when updating.\n",
      " |              Normally :py:class:`gluon.Trainer` does this for you.\n",
      " |      \n",
      " |      default_init : Initializer\n",
      " |          Default initializer is used when both :py:func:`init`\n",
      " |          and :py:meth:`Parameter.init` are ``None``.\n",
      " |      force_reinit : bool, default False\n",
      " |          Whether to force re-initialization if parameter is already initialized.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> weight = mx.gluon.Parameter('weight', shape=(2, 2))\n",
      " |      >>> weight.initialize(ctx=mx.cpu(0))\n",
      " |      >>> weight.data()\n",
      " |      [[-0.01068833  0.01729892]\n",
      " |       [ 0.02042518 -0.01618656]]\n",
      " |      <NDArray 2x2 @cpu(0)>\n",
      " |      >>> weight.grad()\n",
      " |      [[ 0.  0.]\n",
      " |       [ 0.  0.]]\n",
      " |      <NDArray 2x2 @cpu(0)>\n",
      " |      >>> weight.initialize(ctx=[mx.gpu(0), mx.gpu(1)])\n",
      " |      >>> weight.data(mx.gpu(0))\n",
      " |      [[-0.00873779 -0.02834515]\n",
      " |       [ 0.05484822 -0.06206018]]\n",
      " |      <NDArray 2x2 @gpu(0)>\n",
      " |      >>> weight.data(mx.gpu(1))\n",
      " |      [[-0.00873779 -0.02834515]\n",
      " |       [ 0.05484822 -0.06206018]]\n",
      " |      <NDArray 2x2 @gpu(1)>\n",
      " |  \n",
      " |  list_ctx(self)\n",
      " |      Returns a list of contexts this parameter is initialized on.\n",
      " |  \n",
      " |  list_data(self)\n",
      " |      Returns copies of this parameter on all contexts, in the same order\n",
      " |      as creation. For sparse parameters, use :py:meth:`Parameter.list_row_sparse_data`\n",
      " |      instead.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of NDArrays\n",
      " |  \n",
      " |  list_grad(self)\n",
      " |      Returns gradient buffers on all contexts, in the same order\n",
      " |      as :py:meth:`values`.\n",
      " |  \n",
      " |  list_row_sparse_data(self, row_id)\n",
      " |      Returns copies of the 'row_sparse' parameter on all contexts, in the same order\n",
      " |      as creation. The copy only retains rows whose ids occur in provided row ids.\n",
      " |      The parameter must have been initialized before.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      row_id: NDArray\n",
      " |          Row ids to retain for the 'row_sparse' parameter.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of NDArrays\n",
      " |  \n",
      " |  reset_ctx(self, ctx)\n",
      " |      Re-assign Parameter to other contexts.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context or list of Context, default ``context.current_context()``.\n",
      " |          Assign Parameter to given context. If ctx is a list of Context, a\n",
      " |          copy will be made for each context.\n",
      " |  \n",
      " |  row_sparse_data(self, row_id)\n",
      " |      Returns a copy of the 'row_sparse' parameter on the same context as row_id's.\n",
      " |      The copy only retains rows whose ids occur in provided row ids.\n",
      " |      The parameter must have been initialized on this context before.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      row_id: NDArray\n",
      " |          Row ids to retain for the 'row_sparse' parameter.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      NDArray on row_id's context\n",
      " |  \n",
      " |  set_data(self, data)\n",
      " |      Sets this parameter's value on all contexts.\n",
      " |  \n",
      " |  var(self)\n",
      " |      Returns a symbol representing this parameter.\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradient buffer on all contexts to 0. No action is taken if\n",
      " |      parameter is uninitialized or doesn't require gradient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  dtype\n",
      " |      The type of the parameter.\n",
      " |      \n",
      " |      Setting the dtype value is equivalent to casting the value of the parameter\n",
      " |  \n",
      " |  grad_req\n",
      " |  \n",
      " |  shape\n",
      " |      The shape of the parameter.\n",
      " |      \n",
      " |      By default, an unknown dimension size is 0. However, when the NumPy semantic\n",
      " |      is turned on, unknown dimension size is -1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dense.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense.weight?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
