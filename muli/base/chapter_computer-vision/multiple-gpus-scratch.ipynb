{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多GPU来训练 --- 从0开始\n",
    "\n",
    "本教程我们将展示如何使用多个GPU来加速训练。正如你期望的那样，这个教程需要至少两块GPU来运行。事实上，一台机器上安装多块GPU非常常见，因为通常主板上会有多个PCIe插槽。下图是一台服务器上安装了8块Titan X。\n",
    "\n",
    "![](../img/8x-titan-x.png)\n",
    "\n",
    "如果正确安装了NVIDIA驱动，我们可以通过`nvidia-smi`来查看当前系统有多少个GPU。\n",
    "\n",
    "```{.python .input  n=1}\n",
    "!nvidia-smi\n",
    "```\n",
    "\n",
    "在[自动并行](./auto-parallelism.md)里我们提到虽然大部分的运算可以要么全部使用所有的CPU计算资源，或者单GPU的资源。但对于多GPU的情况，我们仍然需要来实现对应的算法。这些算法中最常用的叫做数据并行。\n",
    "\n",
    "## 数据并行\n",
    "\n",
    "数据并行目前是深度学习里面使用最广泛的用来将任务划分到多设备的办法。它是这样工作的，假设这里有*k*个GPU，每个GPU将维护一个模型参数的复制。然后每次我们将一个批量里面的样本划分成*k*块并分每个GPU一块。每个GPU使用分到的数据计算梯度。然后我们将所有GPU上梯度相加得到这个批量上的完整梯度。之后每个GPU使用这个完整梯度对自己维护的模型做更新。\n",
    "\n",
    "\n",
    "## 定义模型\n",
    "\n",
    "我们使用[卷积神经网络 --- 从0开始](../chapter_convolutional-neural-networks/cnn-scratch.md)里介绍的LeNet来作为本章的样例任务。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 28 10:51:42 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 60%   58C    P2    92W / 250W |   7760MiB / 11016MiB |     45%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1942      G   /usr/lib/xorg/Xorg                 12MiB |\n",
      "|    0   N/A  N/A      2042      G   /usr/bin/gnome-shell               65MiB |\n",
      "|    0   N/A  N/A     27635      G   /usr/bin/gnome-shell              179MiB |\n",
      "|    0   N/A  N/A     29142      G   /usr/lib/xorg/Xorg                114MiB |\n",
      "|    0   N/A  N/A     30059      G   ...mviewer/tv_bin/TeamViewer       22MiB |\n",
      "|    0   N/A  N/A     37807      C   ...da3/envs/gluon/bin/python     7359MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd \n",
    "from mxnet import gluon \n",
    "# initialize parameters \n",
    "scale=0.01\n",
    "W1=nd.random.normal(shape=(20,1,3,3))*scale \n",
    "b1=nd.zeros(shape=20) \n",
    "W2=nd.random.normal(shape=(50,20,5,5))*scale \n",
    "b2=nd.zeros(shape=50)\n",
    "W3=nd.random.normal(shape=(800,128))*scale \n",
    "b3=nd.zeros(shape=128)\n",
    "W4=nd.random.normal(shape=(128,10))*scale \n",
    "b4=nd.zeros(shape=10) \n",
    "params=[W1,b1,W2,b2,W3,b3,W4,b4]\n",
    "# network and loss \n",
    "\n",
    "def lenet(X,params):\n",
    "    # first conv \n",
    "    h1_conv=nd.Convolution(data=X,weight=params[0],bias=params[1],\n",
    "                          kernel=(3,3),num_filter=20) \n",
    "    h1_activation=nd.relu(h1_conv)\n",
    "    h1=nd.Pooling(data=h1_activation,pool_type='avg',\n",
    "                kernel=(2,2),stride=(2,2)) \n",
    "    # second conv \n",
    "    h2_conv=nd.Convolution(data=h1,weight=params[2],bias=params[3],\n",
    "                          kernel=(5,5),num_filter=50)\n",
    "    h2_activation=nd.relu(h2_conv) \n",
    "    h2=nd.Pooling(data=h2_activation,pool_type='avg',\n",
    "                 kernel=(2,2),stride=(2,2)) \n",
    "    h2=nd.flatten(h2) \n",
    "    # first dense \n",
    "    h3_linear=nd.dot(h2,params[4]+params[5]) \n",
    "    h3=nd.relu(h3_linear) \n",
    "    # second dense \n",
    "    yhat=nd.dot(h3,params[6])+params[7]\n",
    "    return yhat \n",
    "loss=gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "然后我们先实现几个在GPU同步数据的辅助函数。\n",
    "\n",
    "## 在多GPU之间同步数据\n",
    "\n",
    "下面函数将模型参数复制到某个特定设备并初始化梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 weight =  \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "<NDArray 20 @cpu(0)>\n",
      "b1 grad = \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "<NDArray 20 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gpu,cpu\n",
    "def get_params(params,ctx):\n",
    "    new_params=[p.copyto(ctx) for p in params]\n",
    "    for p in new_params:\n",
    "        p.attach_grad()\n",
    "    return new_params \n",
    "# copy param to GPU(0) \n",
    "# new_params=get_params(params,gpu(0))\n",
    "new_params=get_params(params,cpu(0))\n",
    "\n",
    "print('b1 weight = ',new_params[1]) \n",
    "print('b1 grad =',new_params[1].grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定分布在多个GPU之间数据，我们定义一个函数它将这些数据加起来，然后再广播到所有GPU上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: [\n",
      "[[1. 1.]]\n",
      "<NDArray 1x2 @cpu(0)>, \n",
      "[[2. 2.]]\n",
      "<NDArray 1x2 @cpu(1)>]\n",
      "After: [\n",
      "[[3. 3.]]\n",
      "<NDArray 1x2 @cpu(0)>, \n",
      "[[3. 3.]]\n",
      "<NDArray 1x2 @cpu(1)>]\n"
     ]
    }
   ],
   "source": [
    "def allreduce(data):\n",
    "    # sum on data[0].context, and then broadcast \n",
    "    for i in range(1,len(data)):\n",
    "        data[0][:]+=data[i].copyto(data[0].context) \n",
    "    for i in range(1,len(data)):\n",
    "        data[0].copyto(data[i]) \n",
    "data=[nd.ones((1,2),ctx=cpu(i))*(i+1) for i in ranbge(2)]\n",
    "print('Before:',data)\n",
    "allreduce(data) \n",
    "print('After:',data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后给定一个批量，我们划分它并复制到各个GPU上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]\n",
      " [12. 13. 14. 15.]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "load into: [cpu(0), cpu(1)]\n",
      "Output: [\n",
      "[[0. 1. 2. 3.]\n",
      " [4. 5. 6. 7.]]\n",
      "<NDArray 2x4 @cpu(0)>, \n",
      "[[ 8.  9. 10. 11.]\n",
      " [12. 13. 14. 15.]]\n",
      "<NDArray 2x4 @cpu(1)>]\n"
     ]
    }
   ],
   "source": [
    "def split_and_load(data,ctx):\n",
    "    n,k=data.shape[0],len(ctx) \n",
    "    m=n//k \n",
    "    assert m*k==n,'# examples is not devided by # devices'\n",
    "    return [data[i*m:(i+1)*m].as_in_context(ctx[i]) for i in range(k)] \n",
    "\n",
    "batch=nd.arange(16).reshape((4,4)) \n",
    "# ctx=[gpu(0),gpu(1)]\n",
    "ctx=[cpu(0),cpu(1)]\n",
    "splitted=split_and_load(batch,ctx) \n",
    "print('Input:',batch) \n",
    "print('load into:',ctx) \n",
    "print('Output:',splitted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 训练一个批量\n",
    "\n",
    "现在我们可以实现如何使用数据并行在多个GPU上训练一个批量了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd \n",
    "import sys \n",
    "sys.path.append('..') \n",
    "import utils \n",
    "def train_batch(data,label,params,ctx,lr):\n",
    "    # split the data batch and load them on GPUs\n",
    "    data_list=split_and_load(data,ctx) \n",
    "    label_list=split_and_load(label,ctx)\n",
    "    # run forward on each GPU \n",
    "    with autograd.record():\n",
    "        losses=[loss(lenet(X,W),Y)\n",
    "               for X,Y,W in zip(data_list,label_list,params)] \n",
    "    # run backward on each gpu \n",
    "    for l in losses:\n",
    "        l.backward()\n",
    "    # aggregate gradient over GPUs \n",
    "    for i in range(len(params[0])):\n",
    "        allreduce([params[c][i].grad for c in range(len(ctx))]) \n",
    "    # update parameter with SGD on each GPU \n",
    "    for p in params:\n",
    "        utils.SGD(p,lr/data.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练 \n",
    "现在我们可以定义完整的训练函数。这个跟前面教程里没有什么区别。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "\n",
    "def train(num_gpus,batch_size,lr):\n",
    "    train_data,test_data=utils.load_data_fashion_mnist(batch_size)\n",
    "#     ctx=[gpu[i] for i in range(num_gpus)] \n",
    "    ctx=[cpu(i) for i in range(num_gpus)] \n",
    "    print('Running on ',ctx) \n",
    "    # copy parameters to all GPUs \n",
    "    dev_params=[get_params(params,c) for c in ctx]\n",
    "    for epoch in range(5):\n",
    "        #train\n",
    "        start=time() \n",
    "        for data,label in train_data:\n",
    "            train_batch(data,label,dev_params,ctx,lr) \n",
    "        nd.waitall()\n",
    "        print('Epoch %d ,trainning time %.1f sec'%(\n",
    "                epoch,time()-start)) \n",
    "        # validating on GPU 0 \n",
    "        net=lambda data:lenet(data,dev_params[0]) \n",
    "        test_acc=utils.evaluate_accuracy(test_data,net,ctx[0]) \n",
    "        print('       validation accuracy = %.4f '%(test_acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们使用一个GPU来训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  [cpu(0)]\n",
      "Epoch 0 ,trainning time 9.2 sec\n",
      "       validation accuracy = 0.0999 \n",
      "Epoch 1 ,trainning time 9.2 sec\n",
      "       validation accuracy = 0.1000 \n",
      "Epoch 2 ,trainning time 8.6 sec\n",
      "       validation accuracy = 0.1000 \n",
      "Epoch 3 ,trainning time 8.7 sec\n",
      "       validation accuracy = 0.0998 \n",
      "Epoch 4 ,trainning time 9.0 sec\n",
      "       validation accuracy = 0.1000 \n"
     ]
    }
   ],
   "source": [
    "train(1,256,0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用多个GPU但不改变其他参数会得到跟单GPU一致的结果（但数据是随机顺序，所以会有细微区别）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  [cpu(0), cpu(1)]\n",
      "Epoch 0 ,trainning time 24.1 sec\n",
      "       validation accuracy = 0.1003 \n",
      "Epoch 1 ,trainning time 24.6 sec\n",
      "       validation accuracy = 0.0995 \n",
      "Epoch 2 ,trainning time 23.9 sec\n",
      "       validation accuracy = 0.0998 \n",
      "Epoch 3 ,trainning time 24.3 sec\n",
      "       validation accuracy = 0.0994 \n",
      "Epoch 4 ,trainning time 24.3 sec\n",
      "       validation accuracy = 0.0998 \n"
     ]
    }
   ],
   "source": [
    "train(2,256*2,0.3*2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但在多GPU时，通常我们需要增加批量大小使得每个GPU能得到足够多的任务来保证性能。但一个大的批量大小可能使得收敛变慢。这时候的一个常用做法是将学习率增大些。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  [cpu(0), cpu(1)]\n",
      "Epoch 0 ,trainning time 23.9 sec\n",
      "       validation accuracy = 0.1007 \n",
      "Epoch 1 ,trainning time 24.0 sec\n",
      "       validation accuracy = 0.1001 \n",
      "Epoch 2 ,trainning time 24.1 sec\n",
      "       validation accuracy = 0.0995 \n",
      "Epoch 3 ,trainning time 23.0 sec\n",
      "       validation accuracy = 0.0996 \n",
      "Epoch 4 ,trainning time 23.5 sec\n",
      "       validation accuracy = 0.1009 \n"
     ]
    }
   ],
   "source": [
    "train(2,512,0.6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "可以看到使用两个GPU能有效的减少训练时间。\n",
    "\n",
    "## 结论\n",
    "\n",
    "数据并行可以有效的在多GPU上提升训练性能。\n",
    "\n",
    "## 练习\n",
    "\n",
    "- 试试不同的批量大小和学习率\n",
    "- 将预测也改成多GPU版本\n",
    "- 注意到我们使用GPU 0来做梯度求和，会有带来什么问题吗？\n",
    "\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1884)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
