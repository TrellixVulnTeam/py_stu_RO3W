{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多GPU来训练 --- 使用Gluon\n",
    "\n",
    "\n",
    "在Gluon里可以很容易的使用数据并行。在[多GPU来训练 --- 从0开始](./multiple-gpus-scratch.md)里我们手动实现了几个数据同步函数来使用数据并行，Gluon里实现了同样的功能。\n",
    "\n",
    "\n",
    "## 多设备上的初始化\n",
    "\n",
    "之前我们介绍了如果使用`initialize()`里的`ctx`在CPU或者特定GPU上初始化模型。事实上，`ctx`可以接受一系列的设备，它会将初始好的参数复制所有的设备上。\n",
    "\n",
    "这里我们使用之前介绍Resnet18来作为演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils \n",
    "from mxnet import gpu,cpu \n",
    "net=utils.resnet18(10) \n",
    "# ctx=[gpu(0),gpu(0)] \n",
    "ctx=[gpu(0),cpu(1)]\n",
    "net.initialize(ctx=ctx) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "记得前面提到的[延迟初始化](../chapter_gluon-basics/parameters.md)，这里参数还没有被初始化。我们需要先给定数据跑一次。\n",
    "\n",
    "Gluon提供了之前我们实现的`split_and_load`函数，它将数据分割并返回各个设备上的复制。然后根据输入的设备，计算也会在相应的数据上执行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "[[[[0.5946477  0.76241076 0.9286425  ... 0.55939174 0.01368841\n",
      "    0.42221504]\n",
      "   [0.20991999 0.13124892 0.6999888  ... 0.7284153  0.06722905\n",
      "    0.00324187]\n",
      "   [0.03553848 0.04015033 0.42025933 ... 0.00680894 0.36719233\n",
      "    0.72177184]\n",
      "   ...\n",
      "   [0.25489825 0.7400993  0.80378944 ... 0.14468406 0.06724603\n",
      "    0.61052424]\n",
      "   [0.59011513 0.6301808  0.715846   ... 0.99822533 0.366307\n",
      "    0.6627515 ]\n",
      "   [0.6474741  0.16908157 0.2554662  ... 0.731404   0.2669775\n",
      "    0.94236565]]]\n",
      "\n",
      "\n",
      " [[[0.3346171  0.29929847 0.04035985 ... 0.9418107  0.14765576\n",
      "    0.7996131 ]\n",
      "   [0.41482422 0.93953705 0.82133496 ... 0.71857834 0.19046582\n",
      "    0.2492019 ]\n",
      "   [0.16201365 0.57839215 0.40134495 ... 0.5595262  0.05132892\n",
      "    0.47892925]\n",
      "   ...\n",
      "   [0.3079302  0.12343674 0.0858999  ... 0.5605767  0.2879493\n",
      "    0.9880256 ]\n",
      "   [0.03409025 0.5199983  0.45360327 ... 0.16639715 0.29464877\n",
      "    0.38487077]\n",
      "   [0.81168246 0.7024145  0.6347502  ... 0.16443568 0.47478315\n",
      "    0.74929905]]]]\n",
      "<NDArray 2x1x28x28 @gpu(0)>, \n",
      "[[[[0.95384276 0.61584395 0.44048318 ... 0.93169624 0.41792694\n",
      "    0.5444825 ]\n",
      "   [0.01962628 0.35090458 0.91959965 ... 0.3258905  0.8848927\n",
      "    0.19673927]\n",
      "   [0.3496516  0.89972854 0.6247903  ... 0.21345687 0.469547\n",
      "    0.190233  ]\n",
      "   ...\n",
      "   [0.60758984 0.40516785 0.5343159  ... 0.98180103 0.18722537\n",
      "    0.67084736]\n",
      "   [0.4075129  0.28534335 0.29537845 ... 0.06619456 0.32603487\n",
      "    0.5905522 ]\n",
      "   [0.7503637  0.54369766 0.43480188 ... 0.28678495 0.53656906\n",
      "    0.69230676]]]\n",
      "\n",
      "\n",
      " [[[0.37921092 0.47964916 0.91283286 ... 0.65726495 0.46728465\n",
      "    0.58591044]\n",
      "   [0.75878185 0.33785692 0.5653516  ... 0.3345375  0.74949163\n",
      "    0.06694086]\n",
      "   [0.8730488  0.47785154 0.7480241  ... 0.22148769 0.01691948\n",
      "    0.855936  ]\n",
      "   ...\n",
      "   [0.6879596  0.08069725 0.42216524 ... 0.6540939  0.82430416\n",
      "    0.803633  ]\n",
      "   [0.82108736 0.52263594 0.18081425 ... 0.8478645  0.06407528\n",
      "    0.43341553]\n",
      "   [0.09428593 0.20584744 0.5714433  ... 0.39324567 0.15467085\n",
      "    0.86779034]]]]\n",
      "<NDArray 2x1x28x28 @cpu(1)>]\n",
      "\n",
      "[[ 0.07788589  0.00734857 -0.06858139 -0.06105129 -0.01568669 -0.03048839\n",
      "  -0.05086926  0.03013314  0.01171404  0.04863874]\n",
      " [ 0.05814621  0.01034843 -0.07526866 -0.06304718 -0.01161004 -0.02814283\n",
      "  -0.05770749  0.03582228  0.01896804  0.04553026]]\n",
      "<NDArray 2x10 @gpu(0)>\n",
      "\n",
      "[[ 0.06706081  0.00215675 -0.07989326 -0.05678979 -0.01074076 -0.02301599\n",
      "  -0.05770484  0.0281867   0.0212204   0.0482414 ]\n",
      " [ 0.07227284  0.00287759 -0.0812291  -0.05831236 -0.01157168 -0.02907148\n",
      "  -0.06432524  0.03312415  0.0148863   0.04780095]]\n",
      "<NDArray 2x10 @cpu(1)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd \n",
    "from mxnet import gluon \n",
    "x=nd.random.uniform(shape=(4,1,28,28)) \n",
    "x_list=gluon.utils.split_and_load(x,ctx) \n",
    "print(x_list) \n",
    "print(net(x_list[0])) \n",
    "print(net(x_list[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候我们可以来看初始的过程发生了什么了。记得我们可以通过`data`来访问参数值，它默认会返回CPU上值。但这里我们只在两个GPU上初始化了，在访问的对应设备的值的时候，我们需要指定设备。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[[ 0.04197619 -0.05456534  0.06447314]\n",
      "  [ 0.06561633  0.04185344 -0.0067775 ]\n",
      "  [-0.05908101  0.04418553  0.04269098]]]\n",
      "<NDArray 1x3x3 @gpu(0)>\n",
      "\n",
      "[[[ 0.04197619 -0.05456534  0.06447314]\n",
      "  [ 0.06561633  0.04185344 -0.0067775 ]\n",
      "  [-0.05908101  0.04418553  0.04269098]]]\n",
      "<NDArray 1x3x3 @cpu(1)>\n",
      "Not initialize on  cpu(0)\n"
     ]
    }
   ],
   "source": [
    "weight=net[1].params.get('weight') \n",
    "print(weight.data(ctx[0])[0])\n",
    "print(weight.data(ctx[1])[0]) \n",
    "try:\n",
    "    weight.data(cpu()) \n",
    "except:\n",
    "    print('Not initialize on ',cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "上一章我们提到过如何在多GPU之间复制梯度求和并广播，这个在`gluon.Trainer`里面会被默认执行。这样我们可以实现完整的训练函数了。\n",
    "\n",
    "## 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import autograd \n",
    "from time import time \n",
    "from mxnet import init \n",
    "\n",
    "def train(num_gpus,batch_size,lr):\n",
    "    train_data,test_data=utils.load_data_fashion_mnist(batch_size) \n",
    "#     ctx=[gpu(i) for i in range(num_gpus)] \n",
    "    ctx=[cpu(i) for i in range(num_gpus)] \n",
    "    print('Running on',ctx) \n",
    "    net=utils.resnet18(10) \n",
    "    net.initialize(init=init.Xavier(),ctx=ctx) \n",
    "    loss=gluon.loss.SoftmaxCrossEntropyLoss() \n",
    "    trainer=gluon.Trainer(\n",
    "        net.collect_params(),'sgd',{'learning_rate':lr}\n",
    "    )\n",
    "    for epoch in range(5):\n",
    "        start=time()\n",
    "        total_loss=0 \n",
    "        for data,label in train_data:\n",
    "            data_list=gluon.utils.split_and_load(data,ctx) \n",
    "            label_list=gluon.utils.split_and_load(label,ctx) \n",
    "            with autograd.record():\n",
    "                losses=[loss(net(X),y) for X,y in zip(data_list,label_list)]\n",
    "            for l in losses:\n",
    "                l.backward()\n",
    "            total_loss+=sum([l.sum().asscalar() for l in losses]) \n",
    "            trainer.step(batch_size) \n",
    "        nd.waitall() \n",
    "        print('Epoch %d, training time = %.1f sec'%(\n",
    "            epoch, time()-start))\n",
    "        test_acc=utils.evaluate_accuracy(test_data,net,ctx[0])\n",
    "        print('       validation accuracy = %.4f'%(test_acc)) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on [cpu(0)]\n",
      "Epoch 0, training time = 71.9 sec\n",
      "       validation accuracy = 0.8761\n",
      "Epoch 1, training time = 71.3 sec\n",
      "       validation accuracy = 0.8957\n",
      "Epoch 2, training time = 73.9 sec\n",
      "       validation accuracy = 0.9126\n",
      "Epoch 3, training time = 74.3 sec\n",
      "       validation accuracy = 0.9135\n",
      "Epoch 4, training time = 72.3 sec\n",
      "       validation accuracy = 0.9057\n"
     ]
    }
   ],
   "source": [
    "train(1,256,.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的参数，但使用两个GPU 。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on [cpu(0), cpu(1)]\n",
      "Epoch 0, training time = 262.0 sec\n",
      "       validation accuracy = 0.8803\n",
      "Epoch 1, training time = 258.5 sec\n",
      "       validation accuracy = 0.8955\n",
      "Epoch 2, training time = 262.7 sec\n",
      "       validation accuracy = 0.8944\n"
     ]
    }
   ],
   "source": [
    "train(2,256,.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
