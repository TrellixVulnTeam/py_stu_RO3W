{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYlaRwNu7ojq"
   },
   "source": [
    "# **Homework 2-1 Phoneme Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emUd7uS7crTz"
   },
   "source": [
    "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
    "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
    "\n",
    "This homework is a multiclass classification task, \n",
    "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
    "\n",
    "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVUGfWTo7_Oj"
   },
   "source": [
    "## Download Data\n",
    "Download data from google drive, then unzip it.\n",
    "\n",
    "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
    "`timit_11/`\n",
    "- `train_11.npy`: training data<br>\n",
    "- `train_label_11.npy`: training label<br>\n",
    "- `test_11.npy`:  testing data<br><br>\n",
    "\n",
    "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_L_4anls8Drv"
   },
   "source": [
    "## Preparing Data\n",
    "Load the training and testing data from the `.npy` file (NumPy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJjLT8em-y9G",
    "outputId": "8edc6bfe-7511-447f-f239-00b96dba6dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Size of training data: (1229932, 429)\n",
      "Size of testing data: (451552, 429)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from radam import radam\n",
    "\n",
    "print('Loading data ...')\n",
    "\n",
    "data_root='timit_11/timit_11/'\n",
    "train = np.load(data_root + 'train_11.npy')\n",
    "train_label = np.load(data_root + 'train_label_11.npy')\n",
    "test = np.load(data_root + 'test_11.npy')\n",
    "\n",
    "print('Size of training data: {}'.format(train.shape))\n",
    "print('Size of testing data: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us5XW_x6udZQ"
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fjf5EcmJtf4e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TIMITDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = torch.from_numpy(X).float()\n",
    "        if y is not None:\n",
    "            y = y.astype(np.int)\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otIC6WhGeh9v"
   },
   "source": [
    "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYqi_lAuvC59",
    "outputId": "13dabe63-4849-47ee-fe04-57427b9d601c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: (1168435, 429)\n",
      "Size of validation set: (61497, 429)\n"
     ]
    }
   ],
   "source": [
    "VAL_RATIO = 0.05\n",
    "\n",
    "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
    "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
    "print('Size of training set: {}'.format(train_x.shape))\n",
    "print('Size of validation set: {}'.format(val_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = train_x.mean(axis=0)\n",
    "# std = train_x.std(axis=0)\n",
    "# train_x = (train_x - mean) / std\n",
    "# val_x = (val_x - mean) / std\n",
    "# test = (test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbCfclUIgMTX"
   },
   "source": [
    "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RUCbQvqJurYc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f6af567917fe>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = TIMITDataset(train_x, train_y)\n",
    "val_set = TIMITDataset(val_x, val_y)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SY7X0lUgb50"
   },
   "source": [
    "Cleanup the unneeded variables to save memory.<br>\n",
    "\n",
    "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8rzkGraeYeN",
    "outputId": "dc790996-a43c-4a99-90d4-e7928892a899"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del train, train_label, train_x, train_y, val_x, val_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRqKNvNZwe3V"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYr1ng5fh9pA"
   },
   "source": [
    "Define model architecture, you are encouraged to change and experiment with the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lbZrwT6Ny0XL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(429, 2048)\n",
    "        self.layer2 = nn.Linear(2048, 2048)\n",
    "        self.layer3 = nn.Linear(2048, 2048)\n",
    "        self.layer4 = nn.Linear(2048, 2048)\n",
    "        self.layer5 = nn.Linear(2048, 2048)\n",
    "        self.layer6 = nn.Linear(2048, 2048)\n",
    "        self.layer7 = nn.Linear(2048, 2048)\n",
    "        self.layer8 = nn.Linear(2048, 2048)\n",
    "        self.out = nn.Linear(2048, 39)\n",
    "        torch.nn.init.xavier_uniform_(self.layer1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.layer2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.layer3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.layer4.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.layer5.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.layer6.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.layer7.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.layer8.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.out.weight)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.bn1 = nn.BatchNorm1d(2048)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "        self.bn3 = nn.BatchNorm1d(2048)\n",
    "        self.bn4 = nn.BatchNorm1d(2048)\n",
    "        self.bn5 = nn.BatchNorm1d(2048)\n",
    "        self.bn6 = nn.BatchNorm1d(2048)\n",
    "        self.bn7 = nn.BatchNorm1d(2048)\n",
    "        self.bn8 = nn.BatchNorm1d(2048)\n",
    "        self.act_fn = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.layer5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.layer6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.layer7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.layer8(x)\n",
    "        x = self.bn8(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRYciXZvPbYh"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "y114Vmm3Ja6o"
   },
   "outputs": [],
   "source": [
    "#check device\n",
    "def get_device():\n",
    "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEX-yjHjhGuH"
   },
   "source": [
    "Fix random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "88xPiUnm0tAd"
   },
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbBcBXkSp6RA"
   },
   "source": [
    "Feel free to change the training parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QTp3ZXg1yO9Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "same_seeds(0)\n",
    "\n",
    "# get device \n",
    "device = get_device()\n",
    "print(f'DEVICE: {device}')\n",
    "\n",
    "# training parameters\n",
    "num_epoch = 1000               # number of training epoch\n",
    "learning_rate = 0.001       # learning rate\n",
    "\n",
    "# the path where checkpoint saved\n",
    "model_path = './model.ckpt'\n",
    "\n",
    "# create model, define a loss function, and optimizer\n",
    "model = Classifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer = radam.RAdam(model.parameters(), lr=learning_rate)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdMWsBs7zzNs",
    "outputId": "c5ed561e-610d-4a35-d936-fd97adf342a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiwei/ML/hw2/radam/radam.py:58: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/1000] Train Acc: 0.292743 Loss: 2.570507 | Val Acc: 0.447664 loss: 1.863715\n",
      "saving model with acc 0.448\n",
      "[002/1000] Train Acc: 0.509695 Loss: 1.633296 | Val Acc: 0.601607 loss: 1.308617\n",
      "saving model with acc 0.602\n",
      "[003/1000] Train Acc: 0.567423 Loss: 1.434306 | Val Acc: 0.644389 loss: 1.162333\n",
      "saving model with acc 0.644\n",
      "[004/1000] Train Acc: 0.599006 Loss: 1.322870 | Val Acc: 0.668748 loss: 1.063398\n",
      "saving model with acc 0.669\n",
      "[005/1000] Train Acc: 0.620674 Loss: 1.246921 | Val Acc: 0.684537 loss: 1.013119\n",
      "saving model with acc 0.685\n",
      "[006/1000] Train Acc: 0.634962 Loss: 1.193965 | Val Acc: 0.691627 loss: 0.985173\n",
      "saving model with acc 0.692\n",
      "[007/1000] Train Acc: 0.646188 Loss: 1.151516 | Val Acc: 0.703335 loss: 0.947691\n",
      "saving model with acc 0.703\n",
      "[008/1000] Train Acc: 0.656168 Loss: 1.115128 | Val Acc: 0.708018 loss: 0.922541\n",
      "saving model with acc 0.708\n",
      "[009/1000] Train Acc: 0.664085 Loss: 1.085693 | Val Acc: 0.713124 loss: 0.901532\n",
      "saving model with acc 0.713\n",
      "[010/1000] Train Acc: 0.670552 Loss: 1.061588 | Val Acc: 0.721629 loss: 0.877647\n",
      "saving model with acc 0.722\n",
      "[011/1000] Train Acc: 0.676139 Loss: 1.041343 | Val Acc: 0.719807 loss: 0.875648\n",
      "[012/1000] Train Acc: 0.681349 Loss: 1.022124 | Val Acc: 0.728084 loss: 0.855628\n",
      "saving model with acc 0.728\n",
      "[013/1000] Train Acc: 0.685654 Loss: 1.007247 | Val Acc: 0.728930 loss: 0.847721\n",
      "saving model with acc 0.729\n",
      "[014/1000] Train Acc: 0.689208 Loss: 0.992884 | Val Acc: 0.729743 loss: 0.841030\n",
      "saving model with acc 0.730\n",
      "[015/1000] Train Acc: 0.692352 Loss: 0.980773 | Val Acc: 0.732637 loss: 0.829158\n",
      "saving model with acc 0.733\n",
      "[016/1000] Train Acc: 0.695789 Loss: 0.967913 | Val Acc: 0.735418 loss: 0.822507\n",
      "saving model with acc 0.735\n",
      "[017/1000] Train Acc: 0.698355 Loss: 0.959440 | Val Acc: 0.736052 loss: 0.821088\n",
      "saving model with acc 0.736\n",
      "[018/1000] Train Acc: 0.700668 Loss: 0.949615 | Val Acc: 0.738312 loss: 0.815477\n",
      "saving model with acc 0.738\n",
      "[019/1000] Train Acc: 0.702723 Loss: 0.939944 | Val Acc: 0.736426 loss: 0.814982\n",
      "[020/1000] Train Acc: 0.705587 Loss: 0.931557 | Val Acc: 0.739158 loss: 0.804494\n",
      "saving model with acc 0.739\n",
      "[021/1000] Train Acc: 0.707301 Loss: 0.923351 | Val Acc: 0.742183 loss: 0.799970\n",
      "saving model with acc 0.742\n",
      "[022/1000] Train Acc: 0.708998 Loss: 0.918030 | Val Acc: 0.742866 loss: 0.795201\n",
      "saving model with acc 0.743\n",
      "[023/1000] Train Acc: 0.710212 Loss: 0.911081 | Val Acc: 0.741532 loss: 0.797944\n",
      "[024/1000] Train Acc: 0.712709 Loss: 0.904478 | Val Acc: 0.743142 loss: 0.795091\n",
      "saving model with acc 0.743\n",
      "[025/1000] Train Acc: 0.713979 Loss: 0.898389 | Val Acc: 0.747760 loss: 0.782953\n",
      "saving model with acc 0.748\n",
      "[026/1000] Train Acc: 0.714854 Loss: 0.893565 | Val Acc: 0.748167 loss: 0.778586\n",
      "saving model with acc 0.748\n",
      "[027/1000] Train Acc: 0.716500 Loss: 0.888633 | Val Acc: 0.745044 loss: 0.782215\n",
      "[028/1000] Train Acc: 0.718292 Loss: 0.881801 | Val Acc: 0.746947 loss: 0.778083\n",
      "[029/1000] Train Acc: 0.719116 Loss: 0.879119 | Val Acc: 0.746963 loss: 0.777352\n",
      "[030/1000] Train Acc: 0.720256 Loss: 0.874511 | Val Acc: 0.749825 loss: 0.773764\n",
      "saving model with acc 0.750\n",
      "[031/1000] Train Acc: 0.721746 Loss: 0.868403 | Val Acc: 0.749988 loss: 0.772483\n",
      "saving model with acc 0.750\n",
      "[032/1000] Train Acc: 0.723167 Loss: 0.865402 | Val Acc: 0.749256 loss: 0.769269\n",
      "[033/1000] Train Acc: 0.723977 Loss: 0.860625 | Val Acc: 0.749467 loss: 0.765414\n",
      "[034/1000] Train Acc: 0.724561 Loss: 0.858430 | Val Acc: 0.750801 loss: 0.765779\n",
      "saving model with acc 0.751\n",
      "[035/1000] Train Acc: 0.726508 Loss: 0.852954 | Val Acc: 0.752167 loss: 0.770108\n",
      "saving model with acc 0.752\n",
      "[036/1000] Train Acc: 0.726667 Loss: 0.850769 | Val Acc: 0.751110 loss: 0.765924\n",
      "[037/1000] Train Acc: 0.727785 Loss: 0.847547 | Val Acc: 0.751061 loss: 0.761722\n",
      "[038/1000] Train Acc: 0.728885 Loss: 0.842927 | Val Acc: 0.751159 loss: 0.761971\n",
      "[039/1000] Train Acc: 0.729141 Loss: 0.841376 | Val Acc: 0.750589 loss: 0.765230\n",
      "[040/1000] Train Acc: 0.730421 Loss: 0.838063 | Val Acc: 0.753695 loss: 0.760639\n",
      "saving model with acc 0.754\n",
      "[041/1000] Train Acc: 0.731021 Loss: 0.834536 | Val Acc: 0.754915 loss: 0.755022\n",
      "saving model with acc 0.755\n",
      "[042/1000] Train Acc: 0.731444 Loss: 0.830978 | Val Acc: 0.754004 loss: 0.753567\n",
      "[043/1000] Train Acc: 0.732806 Loss: 0.828366 | Val Acc: 0.754476 loss: 0.756239\n",
      "[044/1000] Train Acc: 0.733038 Loss: 0.825834 | Val Acc: 0.756590 loss: 0.753368\n",
      "saving model with acc 0.757\n",
      "[045/1000] Train Acc: 0.734191 Loss: 0.823861 | Val Acc: 0.753516 loss: 0.756260\n",
      "[046/1000] Train Acc: 0.734449 Loss: 0.821521 | Val Acc: 0.753923 loss: 0.756753\n",
      "[047/1000] Train Acc: 0.735447 Loss: 0.818290 | Val Acc: 0.755972 loss: 0.750251\n",
      "[048/1000] Train Acc: 0.736120 Loss: 0.815915 | Val Acc: 0.755972 loss: 0.749400\n",
      "[049/1000] Train Acc: 0.736941 Loss: 0.813070 | Val Acc: 0.756004 loss: 0.753701\n",
      "[050/1000] Train Acc: 0.737934 Loss: 0.809316 | Val Acc: 0.756915 loss: 0.745614\n",
      "saving model with acc 0.757\n",
      "[051/1000] Train Acc: 0.737848 Loss: 0.809259 | Val Acc: 0.754622 loss: 0.748337\n",
      "[052/1000] Train Acc: 0.738577 Loss: 0.806258 | Val Acc: 0.757419 loss: 0.750909\n",
      "saving model with acc 0.757\n",
      "[053/1000] Train Acc: 0.739071 Loss: 0.804785 | Val Acc: 0.756769 loss: 0.743499\n",
      "[054/1000] Train Acc: 0.739504 Loss: 0.802285 | Val Acc: 0.756086 loss: 0.745939\n",
      "[055/1000] Train Acc: 0.740450 Loss: 0.800137 | Val Acc: 0.757874 loss: 0.745109\n",
      "saving model with acc 0.758\n",
      "[056/1000] Train Acc: 0.740850 Loss: 0.798229 | Val Acc: 0.757939 loss: 0.744866\n",
      "saving model with acc 0.758\n",
      "[057/1000] Train Acc: 0.741111 Loss: 0.795284 | Val Acc: 0.755923 loss: 0.746530\n",
      "[058/1000] Train Acc: 0.741284 Loss: 0.795456 | Val Acc: 0.758720 loss: 0.741114\n",
      "saving model with acc 0.759\n",
      "[059/1000] Train Acc: 0.741600 Loss: 0.793321 | Val Acc: 0.759045 loss: 0.740096\n",
      "saving model with acc 0.759\n",
      "[060/1000] Train Acc: 0.742839 Loss: 0.791112 | Val Acc: 0.758915 loss: 0.744846\n",
      "[061/1000] Train Acc: 0.743372 Loss: 0.789620 | Val Acc: 0.758801 loss: 0.740126\n",
      "[062/1000] Train Acc: 0.744256 Loss: 0.787205 | Val Acc: 0.758183 loss: 0.743024\n",
      "[063/1000] Train Acc: 0.744044 Loss: 0.785406 | Val Acc: 0.758086 loss: 0.745373\n",
      "[064/1000] Train Acc: 0.745501 Loss: 0.782506 | Val Acc: 0.759614 loss: 0.739545\n",
      "saving model with acc 0.760\n",
      "[065/1000] Train Acc: 0.745418 Loss: 0.781746 | Val Acc: 0.759972 loss: 0.735894\n",
      "saving model with acc 0.760\n",
      "[066/1000] Train Acc: 0.745234 Loss: 0.780562 | Val Acc: 0.759728 loss: 0.738222\n",
      "[067/1000] Train Acc: 0.746121 Loss: 0.778634 | Val Acc: 0.760021 loss: 0.738102\n",
      "saving model with acc 0.760\n",
      "[068/1000] Train Acc: 0.746233 Loss: 0.777122 | Val Acc: 0.759842 loss: 0.734017\n",
      "[069/1000] Train Acc: 0.746203 Loss: 0.775947 | Val Acc: 0.758899 loss: 0.737439\n",
      "[070/1000] Train Acc: 0.747118 Loss: 0.774801 | Val Acc: 0.759192 loss: 0.741384\n",
      "[071/1000] Train Acc: 0.747286 Loss: 0.773982 | Val Acc: 0.760655 loss: 0.739817\n",
      "saving model with acc 0.761\n",
      "[072/1000] Train Acc: 0.747471 Loss: 0.772390 | Val Acc: 0.761305 loss: 0.737303\n",
      "saving model with acc 0.761\n",
      "[073/1000] Train Acc: 0.748236 Loss: 0.770266 | Val Acc: 0.762395 loss: 0.732742\n",
      "saving model with acc 0.762\n",
      "[074/1000] Train Acc: 0.748901 Loss: 0.768912 | Val Acc: 0.760525 loss: 0.736522\n",
      "[075/1000] Train Acc: 0.749188 Loss: 0.767450 | Val Acc: 0.762427 loss: 0.733841\n",
      "saving model with acc 0.762\n",
      "[076/1000] Train Acc: 0.749457 Loss: 0.765691 | Val Acc: 0.762639 loss: 0.729405\n",
      "saving model with acc 0.763\n",
      "[077/1000] Train Acc: 0.750006 Loss: 0.764116 | Val Acc: 0.762102 loss: 0.729906\n",
      "[078/1000] Train Acc: 0.749845 Loss: 0.764930 | Val Acc: 0.761159 loss: 0.741297\n",
      "[079/1000] Train Acc: 0.750113 Loss: 0.763114 | Val Acc: 0.762265 loss: 0.728789\n",
      "[080/1000] Train Acc: 0.751296 Loss: 0.759764 | Val Acc: 0.762736 loss: 0.731226\n",
      "saving model with acc 0.763\n",
      "[081/1000] Train Acc: 0.751068 Loss: 0.760551 | Val Acc: 0.763338 loss: 0.731054\n",
      "saving model with acc 0.763\n",
      "[082/1000] Train Acc: 0.751907 Loss: 0.757922 | Val Acc: 0.761988 loss: 0.731746\n",
      "[083/1000] Train Acc: 0.752002 Loss: 0.756799 | Val Acc: 0.761875 loss: 0.733577\n",
      "[084/1000] Train Acc: 0.752008 Loss: 0.756200 | Val Acc: 0.762866 loss: 0.732752\n",
      "[085/1000] Train Acc: 0.752523 Loss: 0.754288 | Val Acc: 0.764249 loss: 0.727091\n",
      "saving model with acc 0.764\n",
      "[086/1000] Train Acc: 0.752730 Loss: 0.754011 | Val Acc: 0.762558 loss: 0.726913\n",
      "[087/1000] Train Acc: 0.753978 Loss: 0.751354 | Val Acc: 0.762476 loss: 0.729705\n",
      "[088/1000] Train Acc: 0.753390 Loss: 0.751669 | Val Acc: 0.762932 loss: 0.727658\n",
      "[089/1000] Train Acc: 0.753456 Loss: 0.750281 | Val Acc: 0.761631 loss: 0.730032\n",
      "[090/1000] Train Acc: 0.754322 Loss: 0.749398 | Val Acc: 0.762915 loss: 0.725216\n",
      "[091/1000] Train Acc: 0.754730 Loss: 0.747293 | Val Acc: 0.762850 loss: 0.734627\n",
      "[092/1000] Train Acc: 0.754727 Loss: 0.747145 | Val Acc: 0.763631 loss: 0.730148\n",
      "[093/1000] Train Acc: 0.755172 Loss: 0.745995 | Val Acc: 0.763306 loss: 0.723855\n",
      "[094/1000] Train Acc: 0.755475 Loss: 0.745139 | Val Acc: 0.765631 loss: 0.722421\n",
      "saving model with acc 0.766\n",
      "[095/1000] Train Acc: 0.755552 Loss: 0.743750 | Val Acc: 0.763143 loss: 0.722309\n",
      "[096/1000] Train Acc: 0.755777 Loss: 0.743335 | Val Acc: 0.763078 loss: 0.725517\n",
      "[097/1000] Train Acc: 0.756138 Loss: 0.741658 | Val Acc: 0.762704 loss: 0.727401\n",
      "[098/1000] Train Acc: 0.756415 Loss: 0.740361 | Val Acc: 0.764867 loss: 0.725624\n",
      "[099/1000] Train Acc: 0.757063 Loss: 0.738885 | Val Acc: 0.762265 loss: 0.727047\n",
      "[100/1000] Train Acc: 0.756827 Loss: 0.740215 | Val Acc: 0.764184 loss: 0.725975\n",
      "[101/1000] Train Acc: 0.757537 Loss: 0.737795 | Val Acc: 0.763419 loss: 0.724415\n",
      "[102/1000] Train Acc: 0.756924 Loss: 0.738219 | Val Acc: 0.763062 loss: 0.725770\n",
      "[103/1000] Train Acc: 0.758129 Loss: 0.735910 | Val Acc: 0.764395 loss: 0.725741\n",
      "[104/1000] Train Acc: 0.757863 Loss: 0.735973 | Val Acc: 0.764525 loss: 0.720962\n",
      "[105/1000] Train Acc: 0.758028 Loss: 0.734537 | Val Acc: 0.763988 loss: 0.719957\n",
      "[106/1000] Train Acc: 0.758409 Loss: 0.733382 | Val Acc: 0.766200 loss: 0.721408\n",
      "saving model with acc 0.766\n",
      "[107/1000] Train Acc: 0.758918 Loss: 0.732625 | Val Acc: 0.764541 loss: 0.727817\n",
      "[108/1000] Train Acc: 0.758879 Loss: 0.732773 | Val Acc: 0.762688 loss: 0.731574\n",
      "[109/1000] Train Acc: 0.758861 Loss: 0.731123 | Val Acc: 0.765533 loss: 0.722971\n",
      "[110/1000] Train Acc: 0.759596 Loss: 0.731037 | Val Acc: 0.764330 loss: 0.727753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-77022c11ea48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "\n",
    "best_acc = 0.0\n",
    "#best_loss = 1000.0\n",
    "for epoch in range(num_epoch):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train() # set the model to training mode\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(inputs) \n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "        batch_loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "    # validation\n",
    "    if len(val_set) > 0:\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                batch_loss = criterion(outputs, labels) \n",
    "                _, val_pred = torch.max(outputs, 1) \n",
    "            \n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                val_loss += batch_loss.item()\n",
    "\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
    "            ))\n",
    "\n",
    "            # if the model improves, save a checkpoint at this epoch\n",
    "            if val_acc > best_acc:\n",
    "            #if val_loss < best_loss:\n",
    "                best_acc = val_acc\n",
    "                #best_loss = val_loss\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(val_acc/len(val_set)))\n",
    "    else:\n",
    "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
    "        ))\n",
    "    #lr_scheduler.step()\n",
    "\n",
    "# if not validating, save the last epoch\n",
    "if len(val_set) == 0:\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('saving model at last epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdMWsBs7zzNs",
    "outputId": "c5ed561e-610d-4a35-d936-fd97adf342a0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/1000] Train Acc: 0.761098 Loss: 0.723523 | Val Acc: 0.765745 loss: 0.720439\n",
      "saving model with acc 0.766\n",
      "[002/1000] Train Acc: 0.762390 Loss: 0.719281 | Val Acc: 0.766672 loss: 0.714885\n",
      "saving model with acc 0.767\n",
      "[003/1000] Train Acc: 0.764099 Loss: 0.713187 | Val Acc: 0.765159 loss: 0.725051\n",
      "[004/1000] Train Acc: 0.764737 Loss: 0.710615 | Val Acc: 0.765972 loss: 0.716813\n",
      "[005/1000] Train Acc: 0.765560 Loss: 0.708870 | Val Acc: 0.766867 loss: 0.712794\n",
      "saving model with acc 0.767\n",
      "[006/1000] Train Acc: 0.765800 Loss: 0.706724 | Val Acc: 0.766541 loss: 0.718818\n",
      "[007/1000] Train Acc: 0.767049 Loss: 0.703501 | Val Acc: 0.766379 loss: 0.716772\n",
      "[008/1000] Train Acc: 0.766999 Loss: 0.704303 | Val Acc: 0.766493 loss: 0.721991\n",
      "[009/1000] Train Acc: 0.767343 Loss: 0.702299 | Val Acc: 0.766574 loss: 0.717801\n",
      "[010/1000] Train Acc: 0.768166 Loss: 0.699730 | Val Acc: 0.767224 loss: 0.718137\n",
      "saving model with acc 0.767\n",
      "[011/1000] Train Acc: 0.767953 Loss: 0.699904 | Val Acc: 0.767615 loss: 0.721102\n",
      "saving model with acc 0.768\n",
      "[012/1000] Train Acc: 0.768542 Loss: 0.699018 | Val Acc: 0.766151 loss: 0.723875\n",
      "[013/1000] Train Acc: 0.768970 Loss: 0.697303 | Val Acc: 0.766769 loss: 0.720057\n",
      "[014/1000] Train Acc: 0.769245 Loss: 0.695683 | Val Acc: 0.767273 loss: 0.716096\n",
      "[015/1000] Train Acc: 0.769306 Loss: 0.696358 | Val Acc: 0.767078 loss: 0.716215\n",
      "[016/1000] Train Acc: 0.768755 Loss: 0.696146 | Val Acc: 0.766899 loss: 0.713876\n",
      "[017/1000] Train Acc: 0.769456 Loss: 0.693808 | Val Acc: 0.766850 loss: 0.718846\n",
      "[018/1000] Train Acc: 0.770130 Loss: 0.693334 | Val Acc: 0.767013 loss: 0.715460\n",
      "[019/1000] Train Acc: 0.769974 Loss: 0.693132 | Val Acc: 0.766167 loss: 0.720445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-46aa232f7f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = radam.RAdam(model.parameters(), lr=0.0001)\n",
    "# start training\n",
    "\n",
    "best_acc = 0.0\n",
    "#best_loss = 1000.0\n",
    "for epoch in range(num_epoch):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train() # set the model to training mode\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(inputs) \n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "        batch_loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "    # validation\n",
    "    if len(val_set) > 0:\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                batch_loss = criterion(outputs, labels) \n",
    "                _, val_pred = torch.max(outputs, 1) \n",
    "            \n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                val_loss += batch_loss.item()\n",
    "\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
    "            ))\n",
    "\n",
    "            # if the model improves, save a checkpoint at this epoch\n",
    "            if val_acc > best_acc:\n",
    "            #if val_loss < best_loss:\n",
    "                best_acc = val_acc\n",
    "                #best_loss = val_loss\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(val_acc/len(val_set)))\n",
    "    else:\n",
    "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
    "        ))\n",
    "    #lr_scheduler.step()\n",
    "\n",
    "# if not validating, save the last epoch\n",
    "if len(val_set) == 0:\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('saving model at last epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = radam.RAdam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdMWsBs7zzNs",
    "outputId": "c5ed561e-610d-4a35-d936-fd97adf342a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/1000] Train Acc: 0.768797 Loss: 0.697942 | Val Acc: 0.766883 loss: 0.719340\n",
      "saving model with acc 0.767\n",
      "[002/1000] Train Acc: 0.768425 Loss: 0.696686 | Val Acc: 0.766298 loss: 0.719098\n",
      "[003/1000] Train Acc: 0.768390 Loss: 0.698017 | Val Acc: 0.766574 loss: 0.717034\n",
      "[004/1000] Train Acc: 0.768899 Loss: 0.696996 | Val Acc: 0.766232 loss: 0.719945\n",
      "[005/1000] Train Acc: 0.769029 Loss: 0.696411 | Val Acc: 0.767680 loss: 0.721514\n",
      "saving model with acc 0.768\n",
      "[006/1000] Train Acc: 0.769313 Loss: 0.696667 | Val Acc: 0.766785 loss: 0.718785\n",
      "[007/1000] Train Acc: 0.769112 Loss: 0.694722 | Val Acc: 0.766574 loss: 0.721841\n",
      "[008/1000] Train Acc: 0.769601 Loss: 0.694119 | Val Acc: 0.767241 loss: 0.718161\n",
      "[009/1000] Train Acc: 0.769724 Loss: 0.694600 | Val Acc: 0.766737 loss: 0.721022\n",
      "[010/1000] Train Acc: 0.769725 Loss: 0.693575 | Val Acc: 0.767387 loss: 0.716493\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-77022c11ea48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "\n",
    "best_acc = 0.0\n",
    "#best_loss = 1000.0\n",
    "for epoch in range(num_epoch):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train() # set the model to training mode\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(inputs) \n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "        batch_loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "    # validation\n",
    "    if len(val_set) > 0:\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                batch_loss = criterion(outputs, labels) \n",
    "                _, val_pred = torch.max(outputs, 1) \n",
    "            \n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                val_loss += batch_loss.item()\n",
    "\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
    "            ))\n",
    "\n",
    "            # if the model improves, save a checkpoint at this epoch\n",
    "            if val_acc > best_acc:\n",
    "            #if val_loss < best_loss:\n",
    "                best_acc = val_acc\n",
    "                #best_loss = val_loss\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(val_acc/len(val_set)))\n",
    "    else:\n",
    "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
    "        ))\n",
    "    #lr_scheduler.step()\n",
    "\n",
    "# if not validating, save the last epoch\n",
    "if len(val_set) == 0:\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('saving model at last epoch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hi7jTn3PX-m"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfUECMFCn5VG"
   },
   "source": [
    "Create a testing dataset, and load model from the saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PKjtAScPWtr",
    "outputId": "8c17272b-536a-4692-a95f-a3292766c698"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create testing dataset\n",
    "test_set = TIMITDataset(test, None)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# create model and load weights from checkpoint\n",
    "model = Classifier().to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "940TtCCdoYd0"
   },
   "source": [
    "Make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "84HU5GGjPqR0"
   },
   "outputs": [],
   "source": [
    "predict = []\n",
    "raw_output = []\n",
    "model.eval() # set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inputs = data\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "\n",
    "        for y in test_pred.cpu().numpy():\n",
    "            predict.append(y)\n",
    "        for output in outputs.cpu().numpy():\n",
    "            raw_output.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_output = np.array(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"output\", raw_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Size of training data: (1229932,)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data ...')\n",
    "\n",
    "data_root='timit_11/timit_11/'\n",
    "train_label = np.load(data_root + 'train_label_11.npy')\n",
    "\n",
    "print('Size of training data: {}'.format(train_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transition matrix\n",
    "trans_table = np.zeros((39,39))\n",
    "train_label = train_label.astype('int')\n",
    "\n",
    "for i in range(len(train_label)-1):\n",
    "    trans_table[train_label[i], train_label[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_table_norm = trans_table / np.sum(trans_table,axis=1,keepdims=True)\n",
    "trans_table_norm += 1e-17 #prevent dividing by zero\n",
    "trans_table_norm = np.log(trans_table_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softmax(dim = 1)\n",
    "test_ln_softmax = m(torch.tensor(raw_output))\n",
    "test_ln_softmax = np.array(test_ln_softmax)\n",
    "test_ln_softmax = test_ln_softmax + 1e-17\n",
    "test_ln_softmax = np.log(test_ln_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 451551/451551 [00:08<00:00, 55648.90it/s]\n"
     ]
    }
   ],
   "source": [
    "tracking = np.zeros((451552, 39))\n",
    "last_state = test_ln_softmax[0]\n",
    "for i in tqdm(range(1,len(test_ln_softmax))):\n",
    "    current_state = np.zeros(39)\n",
    "    prob = last_state.reshape(39,1) + trans_table_norm + test_ln_softmax[i]\n",
    "    current_state = np.max(prob, axis=0)\n",
    "    tracking[i] = np.argmax(prob, axis=0)\n",
    "    last_state = current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(raw_output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls = [25] #depend on last predict value\n",
    "\n",
    "for i in range(0,451551):\n",
    "    back = tracking[451552-i-1][int(pred_ls[-1])]\n",
    "    pred_ls.append(int(back))\n",
    "    \n",
    "pred_ls = pred_ls[::-1]\n",
    "predict = pred_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWDf_C-omElb"
   },
   "source": [
    "Write prediction to a CSV file.\n",
    "\n",
    "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuljYSPHcZir"
   },
   "outputs": [],
   "source": [
    "with open('prediction.csv', 'w') as f:\n",
    "    f.write('Id,Class\\n')\n",
    "    for i, y in enumerate(predict):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "1. [Sample Code](https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb)\n",
    "2. [Radam](https://github.com/LiyuanLucasLiu/RAdam/tree/master/radam)\n",
    "3. [L. Li et al., \"Hybrid Deep Neural Network--Hidden Markov Model (DNN-HMM) Based Speech Emotion Recognition,\" 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction, Geneva, Switzerland, 2013, pp. 312-317, doi: 10.1109/ACII.2013.58.](https://ieeexplore.ieee.org/document/6681449)\n",
    "4. [Abdel-rahman Mohamed, \"Deep Neural Network acoustic models for ASR\"](https://central.bac-lac.gc.ca/.item?id=TC-OTU-44123&op=pdf&app=Library&oclc_number=1032963679)\n",
    "\n",
    "另外文章還有提到先用CD algorithm Pretrain 一個Deep Beliefs Network, 再利用這些weight訓練DNN, 這個方法礙於時間還沒嘗試, 應該可以做更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SHARE MLSpring2021 - HW2-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
