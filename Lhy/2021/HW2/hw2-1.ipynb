{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYlaRwNu7ojq"
   },
   "source": [
    "# **Homework 2-1 Phoneme Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJjLT8em-y9G",
    "outputId": "ec6bb275-cd78-4e0a-e0ba-42382ce35d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 106\n",
      "Loading data ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 149244016 into shape (1229932,429)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d6136d00b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading data ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/timit_11/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_11.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_label_11.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'test_11.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 440\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 149244016 into shape (1229932,429)"
     ]
    }
   ],
   "source": [
    "Prediction = []\n",
    "for seed in range(106, 131):\n",
    "    print('Seed:', seed)\n",
    "    import numpy as np\n",
    "    print('Loading data ...')\n",
    "    data_root='../data/timit_11/'\n",
    "    train = np.load(data_root + 'train_11.npy')\n",
    "    train_label = np.load(data_root + 'train_label_11.npy')\n",
    "    test = np.load(data_root + 'test_11.npy')\n",
    "    print('Size of training data: {}'.format(train.shape))\n",
    "    print('Size of testing data: {}'.format(test.shape))\n",
    "\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    class TIMITDataset(Dataset):\n",
    "        def __init__(self, X, y=None):\n",
    "            self.data = torch.from_numpy(X).float()\n",
    "            if y is not None:\n",
    "                y = y.astype(np.int)\n",
    "                self.label = torch.LongTensor(y)\n",
    "            else:\n",
    "                self.label = None\n",
    "        def __getitem__(self, idx):\n",
    "            if self.label is not None:\n",
    "                return self.data[idx], self.label[idx]\n",
    "            else:\n",
    "                return self.data[idx]\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "    \n",
    "    VAL_RATIO = 0.0\n",
    "    percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
    "    train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
    "    print('Size of training set: {}'.format(train_x.shape))\n",
    "    print('Size of validation set: {}'.format(val_x.shape))\n",
    "    \n",
    "    BATCH_SIZE = 3000\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_set = TIMITDataset(train_x, train_y)\n",
    "    val_set = TIMITDataset(val_x, val_y)\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    import gc\n",
    "    del train, train_label, train_x, train_y, val_x, val_y\n",
    "    gc.collect()\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    class Classifier(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Classifier, self).__init__()\n",
    "            self.layer1 = nn.Linear(429, 1024)\n",
    "            self.layer2 = nn.Linear(1024, 1024)\n",
    "            self.layer3 = nn.Linear(1024, 1024)\n",
    "            self.layer4 = nn.Linear(1024, 1024)\n",
    "            self.layer5 = nn.Linear(1024, 1024)\n",
    "            self.out = nn.Linear(1024, 39) \n",
    "            self.bn1 = nn.BatchNorm1d(1024)\n",
    "            self.bn2 = nn.BatchNorm1d(1024)\n",
    "            self.bn3 = nn.BatchNorm1d(1024)\n",
    "            self.bn4 = nn.BatchNorm1d(1024)\n",
    "            self.bn5 = nn.BatchNorm1d(1024)\n",
    "            self.dropout1 = nn.Dropout(p=0.3)\n",
    "            self.dropout2 = nn.Dropout(p=0.3)\n",
    "            self.dropout3 = nn.Dropout(p=0.3)\n",
    "            self.dropout4 = nn.Dropout(p=0.3)\n",
    "            self.dropout5 = nn.Dropout(p=0.3)\n",
    "            self.act_fn = nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            x = self.layer1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.act_fn(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.act_fn(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = self.act_fn(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = self.layer4(x)\n",
    "            x = self.bn4(x)\n",
    "            x = self.act_fn(x)\n",
    "            x = self.dropout4(x)\n",
    "            x = self.layer5(x)\n",
    "            x = self.bn5(x)\n",
    "            x = self.act_fn(x)\n",
    "            x = self.dropout5(x)\n",
    "            x = self.out(x)\n",
    "            return x\n",
    "        \n",
    "    #check device\n",
    "    def get_device():\n",
    "          return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # fix random seed\n",
    "    def same_seeds(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)  \n",
    "        np.random.seed(seed)  \n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "    same_seeds(seed)\n",
    "    device = get_device()\n",
    "    num_epoch = 200\n",
    "    learning_rate = 0.001\n",
    "    model_path = './model.ckpt'\n",
    "    model = Classifier().to(device)\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # start training\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epoch):\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        # training\n",
    "        model.train() # set the model to training mode\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(inputs) \n",
    "            batch_loss = criterion(outputs, labels)\n",
    "            _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "            batch_loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "            train_loss += batch_loss.item()\n",
    "        # validation\n",
    "        if len(val_set) > 0:\n",
    "            model.eval() # set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    batch_loss = criterion(outputs, labels) \n",
    "                    _, val_pred = torch.max(outputs, 1) \n",
    "\n",
    "                    val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                    epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
    "                ))\n",
    "\n",
    "                # if the model improves, save a checkpoint at this epoch\n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
    "        else:\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
    "            ))\n",
    "    # if not validating, save the last epoch\n",
    "    if len(val_set) == 0:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print('saving model at last epoch')\n",
    "        \n",
    "    # create testing dataset\n",
    "    test_set = TIMITDataset(test, None)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # create model and load weights from checkpoint\n",
    "    model = Classifier().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    predict = []\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs = data\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "\n",
    "            for y in test_pred.cpu().numpy():\n",
    "                predict.append(y)\n",
    "    for i in range(3, len(predict)-3):\n",
    "        if predict[i] != predict[i-1] and predict[i] != predict[i+1]:\n",
    "            if np.argmax(np.bincount(np.array(predict[i-3:i]))) == np.argmax(np.bincount(np.array(predict[i+1:i+4]))):\n",
    "                predict[i] = np.argmax(np.bincount(np.array(predict[i-3:i])))\n",
    "    Prediction.append(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predict)):\n",
    "    predict[i] = np.argmax(np.bincount(np.array([Prediction[0][i], Prediction[1][i], Prediction[2][i], Prediction[3][i], Prediction[4][i],\n",
    "                                                 Prediction[5][i], Prediction[6][i], Prediction[7][i], Prediction[8][i], Prediction[9][i],\n",
    "                                                 Prediction[10][i], Prediction[11][i], Prediction[12][i], Prediction[13][i], Prediction[14][i],\n",
    "                                                 Prediction[15][i], Prediction[16][i], Prediction[17][i], Prediction[18][i], Prediction[19][i],\n",
    "                                                 Prediction[20][i], Prediction[21][i], Prediction[22][i], Prediction[23][i], Prediction[24][i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuljYSPHcZir"
   },
   "outputs": [],
   "source": [
    "with open('prediction.csv', 'w') as f:\n",
    "    f.write('Id,Class\\n')\n",
    "    for i, y in enumerate(predict):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "「SHARE MLSpring2021 - HW2-1.ipynb」的副本",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
